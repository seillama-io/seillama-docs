{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Welcome to Seillama Docs! I try to document all the technical things I do in my spare time, mostly for my own use. On this site you will find some gists around my areas of interest like Hybrid-Cloud, DevSecOps, Containers, Self-Hosted Software and Machine Learning.","title":"Home"},{"location":"#welcome-to-seillama-docs","text":"I try to document all the technical things I do in my spare time, mostly for my own use. On this site you will find some gists around my areas of interest like Hybrid-Cloud, DevSecOps, Containers, Self-Hosted Software and Machine Learning.","title":"Welcome to Seillama Docs!"},{"location":"containers/podman/podman-wsl/","text":"Install Podman on Windows Subsystem for Linux Podman is a daemonless container engine for developing, managing, and running OCI Containers on your Linux System. Containers can either be run as root or in rootless mode. Podman is supported on Windows but by default sets up during install it's own WSL2 instance, which is great if you are fine using Windows powershell to run podman commands, but in my case I heavily rely on my custom Ubuntu WSL instance to work on Windows, so I want to be able to run podman there. This document shows how to install Podman in Ubuntu WSL2 (tested with Podman v3.4.4 on Ubuntu 22.04 ). Install required dependencies: sudo apt update -y sudo apt install -y podman qemu-system Initialize Podman machine: podman machine init Download required gvproxy binary from GitHub releases e.g. : sudo wget https://github.com/containers/gvisor-tap-vsock/releases/download/v0.6.1/gvproxy-linux -O /usr/libexec/podman/gvproxy sudo chmod +x /usr/libexec/podman/gvproxy Grant rw access to kvm , required to start Podman machine: sudo chmod 666 /dev/kvm Start Podman machine: podman machine start","title":"Install Podman on Windows Subsystem for Linux"},{"location":"containers/podman/podman-wsl/#install-podman-on-windows-subsystem-for-linux","text":"Podman is a daemonless container engine for developing, managing, and running OCI Containers on your Linux System. Containers can either be run as root or in rootless mode. Podman is supported on Windows but by default sets up during install it's own WSL2 instance, which is great if you are fine using Windows powershell to run podman commands, but in my case I heavily rely on my custom Ubuntu WSL instance to work on Windows, so I want to be able to run podman there. This document shows how to install Podman in Ubuntu WSL2 (tested with Podman v3.4.4 on Ubuntu 22.04 ). Install required dependencies: sudo apt update -y sudo apt install -y podman qemu-system Initialize Podman machine: podman machine init Download required gvproxy binary from GitHub releases e.g. : sudo wget https://github.com/containers/gvisor-tap-vsock/releases/download/v0.6.1/gvproxy-linux -O /usr/libexec/podman/gvproxy sudo chmod +x /usr/libexec/podman/gvproxy Grant rw access to kvm , required to start Podman machine: sudo chmod 666 /dev/kvm Start Podman machine: podman machine start","title":"Install Podman on Windows Subsystem for Linux"},{"location":"devops/cloud-native-toolkit/install-toolkit/","text":"Cloud-Native Toolkit - GitOps Install Overview Cloud-Native Toolkit is an open-source collection of assets that provide an environment for developing cloud-native applications for deployment within Red Hat OpenShift. Components As the name suggests, the Cloud-Native Toolkit provides a collection of tools that can be used in part or in whole to support the activities of software development life cycle. The following provides a listing of the assets that make up the Cloud-Native Toolkit: Environment components After installation, the environment consists of the following components and developer tools: A Red Hat OpenShift Service development cluster A collection of continuous delivery tools deployed into the cluster A set of backend services This diagram illustrates the environment: The diagram shows the components in the environment: the cluster, the deployment target environments, the cloud services, and the tools. The following best-of-breed open-source software tools are installed in the cluster's tools namespace: Capability Tool Description Continuous Integration Tekton CI Tekton is an emerging tool for Continuous Integration with Kubernetes and OpenShift API Contract Testing Pact Pact enables API contract testing Code Analysis SonarQube SonarQube can scan code and display the results in a dashboard Container Image Registry Container Registry Stores container images to be deployed Artifact Management Artifactory Artifactory is an artifact storage and Helm chart repository Continuous Delivery ArgoCD ArgoCD support Continuous Delivery with GitOps Web IDE Code Ready Workspace IDE for editing and managing code in a web browser Install the Toolkit Prerequisites OpenShift Cluster available with admin access docker CLI available on your workstation GitHub account yq CLI installed on your workstation On OpenShift Console On the OpenShift console, click \"Copy login command\" on the top right corner: Click \"Display Token\": Copy the token and server URL available in the provided oc login command On your workstation Create a cntk directory: mkdir cntk cd cntk Install iascable CLI: curl -sL https://iascable.cloudnativetoolkit.dev/install.sh | sh Get OpenShift GitOps and Developer Tools Bills of Materials: curl -sL https://github.com/cloud-native-toolkit/automation-solutions/releases/download/200-openshift-gitops_v1.0.1/bom.yaml > 200 -openshift-gitops.yaml curl -sL https://github.com/cloud-native-toolkit/automation-solutions/releases/download/220-dev-tools_v1.0.0/bom.yaml > 220 -dev-tools.yaml Build both BOMs using iascable : iascable build -i 200 -openshift-gitops.yaml iascable build -i 220 -dev-tools.yaml Navigate to output directory and open a helper docker container that contains terraform CLI required to run the automation: cd output ./launch.sh In your docker container, create the required environment variables required to run the terraform automation on your OpenShift cluster: export TF_VAR_config_banner_text = \"Cloud-Native Toolkit\" export TF_VAR_gitops_repo_repo = <GITOPS_REPOSITORY> export TF_VAR_server_url = <OPENSHIFT_SERVER_URL> export TF_VAR_cluster_login_token = <OPENSHIFT_SERVER_TOKEN> Help TF_VAR_server_url is the OpenShift server URL you retrieved in the first step TF_VAR_cluster_login_token is the OpenShift login token you retrieved in the first step Navigate to 200-openshift-gitops terraform module and run the automation: cd 200 -openshift-gitops/terraform/ terraform init terraform apply --auto-approve NOTE By default, when not providing a gitops_repo_host terraform variable, a Gitea instance is being deployed in the cluster to host the GitOps repository, to install the developer tools we'll need to retrieve the provisioned Gitea host, username and token. We'll now need to login to the OpenShift cluster using the oc login command from first step to retrieve Gitea credentials to pass to next terraform module: oc login --token = ${ LOGIN_TOKEN } --server = ${ SERVER_URL } Navigate to 220-dev-tools terraform module and run the automation: cd ../../220-dev-tools/terraform export TF_VAR_gitops_repo_gitea_host = $( oc get route -n gitea gitea -o yaml | yq .spec.host ) export TF_VAR_gitops_repo_gitea_username = $( oc get secret -n gitea gitea-access -o yaml | yq .data.username | base64 -d ) export TF_VAR_gitops_repo_gitea_token = $( oc get secret -n gitea gitea-access -o yaml | yq .data.password | base64 -d ) terraform init terraform apply --auto-approve Artifactory initial setup Navigate to OpenShift console. Click this icon Click \"Artifactory\" Log in using default username admin and password password . Click \"Get Started\". Reset password following Artifactory requirements, and save it somewhere safe (e.g. 1Password password manager). Then click \"Next\". Paste the URL into the Select Base URL form and remove any trailing context roots, similar to the one in this view. The next page in the wizard is the Configure a Proxy Server page. This is to setup a proxy for external resources. You can click Next to skip this step. The next page in the wizard is the Create Repositories page. Select \"Generic\", then press \"Next\". The next page in the wizard is the Onboarding Complete page. Press \"Finish\". Allow Anonymous Access to Artifactory: Click on the Settings tab on the left menu (the one with the gear icon), and then select Security Obtain the encrypted password: In the Artifactory console, press the \"Welcome, admin\" menu button in the top right corner of the console and select \"Edit profile\" In the User Profile: admin page, enter you Artifactory password and press Unlock : Below, in the Authentication Settings section, is the Encrypted Password field Press the Eye icon to view the encrypted password and press the Cut & Paste icon to copy it: In the OpenShift 4 console, go to Administrator > Workloads > Secrets . At the top, select the tools project and filter for artifactory . Select Edit Secret on artifactory-access . Add a key/value for ARTIFACTORY_ENCRYPT and set the value to your encrypt key value: Sonarqube initial setup Navigate to OpenShift console. Click this icon Click \"Sonarqube\" Log in using default username admin and password admin . Reset the default admin password by putting a custom one, save it for next step. Create the sonarqube-access secret in OpenShift tools namespace with your newly created admin password: oc create secret generic sonarqube-access -n tools --from-literal = SONARQUBE_USER = admin --from-literal = SONARQUBE_PASSWORD = ${ SONARQUBE_PASSWORD } Add SETFCAP capability to pipeline Security Context Constraints Note : This Step is only required if you are running the Toolkit on OpenShift 4.10+ with Openshift Pipelines operator < 1.7.2 . Add SETFCAP capability in allowed capability to pipelines-scc so cluster tasks can request it: oc patch scc pipelines-scc --type merge -p '{\"allowedCapabilities\":[\"SETFCAP\"]}' Update ibm-build-tag-push tekton task in tools namespace to request required SETFCAP capability, e.g. with task version 3.0.3 : oc get task ibm-build-tag-push-v3-0-3 -n tools -o yaml | yq '.spec.steps[2].securityContext.capabilities.add = [\"SETFCAP\"]' | oc apply -f - Conclusion At this stage the Cloud-Native Toolkit should be up and running, congratulations!","title":"Cloud-Native Toolkit - GitOps Install"},{"location":"devops/cloud-native-toolkit/install-toolkit/#cloud-native-toolkit-gitops-install","text":"","title":"Cloud-Native Toolkit - GitOps Install"},{"location":"devops/cloud-native-toolkit/install-toolkit/#overview","text":"Cloud-Native Toolkit is an open-source collection of assets that provide an environment for developing cloud-native applications for deployment within Red Hat OpenShift.","title":"Overview"},{"location":"devops/cloud-native-toolkit/install-toolkit/#components","text":"As the name suggests, the Cloud-Native Toolkit provides a collection of tools that can be used in part or in whole to support the activities of software development life cycle. The following provides a listing of the assets that make up the Cloud-Native Toolkit:","title":"Components"},{"location":"devops/cloud-native-toolkit/install-toolkit/#environment-components","text":"After installation, the environment consists of the following components and developer tools: A Red Hat OpenShift Service development cluster A collection of continuous delivery tools deployed into the cluster A set of backend services This diagram illustrates the environment: The diagram shows the components in the environment: the cluster, the deployment target environments, the cloud services, and the tools. The following best-of-breed open-source software tools are installed in the cluster's tools namespace: Capability Tool Description Continuous Integration Tekton CI Tekton is an emerging tool for Continuous Integration with Kubernetes and OpenShift API Contract Testing Pact Pact enables API contract testing Code Analysis SonarQube SonarQube can scan code and display the results in a dashboard Container Image Registry Container Registry Stores container images to be deployed Artifact Management Artifactory Artifactory is an artifact storage and Helm chart repository Continuous Delivery ArgoCD ArgoCD support Continuous Delivery with GitOps Web IDE Code Ready Workspace IDE for editing and managing code in a web browser","title":"Environment components"},{"location":"devops/cloud-native-toolkit/install-toolkit/#install-the-toolkit","text":"","title":"Install the Toolkit"},{"location":"devops/cloud-native-toolkit/install-toolkit/#prerequisites","text":"OpenShift Cluster available with admin access docker CLI available on your workstation GitHub account yq CLI installed on your workstation","title":"Prerequisites"},{"location":"devops/cloud-native-toolkit/install-toolkit/#on-openshift-console","text":"On the OpenShift console, click \"Copy login command\" on the top right corner: Click \"Display Token\": Copy the token and server URL available in the provided oc login command","title":"On OpenShift Console"},{"location":"devops/cloud-native-toolkit/install-toolkit/#on-your-workstation","text":"Create a cntk directory: mkdir cntk cd cntk Install iascable CLI: curl -sL https://iascable.cloudnativetoolkit.dev/install.sh | sh Get OpenShift GitOps and Developer Tools Bills of Materials: curl -sL https://github.com/cloud-native-toolkit/automation-solutions/releases/download/200-openshift-gitops_v1.0.1/bom.yaml > 200 -openshift-gitops.yaml curl -sL https://github.com/cloud-native-toolkit/automation-solutions/releases/download/220-dev-tools_v1.0.0/bom.yaml > 220 -dev-tools.yaml Build both BOMs using iascable : iascable build -i 200 -openshift-gitops.yaml iascable build -i 220 -dev-tools.yaml Navigate to output directory and open a helper docker container that contains terraform CLI required to run the automation: cd output ./launch.sh In your docker container, create the required environment variables required to run the terraform automation on your OpenShift cluster: export TF_VAR_config_banner_text = \"Cloud-Native Toolkit\" export TF_VAR_gitops_repo_repo = <GITOPS_REPOSITORY> export TF_VAR_server_url = <OPENSHIFT_SERVER_URL> export TF_VAR_cluster_login_token = <OPENSHIFT_SERVER_TOKEN> Help TF_VAR_server_url is the OpenShift server URL you retrieved in the first step TF_VAR_cluster_login_token is the OpenShift login token you retrieved in the first step Navigate to 200-openshift-gitops terraform module and run the automation: cd 200 -openshift-gitops/terraform/ terraform init terraform apply --auto-approve NOTE By default, when not providing a gitops_repo_host terraform variable, a Gitea instance is being deployed in the cluster to host the GitOps repository, to install the developer tools we'll need to retrieve the provisioned Gitea host, username and token. We'll now need to login to the OpenShift cluster using the oc login command from first step to retrieve Gitea credentials to pass to next terraform module: oc login --token = ${ LOGIN_TOKEN } --server = ${ SERVER_URL } Navigate to 220-dev-tools terraform module and run the automation: cd ../../220-dev-tools/terraform export TF_VAR_gitops_repo_gitea_host = $( oc get route -n gitea gitea -o yaml | yq .spec.host ) export TF_VAR_gitops_repo_gitea_username = $( oc get secret -n gitea gitea-access -o yaml | yq .data.username | base64 -d ) export TF_VAR_gitops_repo_gitea_token = $( oc get secret -n gitea gitea-access -o yaml | yq .data.password | base64 -d ) terraform init terraform apply --auto-approve","title":"On your workstation"},{"location":"devops/cloud-native-toolkit/install-toolkit/#artifactory-initial-setup","text":"Navigate to OpenShift console. Click this icon Click \"Artifactory\" Log in using default username admin and password password . Click \"Get Started\". Reset password following Artifactory requirements, and save it somewhere safe (e.g. 1Password password manager). Then click \"Next\". Paste the URL into the Select Base URL form and remove any trailing context roots, similar to the one in this view. The next page in the wizard is the Configure a Proxy Server page. This is to setup a proxy for external resources. You can click Next to skip this step. The next page in the wizard is the Create Repositories page. Select \"Generic\", then press \"Next\". The next page in the wizard is the Onboarding Complete page. Press \"Finish\". Allow Anonymous Access to Artifactory: Click on the Settings tab on the left menu (the one with the gear icon), and then select Security Obtain the encrypted password: In the Artifactory console, press the \"Welcome, admin\" menu button in the top right corner of the console and select \"Edit profile\" In the User Profile: admin page, enter you Artifactory password and press Unlock : Below, in the Authentication Settings section, is the Encrypted Password field Press the Eye icon to view the encrypted password and press the Cut & Paste icon to copy it: In the OpenShift 4 console, go to Administrator > Workloads > Secrets . At the top, select the tools project and filter for artifactory . Select Edit Secret on artifactory-access . Add a key/value for ARTIFACTORY_ENCRYPT and set the value to your encrypt key value:","title":"Artifactory initial setup"},{"location":"devops/cloud-native-toolkit/install-toolkit/#sonarqube-initial-setup","text":"Navigate to OpenShift console. Click this icon Click \"Sonarqube\" Log in using default username admin and password admin . Reset the default admin password by putting a custom one, save it for next step. Create the sonarqube-access secret in OpenShift tools namespace with your newly created admin password: oc create secret generic sonarqube-access -n tools --from-literal = SONARQUBE_USER = admin --from-literal = SONARQUBE_PASSWORD = ${ SONARQUBE_PASSWORD }","title":"Sonarqube initial setup"},{"location":"devops/cloud-native-toolkit/install-toolkit/#add-setfcap-capability-to-pipeline-security-context-constraints","text":"Note : This Step is only required if you are running the Toolkit on OpenShift 4.10+ with Openshift Pipelines operator < 1.7.2 . Add SETFCAP capability in allowed capability to pipelines-scc so cluster tasks can request it: oc patch scc pipelines-scc --type merge -p '{\"allowedCapabilities\":[\"SETFCAP\"]}' Update ibm-build-tag-push tekton task in tools namespace to request required SETFCAP capability, e.g. with task version 3.0.3 : oc get task ibm-build-tag-push-v3-0-3 -n tools -o yaml | yq '.spec.steps[2].securityContext.capabilities.add = [\"SETFCAP\"]' | oc apply -f -","title":"Add SETFCAP capability to pipeline Security Context Constraints"},{"location":"devops/cloud-native-toolkit/install-toolkit/#conclusion","text":"At this stage the Cloud-Native Toolkit should be up and running, congratulations!","title":"Conclusion"},{"location":"kubernetes/install/k3s/","text":"Kubernetes install - K3s Main docs: https://k3s.io/ https://docs.k3s.io/quick-start Master Node Run the K3s install script from master node: curl -sfL https://get.k3s.io | sh Once the install is done, fetch node token to add nodes to the cluster and copy it for next step: sudo cat /var/lib/rancher/k3s/server/node-token K100f4662d45c9160374695...dbcc53a11::server:b108c...bcb17c Worker Node(s) Run the K3s install script from each worker node: curl -sfL https://get.k3s.io | K3S_URL = https:// ${ K3S_MASTER_NODE_IP } :6443 K3S_TOKEN = ${ K3S_NODE_TOKEN } sh - Access Cluster Once cluster has been deployed, run following from your local workstation to copy kube config file from master node: cd $HOME /.kube echo \"get /etc/rancher/k3s/k3s.yaml ${ CLUSTER_NAME } \" | sftp -s \"sudo /usr/lib/openssh/sftp-server\" ${ MASTER_NODE_USERNAME } @ ${ MASTER_NODE_IP } server = \"https:// ${ MASTER_NODE_IP } :6443\" yq -i \".clusters[0].cluster.server = env(server)\" ${ CLUSTER_NAME } export KUBECONFIG = $HOME /.kube/ ${ CLUSTER_NAME } kubectl get nodes","title":"Kubernetes install - K3s"},{"location":"kubernetes/install/k3s/#kubernetes-install-k3s","text":"Main docs: https://k3s.io/ https://docs.k3s.io/quick-start","title":"Kubernetes install - K3s"},{"location":"kubernetes/install/k3s/#master-node","text":"Run the K3s install script from master node: curl -sfL https://get.k3s.io | sh Once the install is done, fetch node token to add nodes to the cluster and copy it for next step: sudo cat /var/lib/rancher/k3s/server/node-token K100f4662d45c9160374695...dbcc53a11::server:b108c...bcb17c","title":"Master Node"},{"location":"kubernetes/install/k3s/#worker-nodes","text":"Run the K3s install script from each worker node: curl -sfL https://get.k3s.io | K3S_URL = https:// ${ K3S_MASTER_NODE_IP } :6443 K3S_TOKEN = ${ K3S_NODE_TOKEN } sh -","title":"Worker Node(s)"},{"location":"kubernetes/install/k3s/#access-cluster","text":"Once cluster has been deployed, run following from your local workstation to copy kube config file from master node: cd $HOME /.kube echo \"get /etc/rancher/k3s/k3s.yaml ${ CLUSTER_NAME } \" | sftp -s \"sudo /usr/lib/openssh/sftp-server\" ${ MASTER_NODE_USERNAME } @ ${ MASTER_NODE_IP } server = \"https:// ${ MASTER_NODE_IP } :6443\" yq -i \".clusters[0].cluster.server = env(server)\" ${ CLUSTER_NAME } export KUBECONFIG = $HOME /.kube/ ${ CLUSTER_NAME } kubectl get nodes","title":"Access Cluster"},{"location":"kubernetes/install/kubespray/","text":"Kubernetes install - Kubespray Main docs: https://kubespray.io/#/docs/getting-started https://kubernetes.io/docs/setup/production-environment/tools/kubespray/ Prerequisites https://kubernetes.io/docs/setup/production-environment/tools/kubespray/#1-5-meet-the-underlay-requirements Inventory file https://kubernetes.io/docs/setup/production-environment/tools/kubespray/#2-5-compose-an-inventory-file Inside bastion: git clone https://github.com/kubernetes-sigs/kubespray cd kubespray export CLUSTER_NAME = \"CLUSTER_NAME\" cp -r inventory/sample inventory/ ${ CLUSTER_NAME } declare -a IPS =( 10 .0.0.11 10 .0.0.12 10 .0.0.13 10 .0.0.14 10 .0.0.15 10 .0.0.16 ) CONFIG_FILE = inventory/ ${ CLUSTER_NAME } /hosts.yml python3 contrib/inventory_builder/inventory.py ${ IPS [@] } Configure Deployment https://kubernetes.io/docs/setup/production-environment/tools/kubespray/#3-5-plan-your-cluster-deployment Edit inventory/${CLUSTER_NAME}/group_vars/all/all.yml to update following config variables: cloud_provider: \"external\" external_cloud_provider: \"vsphere\" Edit inventory/${CLUSTER_NAME}/group_vars/all/vsphere.yml to update following config variables: ## Values for the external vSphere Cloud Provider external_vsphere_vcenter_ip: \"vcr72.example.com\" external_vsphere_vcenter_port: \"443\" external_vsphere_insecure: \"true\" external_vsphere_user: \"<VSPHERE_USERNAME>\" external_vsphere_password: \"<VSPHERE_PASSWORD>\" external_vsphere_datacenter: \"TEST\" external_vsphere_kubernetes_cluster_id: \"<CLUSTER_NAME>\" ## Vsphere version where located VMs external_vsphere_version: \"7.0.3\" ## To use vSphere CSI plugin to provision volumes set this value to true vsphere_csi_enabled: false vsphere_csi_controller_replicas: 1 Deploy cluster https://kubernetes.io/docs/setup/production-environment/tools/kubespray/#4-5-deploy-a-cluster ansible-playbook -i inventory/ ${ CLUSTER_NAME } /hosts.yml -b -v cluster.yml Access Cluster Once cluster has been deployed, run following from your local machine to copy kube config file: cd $HOME /.kube echo \"get /root/.kube/config ${ CLUSTER_NAME } \" | sftp -s \"sudo /usr/lib/openssh/sftp-server\" ${ MASTER_NODE_USERNAME } @ ${ MASTER_NODE_IP } server = \"https:// ${ MASTER_NODE_IP } :6443\" yq -i \".clusters[0].cluster.server = env(server)\" ${ CLUSTER_NAME } export KUBECONFIG = $HOME /.kube/ ${ CLUSTER_NAME } kubectl get nodes Known issues VSphere CSI driver We've had an issue with VSphere CSI driver deployment within the cluster, to be looked at... Install without storage option When installing using VSphere external provider without VSphere CSI driver, worker nodes are unschedulable due to a taint added to worker nodes specifying that the node is not initialized nor schedulable even though the nodes seem ready, healthy and schedulable. To fix this issue run the following command for each node: kubectl taint nodes worker-1 node.cloudprovider.kubernetes.io/uninitialized = true:NoSchedule-","title":"Kubernetes install - Kubespray"},{"location":"kubernetes/install/kubespray/#kubernetes-install-kubespray","text":"Main docs: https://kubespray.io/#/docs/getting-started https://kubernetes.io/docs/setup/production-environment/tools/kubespray/","title":"Kubernetes install - Kubespray"},{"location":"kubernetes/install/kubespray/#prerequisites","text":"https://kubernetes.io/docs/setup/production-environment/tools/kubespray/#1-5-meet-the-underlay-requirements","title":"Prerequisites"},{"location":"kubernetes/install/kubespray/#inventory-file","text":"https://kubernetes.io/docs/setup/production-environment/tools/kubespray/#2-5-compose-an-inventory-file Inside bastion: git clone https://github.com/kubernetes-sigs/kubespray cd kubespray export CLUSTER_NAME = \"CLUSTER_NAME\" cp -r inventory/sample inventory/ ${ CLUSTER_NAME } declare -a IPS =( 10 .0.0.11 10 .0.0.12 10 .0.0.13 10 .0.0.14 10 .0.0.15 10 .0.0.16 ) CONFIG_FILE = inventory/ ${ CLUSTER_NAME } /hosts.yml python3 contrib/inventory_builder/inventory.py ${ IPS [@] }","title":"Inventory file"},{"location":"kubernetes/install/kubespray/#configure-deployment","text":"https://kubernetes.io/docs/setup/production-environment/tools/kubespray/#3-5-plan-your-cluster-deployment Edit inventory/${CLUSTER_NAME}/group_vars/all/all.yml to update following config variables: cloud_provider: \"external\" external_cloud_provider: \"vsphere\" Edit inventory/${CLUSTER_NAME}/group_vars/all/vsphere.yml to update following config variables: ## Values for the external vSphere Cloud Provider external_vsphere_vcenter_ip: \"vcr72.example.com\" external_vsphere_vcenter_port: \"443\" external_vsphere_insecure: \"true\" external_vsphere_user: \"<VSPHERE_USERNAME>\" external_vsphere_password: \"<VSPHERE_PASSWORD>\" external_vsphere_datacenter: \"TEST\" external_vsphere_kubernetes_cluster_id: \"<CLUSTER_NAME>\" ## Vsphere version where located VMs external_vsphere_version: \"7.0.3\" ## To use vSphere CSI plugin to provision volumes set this value to true vsphere_csi_enabled: false vsphere_csi_controller_replicas: 1","title":"Configure Deployment"},{"location":"kubernetes/install/kubespray/#deploy-cluster","text":"https://kubernetes.io/docs/setup/production-environment/tools/kubespray/#4-5-deploy-a-cluster ansible-playbook -i inventory/ ${ CLUSTER_NAME } /hosts.yml -b -v cluster.yml","title":"Deploy cluster"},{"location":"kubernetes/install/kubespray/#access-cluster","text":"Once cluster has been deployed, run following from your local machine to copy kube config file: cd $HOME /.kube echo \"get /root/.kube/config ${ CLUSTER_NAME } \" | sftp -s \"sudo /usr/lib/openssh/sftp-server\" ${ MASTER_NODE_USERNAME } @ ${ MASTER_NODE_IP } server = \"https:// ${ MASTER_NODE_IP } :6443\" yq -i \".clusters[0].cluster.server = env(server)\" ${ CLUSTER_NAME } export KUBECONFIG = $HOME /.kube/ ${ CLUSTER_NAME } kubectl get nodes","title":"Access Cluster"},{"location":"kubernetes/install/kubespray/#known-issues","text":"","title":"Known issues"},{"location":"kubernetes/install/kubespray/#vsphere-csi-driver","text":"We've had an issue with VSphere CSI driver deployment within the cluster, to be looked at...","title":"VSphere CSI driver"},{"location":"kubernetes/install/kubespray/#install-without-storage-option","text":"When installing using VSphere external provider without VSphere CSI driver, worker nodes are unschedulable due to a taint added to worker nodes specifying that the node is not initialized nor schedulable even though the nodes seem ready, healthy and schedulable. To fix this issue run the following command for each node: kubectl taint nodes worker-1 node.cloudprovider.kubernetes.io/uninitialized = true:NoSchedule-","title":"Install without storage option"},{"location":"kubernetes/install/rpi/","text":"Self-hosting platform on Raspberry Pi - K3s This doc helps deploy a Kubernetes self-hosting platform on Raspberry Pi devices with K3s . It uses a Terraform modules I have created to deploy the necessary software in the cluster. Roadmap Configure Kubernetes cluster Self-host password manager: Bitwarden Self-host IoT dev platform: Node-RED Self-host home cloud: NextCloud Self-host home Media Center Transmission Flaresolverr Jackett Sonarr Radarr Plex Self-host ads/trackers protection: Pi-Hole Prerequisites Accessible K8s/K3s cluster on your Pi. With cert-manager CustomResourceDefinition installed: kubectl apply -f https://github.com/jetstack/cert-manager/releases/download/v0.16.0/cert-manager.crds.yaml For transmission bittorrent client, an OpenVPN config file stored in openvpn.ignore.ovpn , with auth-user-pass set to /config/openvpn-credentials.txt (auto auth), including cert and key. Usage Clone the repository: $ git clone https://github.com/NoeSamaille/terraform-self-hosting-platform-rpi $ cd terraform-self-hosting-platform-rpi Configure your environment: $ mv terraform.tfvars.template terraform.tfvars $ vim terraform.tfvars Once it's done you can start deploying resources: $ source scripts/init.sh # Generates service passwords $ terraform init $ terraform plan $ terraform apply --auto-approve ... output ommited ... Apply complete! Resources: 32 added, 0 changed, 0 destroyed. To destroy all the resources: $ terraform destroy --auto-approve ... output ommited ... Apply complete! Resources: 0 added, 0 changed, 32 destroyed. How to set up nodes Base pi set up Note : here we'll set up pi-master i.e. our master pi, if you have additional workers (optional) you'll then have to repeat the following steps for each of the workers, replacing references to pi-master by pi-worker-1 , pi-worker-2 , etc. Connect via SSH to the pi: user@workstation $ ssh pi@<PI_IP> ... output ommited ... pi@raspberrypi:~ $ Change password: pi@raspberrypi:~ $ passwd ... output ommited ... passwd: password updated successfully Change host names: pi@raspberrypi:~ $ sudo -i root@raspberrypi:~ $ echo \"pi-master\" > /etc/hostname root@raspberrypi:~ $ sed -i \"s/ $HOSTNAME /pi-master/\" /etc/hosts Enable container features: root@raspberrypi:~ $ sed -i 's/$/ cgroup_enable=cpuset cgroup_memory=1 cgroup_enable=memory/' /boot/cmdline.txt Make sure the system is up-to-date: root@raspberrypi:~ $ apt update && apt upgrade -y Configure a static IP, Note that This could be also done at the network level via the router admin (DHCP): root@raspberrypi:~ $ cat <<EOF >> /etc/dhcpcd.conf interface eth0 static ip_address=<YOUR_STATIC_IP_HERE>/24 static routers=192.168.1.1 static domain_name_servers=1.1.1.1 EOF Reboot: root@raspberrypi:~ $ reboot Wait for a few sec, then connect via SSH to the pi using the new static IP you've just configured: user@workstation $ ssh pi@<PI_IP> ... output ommited ... pi@pi-master:~ $ OPTIONAL: Set up NFS disk share Create NFS Share on Master Pi On master pi, run the command fdisk -l to list all the connected disks to the system (includes the RAM) and try to identify the disk. pi@pi-master:~ $ sudo fdisk -l If your disk is new and freshly out of the package, you will need to create a partition. pi@pi-master:~ $ sudo mkfs.ext4 /dev/sda You can manually mount the disk to the directory /mnt/hdd . pi@pi-master:~ $ sudo mkdir /mnt/hdd pi@pi-master:~ $ sudo chown -R pi:pi /mnt/hdd/ pi@pi-master:~ $ sudo mount /dev/sda /mnt/hdd To automatically mount the disk on startup, you first need to find the Unique ID of the disk using the command blkid : pi@pi-master:~ $ sudo blkid ... output ommited ... /dev/sda: UUID = \"0ac98c2c-8c32-476b-9009-ffca123a2654\" TYPE = \"ext4\" Edit the file /etc/fstab and add the following line to configure auto-mount of the disk on startup: pi@pi-master:~ $ sudo -i root@pi-master:~ $ echo \"UUID=0ac98c2c-8c32-476b-9009-ffca123a2654 /mnt/hdd ext4 defaults 0 0\" >> /etc/fstab root@pi-master:~ $ exit Reboot the system pi@pi-master:~ $ sudo reboot Verify the disk is correctly mounted on startup with the following command: pi@pi-master:~ $ df -ha /dev/sda Filesystem Size Used Avail Use% Mounted on /dev/sda 458G 73M 435G 1 % /mnt/hdd Install the required dependencies: pi@pi-master:~ $ sudo apt install nfs-kernel-server -y Edit the file /etc/exports by running the following command: pi@pi-master:~ $ sudo -i root@pi-master:~ $ echo \"/mnt/hdd-2 *(rw,no_root_squash,insecure,async,no_subtree_check,anonuid=1000,anongid=1000)\" >> /etc/exports root@pi-master:~ $ exit Start the NFS Server: pi@pi-master:~ $ sudo exportfs -ra Mount NFS share on Worker(s) Note : repeat the following steps for each of the workers pi-worker-1 , pi-worker-2 , etc. Install the necessary dependencies: pi@pi-worker-x:~ $ sudo apt install nfs-common -y Create the directory to mount the NFS Share: pi@pi-worker-x:~ $ sudo mkdir /mnt/hdd pi@pi-worker-x:~ $ sudo chown -R pi:pi /mnt/hdd Configure auto-mount of the NFS Share by adding the following line, where <MASTER_IP>:/mnt/hdd is the IP of pi-master followed by the NFS share path: pi@pi-worker-x:~ $ sudo -i root@pi-worker-x:~ $ echo \"<MASTER_IP>:/mnt/hdd /mnt/hdd nfs rw 0 0\" >> /etc/fstab root@pi-worker-x:~ $ exit Reboot the system pi@pi-worker-x:~ $ sudo reboot Optional : to mount manually you can run the following command, where <MASTER_IP>:/mnt/hdd is the IP of pi-master followed by the NFS share path: pi@pi-worker-x:~ $ sudo mount -t nfs <MASTER_IP>:/mnt/hdd /mnt/hdd Setup K3s Start K3s on Master pi pi@pi-master:~ $ curl -sfL https://get.k3s.io | K3S_KUBECONFIG_MODE = \"644\" INSTALL_K3S_EXEC = \" --no-deploy servicelb --no-deploy traefik\" sh - Register workers Get K3s token on master pi, copy the result: pi@pi-master:~ $ sudo cat /var/lib/rancher/k3s/server/node-token K103166a17...eebca269271 Run K3s installer on worker (repeat on each worker): pi@pi-worker-x:~ $ curl -sfL https://get.k3s.io | K3S_KUBECONFIG_MODE = \"644\" K3S_URL = \"https://<MASTER_IP>:6443\" K3S_TOKEN = \"K103166a17...eebca269271\" sh - Access K3s cluster from workstation Copy kube config file from master pi: user@workstation:~ $ scp pi@<MASTER_IP>:/etc/rancher/k3s/k3s.yaml ~/.kube/config Edit kube config file to replace 127.0.0.1 with <MASTER_IP> : user@workstation:~ $ vim ~/.kube/config Test everything by running a kubectl command: user@workstation:~ $ kubectl get nodes -o wide Tear down K3s Worker(s) user@workstation:~ $ sudo /usr/local/bin/k3s-agent-uninstall.sh Master pi@pi-master:~ $ sudo /usr/local/bin/k3s-uninstall.sh Known issues Node-RED authentication Node-RED authentication isn't set up by default atm, you can set it up by scaling the deployment down, editing the settings.js file to enable authentication and scaling the deployment back up: pi@pi-master:~ $ kubectl scale deployment/node-red --replicas=0 -n node-red pi@pi-master:~ $ vim /path/to/node-red/settings.js pi@pi-master:~ $ kubectl scale deployment/node-red --replicas=1 -n node-red You can either set up authentication through GitHub ( Documentation ): # settings . js ... Ommited ... adminAuth : require ( 'node-red-auth-github' )({ clientID : \"<GITHUB_CLIENT_ID>\" , clientSecret : \"<GITHUB_CLIENT_SECRET>\" , baseURL : \"https://node-red.<DOMAIN>/\" , users : [ { username : \"<GITHUB_USERNAME>\" , permissions : [ \"*\" ]} ] }), ... Ommited ... Or classic user-pass authentication (generate a password hash using node -e \"console.log(require('bcryptjs').hashSync(process.argv[1], 8));\" <your-password-here> ): # settings . js ... Ommited ... adminAuth : { type : \"credentials\" , users : [ { username : \"admin\" , password : \"$2a$08$zZWtXTja0fB1pzD4sHCMyOCMYz2Z6dNbM6tl8sJogENOMcxWV9DN.\" , permissions : \"*\" }, { username : \"guest\" , password : \"$2b$08$wuAqPiKJlVN27eF5qJp.RuQYuy6ZYONW7a/UWYxDTtwKFCdB8F19y\" , permissions : \"read\" } ] }, ... Ommited ... More information in the Docs: Securing Node-RED .","title":"Self-hosting platform on Raspberry Pi - K3s"},{"location":"kubernetes/install/rpi/#self-hosting-platform-on-raspberry-pi-k3s","text":"This doc helps deploy a Kubernetes self-hosting platform on Raspberry Pi devices with K3s . It uses a Terraform modules I have created to deploy the necessary software in the cluster.","title":"Self-hosting platform on Raspberry Pi - K3s"},{"location":"kubernetes/install/rpi/#roadmap","text":"Configure Kubernetes cluster Self-host password manager: Bitwarden Self-host IoT dev platform: Node-RED Self-host home cloud: NextCloud Self-host home Media Center Transmission Flaresolverr Jackett Sonarr Radarr Plex Self-host ads/trackers protection: Pi-Hole","title":"Roadmap"},{"location":"kubernetes/install/rpi/#prerequisites","text":"Accessible K8s/K3s cluster on your Pi. With cert-manager CustomResourceDefinition installed: kubectl apply -f https://github.com/jetstack/cert-manager/releases/download/v0.16.0/cert-manager.crds.yaml For transmission bittorrent client, an OpenVPN config file stored in openvpn.ignore.ovpn , with auth-user-pass set to /config/openvpn-credentials.txt (auto auth), including cert and key.","title":"Prerequisites"},{"location":"kubernetes/install/rpi/#usage","text":"Clone the repository: $ git clone https://github.com/NoeSamaille/terraform-self-hosting-platform-rpi $ cd terraform-self-hosting-platform-rpi Configure your environment: $ mv terraform.tfvars.template terraform.tfvars $ vim terraform.tfvars Once it's done you can start deploying resources: $ source scripts/init.sh # Generates service passwords $ terraform init $ terraform plan $ terraform apply --auto-approve ... output ommited ... Apply complete! Resources: 32 added, 0 changed, 0 destroyed. To destroy all the resources: $ terraform destroy --auto-approve ... output ommited ... Apply complete! Resources: 0 added, 0 changed, 32 destroyed.","title":"Usage"},{"location":"kubernetes/install/rpi/#how-to-set-up-nodes","text":"","title":"How to set up nodes"},{"location":"kubernetes/install/rpi/#base-pi-set-up","text":"Note : here we'll set up pi-master i.e. our master pi, if you have additional workers (optional) you'll then have to repeat the following steps for each of the workers, replacing references to pi-master by pi-worker-1 , pi-worker-2 , etc. Connect via SSH to the pi: user@workstation $ ssh pi@<PI_IP> ... output ommited ... pi@raspberrypi:~ $ Change password: pi@raspberrypi:~ $ passwd ... output ommited ... passwd: password updated successfully Change host names: pi@raspberrypi:~ $ sudo -i root@raspberrypi:~ $ echo \"pi-master\" > /etc/hostname root@raspberrypi:~ $ sed -i \"s/ $HOSTNAME /pi-master/\" /etc/hosts Enable container features: root@raspberrypi:~ $ sed -i 's/$/ cgroup_enable=cpuset cgroup_memory=1 cgroup_enable=memory/' /boot/cmdline.txt Make sure the system is up-to-date: root@raspberrypi:~ $ apt update && apt upgrade -y Configure a static IP, Note that This could be also done at the network level via the router admin (DHCP): root@raspberrypi:~ $ cat <<EOF >> /etc/dhcpcd.conf interface eth0 static ip_address=<YOUR_STATIC_IP_HERE>/24 static routers=192.168.1.1 static domain_name_servers=1.1.1.1 EOF Reboot: root@raspberrypi:~ $ reboot Wait for a few sec, then connect via SSH to the pi using the new static IP you've just configured: user@workstation $ ssh pi@<PI_IP> ... output ommited ... pi@pi-master:~ $","title":"Base pi set up"},{"location":"kubernetes/install/rpi/#optional-set-up-nfs-disk-share","text":"","title":"OPTIONAL: Set up NFS disk share"},{"location":"kubernetes/install/rpi/#create-nfs-share-on-master-pi","text":"On master pi, run the command fdisk -l to list all the connected disks to the system (includes the RAM) and try to identify the disk. pi@pi-master:~ $ sudo fdisk -l If your disk is new and freshly out of the package, you will need to create a partition. pi@pi-master:~ $ sudo mkfs.ext4 /dev/sda You can manually mount the disk to the directory /mnt/hdd . pi@pi-master:~ $ sudo mkdir /mnt/hdd pi@pi-master:~ $ sudo chown -R pi:pi /mnt/hdd/ pi@pi-master:~ $ sudo mount /dev/sda /mnt/hdd To automatically mount the disk on startup, you first need to find the Unique ID of the disk using the command blkid : pi@pi-master:~ $ sudo blkid ... output ommited ... /dev/sda: UUID = \"0ac98c2c-8c32-476b-9009-ffca123a2654\" TYPE = \"ext4\" Edit the file /etc/fstab and add the following line to configure auto-mount of the disk on startup: pi@pi-master:~ $ sudo -i root@pi-master:~ $ echo \"UUID=0ac98c2c-8c32-476b-9009-ffca123a2654 /mnt/hdd ext4 defaults 0 0\" >> /etc/fstab root@pi-master:~ $ exit Reboot the system pi@pi-master:~ $ sudo reboot Verify the disk is correctly mounted on startup with the following command: pi@pi-master:~ $ df -ha /dev/sda Filesystem Size Used Avail Use% Mounted on /dev/sda 458G 73M 435G 1 % /mnt/hdd Install the required dependencies: pi@pi-master:~ $ sudo apt install nfs-kernel-server -y Edit the file /etc/exports by running the following command: pi@pi-master:~ $ sudo -i root@pi-master:~ $ echo \"/mnt/hdd-2 *(rw,no_root_squash,insecure,async,no_subtree_check,anonuid=1000,anongid=1000)\" >> /etc/exports root@pi-master:~ $ exit Start the NFS Server: pi@pi-master:~ $ sudo exportfs -ra","title":"Create NFS Share on Master Pi"},{"location":"kubernetes/install/rpi/#mount-nfs-share-on-workers","text":"Note : repeat the following steps for each of the workers pi-worker-1 , pi-worker-2 , etc. Install the necessary dependencies: pi@pi-worker-x:~ $ sudo apt install nfs-common -y Create the directory to mount the NFS Share: pi@pi-worker-x:~ $ sudo mkdir /mnt/hdd pi@pi-worker-x:~ $ sudo chown -R pi:pi /mnt/hdd Configure auto-mount of the NFS Share by adding the following line, where <MASTER_IP>:/mnt/hdd is the IP of pi-master followed by the NFS share path: pi@pi-worker-x:~ $ sudo -i root@pi-worker-x:~ $ echo \"<MASTER_IP>:/mnt/hdd /mnt/hdd nfs rw 0 0\" >> /etc/fstab root@pi-worker-x:~ $ exit Reboot the system pi@pi-worker-x:~ $ sudo reboot Optional : to mount manually you can run the following command, where <MASTER_IP>:/mnt/hdd is the IP of pi-master followed by the NFS share path: pi@pi-worker-x:~ $ sudo mount -t nfs <MASTER_IP>:/mnt/hdd /mnt/hdd","title":"Mount NFS share on Worker(s)"},{"location":"kubernetes/install/rpi/#setup-k3s","text":"","title":"Setup K3s"},{"location":"kubernetes/install/rpi/#start-k3s-on-master-pi","text":"pi@pi-master:~ $ curl -sfL https://get.k3s.io | K3S_KUBECONFIG_MODE = \"644\" INSTALL_K3S_EXEC = \" --no-deploy servicelb --no-deploy traefik\" sh -","title":"Start K3s on Master pi"},{"location":"kubernetes/install/rpi/#register-workers","text":"Get K3s token on master pi, copy the result: pi@pi-master:~ $ sudo cat /var/lib/rancher/k3s/server/node-token K103166a17...eebca269271 Run K3s installer on worker (repeat on each worker): pi@pi-worker-x:~ $ curl -sfL https://get.k3s.io | K3S_KUBECONFIG_MODE = \"644\" K3S_URL = \"https://<MASTER_IP>:6443\" K3S_TOKEN = \"K103166a17...eebca269271\" sh -","title":"Register workers"},{"location":"kubernetes/install/rpi/#access-k3s-cluster-from-workstation","text":"Copy kube config file from master pi: user@workstation:~ $ scp pi@<MASTER_IP>:/etc/rancher/k3s/k3s.yaml ~/.kube/config Edit kube config file to replace 127.0.0.1 with <MASTER_IP> : user@workstation:~ $ vim ~/.kube/config Test everything by running a kubectl command: user@workstation:~ $ kubectl get nodes -o wide","title":"Access K3s cluster from workstation"},{"location":"kubernetes/install/rpi/#tear-down-k3s","text":"Worker(s) user@workstation:~ $ sudo /usr/local/bin/k3s-agent-uninstall.sh Master pi@pi-master:~ $ sudo /usr/local/bin/k3s-uninstall.sh","title":"Tear down K3s"},{"location":"kubernetes/install/rpi/#known-issues","text":"","title":"Known issues"},{"location":"kubernetes/install/rpi/#node-red-authentication","text":"Node-RED authentication isn't set up by default atm, you can set it up by scaling the deployment down, editing the settings.js file to enable authentication and scaling the deployment back up: pi@pi-master:~ $ kubectl scale deployment/node-red --replicas=0 -n node-red pi@pi-master:~ $ vim /path/to/node-red/settings.js pi@pi-master:~ $ kubectl scale deployment/node-red --replicas=1 -n node-red You can either set up authentication through GitHub ( Documentation ): # settings . js ... Ommited ... adminAuth : require ( 'node-red-auth-github' )({ clientID : \"<GITHUB_CLIENT_ID>\" , clientSecret : \"<GITHUB_CLIENT_SECRET>\" , baseURL : \"https://node-red.<DOMAIN>/\" , users : [ { username : \"<GITHUB_USERNAME>\" , permissions : [ \"*\" ]} ] }), ... Ommited ... Or classic user-pass authentication (generate a password hash using node -e \"console.log(require('bcryptjs').hashSync(process.argv[1], 8));\" <your-password-here> ): # settings . js ... Ommited ... adminAuth : { type : \"credentials\" , users : [ { username : \"admin\" , password : \"$2a$08$zZWtXTja0fB1pzD4sHCMyOCMYz2Z6dNbM6tl8sJogENOMcxWV9DN.\" , permissions : \"*\" }, { username : \"guest\" , password : \"$2b$08$wuAqPiKJlVN27eF5qJp.RuQYuy6ZYONW7a/UWYxDTtwKFCdB8F19y\" , permissions : \"read\" } ] }, ... Ommited ... More information in the Docs: Securing Node-RED .","title":"Node-RED authentication"},{"location":"kubernetes/ops/aliases/","text":"Useful aliases Useful aliases you can add to your .bashrc or .zshrc for day to day Kubernetes operations: alias k = 'kubectl' # kn $1 to switch current namespace alias kn = 'f() { [ \"$1\" ] && kubectl config set-context --current --namespace $1 || kubectl config view --minify | grep namespace | cut -d\" \" -f6 ; } ; f' # kx $1 to switch current context alias kx = 'f() { [ \"$1\" ] && kubectl config use-context $1 || kubectl config current-context ; } ; f'","title":"Useful aliases"},{"location":"kubernetes/ops/aliases/#useful-aliases","text":"Useful aliases you can add to your .bashrc or .zshrc for day to day Kubernetes operations: alias k = 'kubectl' # kn $1 to switch current namespace alias kn = 'f() { [ \"$1\" ] && kubectl config set-context --current --namespace $1 || kubectl config view --minify | grep namespace | cut -d\" \" -f6 ; } ; f' # kx $1 to switch current context alias kx = 'f() { [ \"$1\" ] && kubectl config use-context $1 || kubectl config current-context ; } ; f'","title":"Useful aliases"},{"location":"kubernetes/ops/cert-manager/","text":"Cert-Manager Cert-Manager is a powerful and extensible X.509 certificate controller for Kubernetes and OpenShift workloads. It will obtain certificates from a variety of Issuers, both popular public Issuers as well as private Issuers, and ensure the certificates are valid and up-to-date, and will attempt to renew certificates at a configured time before expiry. Prerequisites Kubernetes cluster accessible with kubectl CLI Helm CLI CFSSL CLI Install (Helm) helm repo add jetstack https://charts.jetstack.io helm repo update jetstack helm install \\ cert-manager jetstack/cert-manager \\ --namespace cert-manager \\ --create-namespace \\ --version v1.11.0 \\ --set installCRDs = true Self-Signed cluster issuer kubectl apply -f - <<EOF apiVersion : cert-manager.io/v1 kind : ClusterIssuer metadata : name : selfsigned-cluster-issuer spec : selfSigned : {} EOF You can then use it as follow: # example-ingress.yaml apiVersion : networking.k8s.io/v1 kind : Ingress metadata : name : kuard annotations : kubernetes.io/ingress.class : \"nginx\" cert-manager.io/issuer : \"selfsigned-cluster-issuer\" spec : tls : - hosts : - example.example.com secretName : quickstart-example-tls rules : - host : example.example.com http : paths : - path : / pathType : Prefix backend : service : name : kuard port : number : 80","title":"Cert-Manager"},{"location":"kubernetes/ops/cert-manager/#cert-manager","text":"Cert-Manager is a powerful and extensible X.509 certificate controller for Kubernetes and OpenShift workloads. It will obtain certificates from a variety of Issuers, both popular public Issuers as well as private Issuers, and ensure the certificates are valid and up-to-date, and will attempt to renew certificates at a configured time before expiry.","title":"Cert-Manager"},{"location":"kubernetes/ops/cert-manager/#prerequisites","text":"Kubernetes cluster accessible with kubectl CLI Helm CLI CFSSL CLI","title":"Prerequisites"},{"location":"kubernetes/ops/cert-manager/#install-helm","text":"helm repo add jetstack https://charts.jetstack.io helm repo update jetstack helm install \\ cert-manager jetstack/cert-manager \\ --namespace cert-manager \\ --create-namespace \\ --version v1.11.0 \\ --set installCRDs = true","title":"Install (Helm)"},{"location":"kubernetes/ops/cert-manager/#self-signed-cluster-issuer","text":"kubectl apply -f - <<EOF apiVersion : cert-manager.io/v1 kind : ClusterIssuer metadata : name : selfsigned-cluster-issuer spec : selfSigned : {} EOF You can then use it as follow: # example-ingress.yaml apiVersion : networking.k8s.io/v1 kind : Ingress metadata : name : kuard annotations : kubernetes.io/ingress.class : \"nginx\" cert-manager.io/issuer : \"selfsigned-cluster-issuer\" spec : tls : - hosts : - example.example.com secretName : quickstart-example-tls rules : - host : example.example.com http : paths : - path : / pathType : Prefix backend : service : name : kuard port : number : 80","title":"Self-Signed cluster issuer"},{"location":"kubernetes/ops/docker-registry/","text":"Docker Registry This document shows how to deploy an on-prem private Docker Registry on Kubernetes, using Helm. Prerequisites Kubernetes cluster accessible with kubectl CLI Helm CLI CFSSL CLI (at least cfssl and cfssljson ). htpasswd CLI Nginx Ingress Controller installed in the cluster Some LB (e.g. HAProxy) is set up to balance registry.<BASE_DOMAIN> to cluster Nginx Ingress Controller load balancer. Generate TLS certs Create a working directory $HOME/docker-registry : mkdir $HOME /docker-registry cd $HOME /docker-registry Get the Docker Registry Helm repository: helm repo add twuni https://twuni.github.io/docker-registry.helm helm repo update twuni Create a script registry-tls.sh that will generate TLS CA, certificate and corresponding secrets: touch registry-tls.sh Put the following content: #!/bin/bash set -e ############# ## setup environment NAMESPACE = ${ NAMESPACE :- docker -registry } RELEASE = ${ RELEASE :- registry } DOMAIN = ${ DOMAIN :- apps .k8s.example.com } ## stop if variable is unset beyond this point set -u ## known expected patterns for SAN CERT_SANS = \"*. ${ RELEASE } . ${ DOMAIN } ,*. ${ DOMAIN } \" ############# ## generate default CA config cfssl print-defaults config > ca-config.json ## generate a CA echo '{\"CN\":\"' ${ RELEASE } . ${ DOMAIN } .ca '\",\"key\":{\"algo\":\"ecdsa\",\"size\":256}}' | \\ cfssl gencert -initca - | \\ cfssljson -bare ca - ## generate certificate echo '{\"CN\":\"' ${ RELEASE } . ${ DOMAIN } '\",\"key\":{\"algo\":\"ecdsa\",\"size\":256}}' | \\ cfssl gencert -config = ca-config.json -ca = ca.pem -ca-key = ca-key.pem -profile www -hostname = \" ${ CERT_SANS } \" - | \\ cfssljson -bare ${ RELEASE } ############# ## load certificates into K8s kubectl delete secret ${ RELEASE } -tls -n ${ NAMESPACE } kubectl delete secret ${ RELEASE } -tls-ca -n ${ NAMESPACE } kubectl -n ${ NAMESPACE } create secret tls ${ RELEASE } -tls \\ --cert = ${ RELEASE } .pem \\ --key = ${ RELEASE } -key.pem kubectl -n ${ NAMESPACE } create secret generic ${ RELEASE } -tls-ca \\ --from-file = ${ RELEASE } . ${ DOMAIN } .crt = ca.pem Run the script, provide a valid base DOMAIN e.g. apps.k8s.example.com : chmod u+x registry-tls.sh export DOMAIN = \"<YOUR_DOMAIN>\" ./registry-tls.sh Configure and Deploy Helm Chart Generate default user and password for registry, provide valid USERNAME and PASSWORD : htpasswd -Bbn ${ USERNAME } ${ PASSWORD } > .htpasswd Install Helm Chart: helm upgrade --install registry twuni/docker-registry -n docker-registry --create-namespace --set secrets.htpasswd = $( cat .htpasswd ) Configure Ingress Create Ingress file, provide valid TLS hosts and rule host : # $HOME/docker-registry/ingress.yaml apiVersion : networking.k8s.io/v1 kind : Ingress metadata : name : registry-ingress namespace : docker-registry annotations : nginx.ingress.kubernetes.io/proxy-body-size : \"0\" nginx.ingress.kubernetes.io/proxy-read-timeout : \"600\" nginx.ingress.kubernetes.io/proxy-send-timeout : \"600\" spec : ingressClassName : nginx tls : - hosts : - registry.apps.k8s.example.com # change me secretName : registry-tls rules : - host : registry.apps.k8s.example.com # change me http : paths : - path : / pathType : Prefix backend : service : name : registry-docker-registry port : number : 5000 Apply ingress configuration: kubectl apply -f ingress.yaml Optional : Registry access from external K8s cluster On each worker node of your K8s cluster that will need access to this private registry, put the content of your ca.pem in a file /usr/local/share/ca-certificates/private-docker-registry.crt and restart your container runtime e.g. on Ubuntu with docker runtime: sudo su - cat <<EOF > /usr/local/share/ca-certificates/private-docker-registry.crt -----BEGIN CERTIFICATE----- ... -----END CERTIFICATE----- EOF sudo update-ca-certificates sudo systemctl restart docker","title":"Docker Registry"},{"location":"kubernetes/ops/docker-registry/#docker-registry","text":"This document shows how to deploy an on-prem private Docker Registry on Kubernetes, using Helm.","title":"Docker Registry"},{"location":"kubernetes/ops/docker-registry/#prerequisites","text":"Kubernetes cluster accessible with kubectl CLI Helm CLI CFSSL CLI (at least cfssl and cfssljson ). htpasswd CLI Nginx Ingress Controller installed in the cluster Some LB (e.g. HAProxy) is set up to balance registry.<BASE_DOMAIN> to cluster Nginx Ingress Controller load balancer.","title":"Prerequisites"},{"location":"kubernetes/ops/docker-registry/#generate-tls-certs","text":"Create a working directory $HOME/docker-registry : mkdir $HOME /docker-registry cd $HOME /docker-registry Get the Docker Registry Helm repository: helm repo add twuni https://twuni.github.io/docker-registry.helm helm repo update twuni Create a script registry-tls.sh that will generate TLS CA, certificate and corresponding secrets: touch registry-tls.sh Put the following content: #!/bin/bash set -e ############# ## setup environment NAMESPACE = ${ NAMESPACE :- docker -registry } RELEASE = ${ RELEASE :- registry } DOMAIN = ${ DOMAIN :- apps .k8s.example.com } ## stop if variable is unset beyond this point set -u ## known expected patterns for SAN CERT_SANS = \"*. ${ RELEASE } . ${ DOMAIN } ,*. ${ DOMAIN } \" ############# ## generate default CA config cfssl print-defaults config > ca-config.json ## generate a CA echo '{\"CN\":\"' ${ RELEASE } . ${ DOMAIN } .ca '\",\"key\":{\"algo\":\"ecdsa\",\"size\":256}}' | \\ cfssl gencert -initca - | \\ cfssljson -bare ca - ## generate certificate echo '{\"CN\":\"' ${ RELEASE } . ${ DOMAIN } '\",\"key\":{\"algo\":\"ecdsa\",\"size\":256}}' | \\ cfssl gencert -config = ca-config.json -ca = ca.pem -ca-key = ca-key.pem -profile www -hostname = \" ${ CERT_SANS } \" - | \\ cfssljson -bare ${ RELEASE } ############# ## load certificates into K8s kubectl delete secret ${ RELEASE } -tls -n ${ NAMESPACE } kubectl delete secret ${ RELEASE } -tls-ca -n ${ NAMESPACE } kubectl -n ${ NAMESPACE } create secret tls ${ RELEASE } -tls \\ --cert = ${ RELEASE } .pem \\ --key = ${ RELEASE } -key.pem kubectl -n ${ NAMESPACE } create secret generic ${ RELEASE } -tls-ca \\ --from-file = ${ RELEASE } . ${ DOMAIN } .crt = ca.pem Run the script, provide a valid base DOMAIN e.g. apps.k8s.example.com : chmod u+x registry-tls.sh export DOMAIN = \"<YOUR_DOMAIN>\" ./registry-tls.sh","title":"Generate TLS certs"},{"location":"kubernetes/ops/docker-registry/#configure-and-deploy-helm-chart","text":"Generate default user and password for registry, provide valid USERNAME and PASSWORD : htpasswd -Bbn ${ USERNAME } ${ PASSWORD } > .htpasswd Install Helm Chart: helm upgrade --install registry twuni/docker-registry -n docker-registry --create-namespace --set secrets.htpasswd = $( cat .htpasswd )","title":"Configure and Deploy Helm Chart"},{"location":"kubernetes/ops/docker-registry/#configure-ingress","text":"Create Ingress file, provide valid TLS hosts and rule host : # $HOME/docker-registry/ingress.yaml apiVersion : networking.k8s.io/v1 kind : Ingress metadata : name : registry-ingress namespace : docker-registry annotations : nginx.ingress.kubernetes.io/proxy-body-size : \"0\" nginx.ingress.kubernetes.io/proxy-read-timeout : \"600\" nginx.ingress.kubernetes.io/proxy-send-timeout : \"600\" spec : ingressClassName : nginx tls : - hosts : - registry.apps.k8s.example.com # change me secretName : registry-tls rules : - host : registry.apps.k8s.example.com # change me http : paths : - path : / pathType : Prefix backend : service : name : registry-docker-registry port : number : 5000 Apply ingress configuration: kubectl apply -f ingress.yaml","title":"Configure Ingress"},{"location":"kubernetes/ops/docker-registry/#optional-registry-access-from-external-k8s-cluster","text":"On each worker node of your K8s cluster that will need access to this private registry, put the content of your ca.pem in a file /usr/local/share/ca-certificates/private-docker-registry.crt and restart your container runtime e.g. on Ubuntu with docker runtime: sudo su - cat <<EOF > /usr/local/share/ca-certificates/private-docker-registry.crt -----BEGIN CERTIFICATE----- ... -----END CERTIFICATE----- EOF sudo update-ca-certificates sudo systemctl restart docker","title":"Optional: Registry access from external K8s cluster"},{"location":"kubernetes/ops/nfs-provisioner/","text":"NFS Provisioner Note Tested with K3S Setup NFS Server Create a NFS data mount (optional but recommended) Note Setup based on Ubuntu 20.04 using LVM Add a disk to the VM Create a LVM partition on the new disk (GPT) sudo cfdisk /dev/sdb Create PV/VG/LV and format the LV # Create the Physical Volume with the newly created partition sudo pvcreate /dev/sdb1 sudo pvdisplay # Create a Volume Group with the newly PV sudo vgcreate data-vg /dev/sdb1 sudo vgdisplay # # Create a Logcal Volumein the new VG sudo lvdisplay sudo lvcreate -l 100 %FREE -n data-lv data-vg #Format the new LV sudo mkfs.ext4 /dev/data-vg/data-lv Mount the new device # Create a folder to mount the device sudo mkdir /export sudo chown nobody:nogroup /export sudo chmod 775 /export #Get the LV Path sudo lvdisplay # --- Logical volume --- # LV Path /dev/data-vg/data-lv # ... # Get the device UUID sudo blkid /dev/data-vg/data-lv #/dev/data-vg/data-lv: UUID=\"0dd3c7f9-d42c-4fb3-9bbb-cbe35d6e4f80\" TYPE=\"ext4\" # Add device to fstab for automount sudo vim /etc/fstab # UUID=0dd3c7f9-d42c-4fb3-9bbb-cbe35d6e4f80 /export ext4 defaults 0 1 # Mount everything (allow to check if it will mount or not, and be sure the server will reboot :) ) sudo mount -a #check if it's correctly mounted mount Install NFS Server # Install NFS server package sudo apt install nfs-kernel-server # Add an export sudo vim /etc/exports # /export *(rw,sync,no_subtree_check) sudo exportfs -ra Optional: Configure firewall # Get NFS Server port rpcinfo -p | grep nfs # Open port in firewall sudo ufw allow 2049 sudo ufw status Install NFS client on each Kubernetes node Note Setup based on Ubuntu 20.04 nodes sudo apt install nfs-common -y Deploy the NFS Provisioner https://github.com/kubernetes-sigs/nfs-subdir-external-provisioner Setup based on K3S Create a helmchart-nfs.yaml file: vim helmchart-nfs.yaml apiVersion : helm.cattle.io/v1 kind : HelmChart metadata : name : nfs namespace : nfs-provisioner spec : chart : nfs-subdir-external-provisioner repo : https://kubernetes-sigs.github.io/nfs-subdir-external-provisioner targetNamespace : nfs-provisioner set : nfs.server : 10.3.13.1 nfs.path : /export storageClass.name : nfs Create a new nfs-provisioner namespace and apply the yaml file: kubectl create ns nfs-provisioner kubectl -f helmchart-nfs.yaml -n helmchart-nfs.yaml kubectl get pod -n helmchart-nfs.yaml Setup based on Helm CLI Prerequisites: Install Helm CLI: https://helm.sh/docs/intro/install/ $ kubectl create ns nfs-provisioner $ helm repo add nfs-subdir-external-provisioner https://kubernetes-sigs.github.io/nfs-subdir-external-provisioner/ $ helm install nfs-subdir-external-provisioner nfs-subdir-external-provisioner/nfs-subdir-external-provisioner \\ --set nfs.server = 10 .3.13.1 \\ --set nfs.path = /export \\ --set storageClass.name = nfs \\ -n nfs-provisioner Set default Storage Class kubectl patch sc nfs -p '{\"metadata\": {\"annotations\":{\"storageclass.kubernetes.io/is-default-class\":\"true\"}}}' Test NFS Provisioning Create a Persistent Volume Claim kubectl create ns test-prov cat <<EOF | kubectl apply -n test-prov -f - kind: PersistentVolumeClaim apiVersion: v1 metadata: name: test-claim spec: storageClassName: nfs accessModes: - ReadWriteMany resources: requests: storage: 1Gi EOF Create Pod cat <<EOF | kubectl apply -n test-prov -f - apiVersion: apps/v1 kind: Deployment metadata: name: test-pvc spec: selector: matchLabels: app: test-pvc strategy: type: Recreate template: metadata: labels: app: test-pvc spec: containers: - name: test-pvc-container image: alpine:3.13 command: [ \"/bin/sh\", \"-c\", \"--\" ] args: [ \"while true; do echo $(hostname) $(date) >> /data/test;sleep 1; done;\" ] volumeMounts: - name: vol1 mountPath: \"/data\" volumes: - name: vol1 persistentVolumeClaim: claimName: test-claim EOF","title":"NFS Provisioner"},{"location":"kubernetes/ops/nfs-provisioner/#nfs-provisioner","text":"Note Tested with K3S","title":"NFS Provisioner"},{"location":"kubernetes/ops/nfs-provisioner/#setup-nfs-server","text":"","title":"Setup NFS Server"},{"location":"kubernetes/ops/nfs-provisioner/#create-a-nfs-data-mount-optional-but-recommended","text":"Note Setup based on Ubuntu 20.04 using LVM Add a disk to the VM Create a LVM partition on the new disk (GPT) sudo cfdisk /dev/sdb Create PV/VG/LV and format the LV # Create the Physical Volume with the newly created partition sudo pvcreate /dev/sdb1 sudo pvdisplay # Create a Volume Group with the newly PV sudo vgcreate data-vg /dev/sdb1 sudo vgdisplay # # Create a Logcal Volumein the new VG sudo lvdisplay sudo lvcreate -l 100 %FREE -n data-lv data-vg #Format the new LV sudo mkfs.ext4 /dev/data-vg/data-lv Mount the new device # Create a folder to mount the device sudo mkdir /export sudo chown nobody:nogroup /export sudo chmod 775 /export #Get the LV Path sudo lvdisplay # --- Logical volume --- # LV Path /dev/data-vg/data-lv # ... # Get the device UUID sudo blkid /dev/data-vg/data-lv #/dev/data-vg/data-lv: UUID=\"0dd3c7f9-d42c-4fb3-9bbb-cbe35d6e4f80\" TYPE=\"ext4\" # Add device to fstab for automount sudo vim /etc/fstab # UUID=0dd3c7f9-d42c-4fb3-9bbb-cbe35d6e4f80 /export ext4 defaults 0 1 # Mount everything (allow to check if it will mount or not, and be sure the server will reboot :) ) sudo mount -a #check if it's correctly mounted mount","title":"Create a NFS data mount (optional but recommended)"},{"location":"kubernetes/ops/nfs-provisioner/#install-nfs-server","text":"# Install NFS server package sudo apt install nfs-kernel-server # Add an export sudo vim /etc/exports # /export *(rw,sync,no_subtree_check) sudo exportfs -ra","title":"Install NFS Server"},{"location":"kubernetes/ops/nfs-provisioner/#optional-configure-firewall","text":"# Get NFS Server port rpcinfo -p | grep nfs # Open port in firewall sudo ufw allow 2049 sudo ufw status","title":"Optional: Configure firewall"},{"location":"kubernetes/ops/nfs-provisioner/#install-nfs-client-on-each-kubernetes-node","text":"Note Setup based on Ubuntu 20.04 nodes sudo apt install nfs-common -y","title":"Install NFS client on each Kubernetes node"},{"location":"kubernetes/ops/nfs-provisioner/#deploy-the-nfs-provisioner","text":"https://github.com/kubernetes-sigs/nfs-subdir-external-provisioner","title":"Deploy the NFS Provisioner"},{"location":"kubernetes/ops/nfs-provisioner/#setup-based-on-k3s","text":"Create a helmchart-nfs.yaml file: vim helmchart-nfs.yaml apiVersion : helm.cattle.io/v1 kind : HelmChart metadata : name : nfs namespace : nfs-provisioner spec : chart : nfs-subdir-external-provisioner repo : https://kubernetes-sigs.github.io/nfs-subdir-external-provisioner targetNamespace : nfs-provisioner set : nfs.server : 10.3.13.1 nfs.path : /export storageClass.name : nfs Create a new nfs-provisioner namespace and apply the yaml file: kubectl create ns nfs-provisioner kubectl -f helmchart-nfs.yaml -n helmchart-nfs.yaml kubectl get pod -n helmchart-nfs.yaml","title":"Setup based on K3S"},{"location":"kubernetes/ops/nfs-provisioner/#setup-based-on-helm-cli","text":"Prerequisites: Install Helm CLI: https://helm.sh/docs/intro/install/ $ kubectl create ns nfs-provisioner $ helm repo add nfs-subdir-external-provisioner https://kubernetes-sigs.github.io/nfs-subdir-external-provisioner/ $ helm install nfs-subdir-external-provisioner nfs-subdir-external-provisioner/nfs-subdir-external-provisioner \\ --set nfs.server = 10 .3.13.1 \\ --set nfs.path = /export \\ --set storageClass.name = nfs \\ -n nfs-provisioner","title":"Setup based on Helm CLI"},{"location":"kubernetes/ops/nfs-provisioner/#set-default-storage-class","text":"kubectl patch sc nfs -p '{\"metadata\": {\"annotations\":{\"storageclass.kubernetes.io/is-default-class\":\"true\"}}}'","title":"Set default Storage Class"},{"location":"kubernetes/ops/nfs-provisioner/#test-nfs-provisioning","text":"","title":"Test NFS Provisioning"},{"location":"kubernetes/ops/nfs-provisioner/#create-a-persistent-volume-claim","text":"kubectl create ns test-prov cat <<EOF | kubectl apply -n test-prov -f - kind: PersistentVolumeClaim apiVersion: v1 metadata: name: test-claim spec: storageClassName: nfs accessModes: - ReadWriteMany resources: requests: storage: 1Gi EOF","title":"Create a Persistent Volume Claim"},{"location":"kubernetes/ops/nfs-provisioner/#create-pod","text":"cat <<EOF | kubectl apply -n test-prov -f - apiVersion: apps/v1 kind: Deployment metadata: name: test-pvc spec: selector: matchLabels: app: test-pvc strategy: type: Recreate template: metadata: labels: app: test-pvc spec: containers: - name: test-pvc-container image: alpine:3.13 command: [ \"/bin/sh\", \"-c\", \"--\" ] args: [ \"while true; do echo $(hostname) $(date) >> /data/test;sleep 1; done;\" ] volumeMounts: - name: vol1 mountPath: \"/data\" volumes: - name: vol1 persistentVolumeClaim: claimName: test-claim EOF","title":"Create Pod"},{"location":"kubernetes/ops/nginx-ingress/","text":"NGINX Ingress Controller The NGINX Ingress Controller is an implementation of a Kubernetes Ingress Controller for NGINX. The Ingress is a Kubernetes resource that lets you configure an HTTP load balancer for applications running on Kubernetes, represented by one or more Services. Such a load balancer is necessary to deliver those applications to clients outside of the Kubernetes cluster. The Ingress resource supports Content-based routing and TLS/SSL termination for each hostname. Prerequisites Kubernetes cluster accessible with kubectl CLI Install Helm Install helm upgrade --install ingress-nginx ingress-nginx \\ --repo https://kubernetes.github.io/ingress-nginx \\ --namespace ingress-nginx --create-namespace","title":"NGINX Ingress Controller"},{"location":"kubernetes/ops/nginx-ingress/#nginx-ingress-controller","text":"The NGINX Ingress Controller is an implementation of a Kubernetes Ingress Controller for NGINX. The Ingress is a Kubernetes resource that lets you configure an HTTP load balancer for applications running on Kubernetes, represented by one or more Services. Such a load balancer is necessary to deliver those applications to clients outside of the Kubernetes cluster. The Ingress resource supports Content-based routing and TLS/SSL termination for each hostname.","title":"NGINX Ingress Controller"},{"location":"kubernetes/ops/nginx-ingress/#prerequisites","text":"Kubernetes cluster accessible with kubectl CLI Install Helm","title":"Prerequisites"},{"location":"kubernetes/ops/nginx-ingress/#install","text":"helm upgrade --install ingress-nginx ingress-nginx \\ --repo https://kubernetes.github.io/ingress-nginx \\ --namespace ingress-nginx --create-namespace","title":"Install"},{"location":"kubernetes/ops/plugins/","text":"Useful plugins Useful plugins you can install to speed your day to day Kubernetes operations: Prerequisites kubectl CLI Kubernetes krew plugin manager: ( set -x ; cd \" $( mktemp -d ) \" && OS = \" $( uname | tr '[:upper:]' '[:lower:]' ) \" && ARCH = \" $( uname -m | sed -e 's/x86_64/amd64/' -e 's/\\(arm\\)\\(64\\)\\?.*/\\1\\2/' -e 's/aarch64$/arm64/' ) \" && KREW = \"krew- ${ OS } _ ${ ARCH } \" && curl -fsSLO \"https://github.com/kubernetes-sigs/krew/releases/latest/download/ ${ KREW } .tar.gz\" && tar zxvf \" ${ KREW } .tar.gz\" && ./ \" ${ KREW } \" install krew ) echo 'export PATH=\"${KREW_ROOT:-$HOME/.krew}/bin:$PATH\"' >> ~/.bashrc source ~/.bashrc kubectl ctx : Context switcher kubectl krew install ctx kubectl ctx # List contexts kubectl ctx ${ NEW_CTX } # Switch current context to ${NEW_CTX} kubectl ns : Namespace switcher kubectl krew install ns kubectl ns # List namespaces kubectl ns ${ NEW_NS } # Switch current namespace to ${NEW_NS}","title":"Useful plugins"},{"location":"kubernetes/ops/plugins/#useful-plugins","text":"Useful plugins you can install to speed your day to day Kubernetes operations:","title":"Useful plugins"},{"location":"kubernetes/ops/plugins/#prerequisites","text":"kubectl CLI Kubernetes krew plugin manager: ( set -x ; cd \" $( mktemp -d ) \" && OS = \" $( uname | tr '[:upper:]' '[:lower:]' ) \" && ARCH = \" $( uname -m | sed -e 's/x86_64/amd64/' -e 's/\\(arm\\)\\(64\\)\\?.*/\\1\\2/' -e 's/aarch64$/arm64/' ) \" && KREW = \"krew- ${ OS } _ ${ ARCH } \" && curl -fsSLO \"https://github.com/kubernetes-sigs/krew/releases/latest/download/ ${ KREW } .tar.gz\" && tar zxvf \" ${ KREW } .tar.gz\" && ./ \" ${ KREW } \" install krew ) echo 'export PATH=\"${KREW_ROOT:-$HOME/.krew}/bin:$PATH\"' >> ~/.bashrc source ~/.bashrc","title":"Prerequisites"},{"location":"kubernetes/ops/plugins/#kubectl-ctx-context-switcher","text":"kubectl krew install ctx kubectl ctx # List contexts kubectl ctx ${ NEW_CTX } # Switch current context to ${NEW_CTX}","title":"kubectl ctx: Context switcher"},{"location":"kubernetes/ops/plugins/#kubectl-ns-namespace-switcher","text":"kubectl krew install ns kubectl ns # List namespaces kubectl ns ${ NEW_NS } # Switch current namespace to ${NEW_NS}","title":"kubectl ns: Namespace switcher"},{"location":"kubernetes/software/artifactory/","text":"JFrog Artifactory Prerequisites Kubernetes cluster accessible with kubectl CLI Install Helm Supporting Docs Artifactory Helm Chart Artifactory Get the JFrog Helm repository: helm repo add jfrog https://charts.jfrog.io helm repo update jfrog Install the chart: helm upgrade --install artifactory --namespace artifactory jfrog/artifactory-oss --create-namespace Optional : if running on OpenShift, grant privileged SCC to the default service account in artifactory namespace: oc adm policy add-scc-to-user privileged -z default -n artifactory","title":"JFrog Artifactory"},{"location":"kubernetes/software/artifactory/#jfrog-artifactory","text":"","title":"JFrog Artifactory"},{"location":"kubernetes/software/artifactory/#prerequisites","text":"Kubernetes cluster accessible with kubectl CLI Install Helm","title":"Prerequisites"},{"location":"kubernetes/software/artifactory/#supporting-docs","text":"Artifactory Helm Chart","title":"Supporting Docs"},{"location":"kubernetes/software/artifactory/#artifactory","text":"Get the JFrog Helm repository: helm repo add jfrog https://charts.jfrog.io helm repo update jfrog Install the chart: helm upgrade --install artifactory --namespace artifactory jfrog/artifactory-oss --create-namespace Optional : if running on OpenShift, grant privileged SCC to the default service account in artifactory namespace: oc adm policy add-scc-to-user privileged -z default -n artifactory","title":"Artifactory"},{"location":"kubernetes/software/gitlab/","text":"GitLab EE This document shows how to deploy an on-prem private GitLab Instance on Kubernetes, using Helm. Prerequisites Kubernetes cluster accessible with kubectl CLI Install Helm Install CFSSL CLI (at least cfssl and cfssljson ). Supporting Docs GitLab EE GitLab Runner GitLab Create a working directory $HOME/gitlab : mkdir $HOME/gitlab cd $HOME/gitlab Get the GitLab Helm repository: helm repo add gitlab https://charts.gitlab.io helm repo update gitlab Create a script gitlab-tls.sh that will generate GitLab TLS CA and certificate and corresponding secrets: touch gitlab-tls.sh Put the following content: #!/bin/bash set -e ############# ## setup environment NAMESPACE = ${ NAMESPACE :- gitlab } RELEASE = ${ RELEASE :- gitlab } DOMAIN = ${ DOMAIN :- apps .k8s.example.com } ## stop if variable is unset beyond this point set -u ## known expected patterns for SAN CERT_SANS = \"*. ${ RELEASE } . ${ DOMAIN } ,*. ${ DOMAIN } \" ############# ## generate default CA config cfssl print-defaults config > ca-config.json ## generate a CA echo '{\"CN\":\"' ${ RELEASE } . ${ DOMAIN } .ca '\",\"key\":{\"algo\":\"ecdsa\",\"size\":256}}' | \\ cfssl gencert -initca - | \\ cfssljson -bare ca - ## generate certificate echo '{\"CN\":\"' ${ RELEASE } . ${ DOMAIN } '\",\"key\":{\"algo\":\"ecdsa\",\"size\":256}}' | \\ cfssl gencert -config = ca-config.json -ca = ca.pem -ca-key = ca-key.pem -profile www -hostname = \" ${ CERT_SANS } \" - | \\ cfssljson -bare ${ RELEASE } ############# ## load certificates into K8s kubectl create ns ${ NAMESPACE } kubectl -n ${ NAMESPACE } create secret tls ${ RELEASE } -tls \\ --cert = ${ RELEASE } .pem \\ --key = ${ RELEASE } -key.pem kubectl -n ${ NAMESPACE } create secret generic ${ RELEASE } -tls-ca \\ --from-file = ${ RELEASE } . ${ DOMAIN } .crt = ca.pem Run the script, provide a valid base DOMAIN e.g. apps.k8s.example.com : chmod u+x gitlab-tls.sh export DOMAIN = \"<YOUR_DOMAIN>\" ./gitlab-tls.sh Populate your Helm chart config gitlab.values.yaml like the following, provide a valid domain : # $HOME/gitlab/gitlab.values.yaml global : hosts : domain : apps.k8s.example.com ingress : configureCertmanager : false tls : enabled : true secretName : gitlab-tls certificates : customCAs : - secret : gitlab-tls-ca certmanager : installCRDs : false install : false gitlab-runner : install : false Note : you can check all available configuration by running helm show values gitlab/gitlab . Install Helm chart using your custom configuration: helm install gitlab gitlab/gitlab --timeout = 600s --values gitlab.values.yaml -n gitlab Wait until all pods are running: watch kubectl get pods -n gitlab GitLab Runner Create a sub folder gitlab-runner and move to there: mkdir gitlab-runner cd gitlab-runner Create your GitLab Runner configuration gitlab-runner.values.yaml like the following, provide a valid gitlabUrl and runnerRegistrationToken : # $HOME/gitlab/gitlab-runner/gitlab-runner.values.yaml gitlabUrl : https://gitlab.apps.k8s.example.com runnerRegistrationToken : \"AoDG...31SGP\" certsSecretName : gitlab-tls-ca Note : see how to retrieve your runner registration token . Install Helm chart using your custom configuration: helm install --namespace gitlab gitlab-runner -f gitlab-runner.values.yaml gitlab/gitlab-runner Wait for runner pod to be up and running: watch kubectl get pods -n gitlab","title":"GitLab EE"},{"location":"kubernetes/software/gitlab/#gitlab-ee","text":"This document shows how to deploy an on-prem private GitLab Instance on Kubernetes, using Helm.","title":"GitLab EE"},{"location":"kubernetes/software/gitlab/#prerequisites","text":"Kubernetes cluster accessible with kubectl CLI Install Helm Install CFSSL CLI (at least cfssl and cfssljson ).","title":"Prerequisites"},{"location":"kubernetes/software/gitlab/#supporting-docs","text":"GitLab EE GitLab Runner","title":"Supporting Docs"},{"location":"kubernetes/software/gitlab/#gitlab","text":"Create a working directory $HOME/gitlab : mkdir $HOME/gitlab cd $HOME/gitlab Get the GitLab Helm repository: helm repo add gitlab https://charts.gitlab.io helm repo update gitlab Create a script gitlab-tls.sh that will generate GitLab TLS CA and certificate and corresponding secrets: touch gitlab-tls.sh Put the following content: #!/bin/bash set -e ############# ## setup environment NAMESPACE = ${ NAMESPACE :- gitlab } RELEASE = ${ RELEASE :- gitlab } DOMAIN = ${ DOMAIN :- apps .k8s.example.com } ## stop if variable is unset beyond this point set -u ## known expected patterns for SAN CERT_SANS = \"*. ${ RELEASE } . ${ DOMAIN } ,*. ${ DOMAIN } \" ############# ## generate default CA config cfssl print-defaults config > ca-config.json ## generate a CA echo '{\"CN\":\"' ${ RELEASE } . ${ DOMAIN } .ca '\",\"key\":{\"algo\":\"ecdsa\",\"size\":256}}' | \\ cfssl gencert -initca - | \\ cfssljson -bare ca - ## generate certificate echo '{\"CN\":\"' ${ RELEASE } . ${ DOMAIN } '\",\"key\":{\"algo\":\"ecdsa\",\"size\":256}}' | \\ cfssl gencert -config = ca-config.json -ca = ca.pem -ca-key = ca-key.pem -profile www -hostname = \" ${ CERT_SANS } \" - | \\ cfssljson -bare ${ RELEASE } ############# ## load certificates into K8s kubectl create ns ${ NAMESPACE } kubectl -n ${ NAMESPACE } create secret tls ${ RELEASE } -tls \\ --cert = ${ RELEASE } .pem \\ --key = ${ RELEASE } -key.pem kubectl -n ${ NAMESPACE } create secret generic ${ RELEASE } -tls-ca \\ --from-file = ${ RELEASE } . ${ DOMAIN } .crt = ca.pem Run the script, provide a valid base DOMAIN e.g. apps.k8s.example.com : chmod u+x gitlab-tls.sh export DOMAIN = \"<YOUR_DOMAIN>\" ./gitlab-tls.sh Populate your Helm chart config gitlab.values.yaml like the following, provide a valid domain : # $HOME/gitlab/gitlab.values.yaml global : hosts : domain : apps.k8s.example.com ingress : configureCertmanager : false tls : enabled : true secretName : gitlab-tls certificates : customCAs : - secret : gitlab-tls-ca certmanager : installCRDs : false install : false gitlab-runner : install : false Note : you can check all available configuration by running helm show values gitlab/gitlab . Install Helm chart using your custom configuration: helm install gitlab gitlab/gitlab --timeout = 600s --values gitlab.values.yaml -n gitlab Wait until all pods are running: watch kubectl get pods -n gitlab","title":"GitLab"},{"location":"kubernetes/software/gitlab/#gitlab-runner","text":"Create a sub folder gitlab-runner and move to there: mkdir gitlab-runner cd gitlab-runner Create your GitLab Runner configuration gitlab-runner.values.yaml like the following, provide a valid gitlabUrl and runnerRegistrationToken : # $HOME/gitlab/gitlab-runner/gitlab-runner.values.yaml gitlabUrl : https://gitlab.apps.k8s.example.com runnerRegistrationToken : \"AoDG...31SGP\" certsSecretName : gitlab-tls-ca Note : see how to retrieve your runner registration token . Install Helm chart using your custom configuration: helm install --namespace gitlab gitlab-runner -f gitlab-runner.values.yaml gitlab/gitlab-runner Wait for runner pod to be up and running: watch kubectl get pods -n gitlab","title":"GitLab Runner"},{"location":"kubernetes/software/harbor/","text":"Harbor Harbor is an open source registry that secures artifacts with policies and role-based access control, ensures images are scanned and free from vulnerabilities, and signs images as trusted. Harbor, a CNCF Graduated project, delivers compliance, performance, and interoperability to help you consistently and securely manage artifacts across cloud native compute platforms like Kubernetes and Docker. Prerequisites Kubernetes/OpenShift cluster accessible with kubectl CLI Install Helm yq CLI installed on your workstation Supporting Docs Deploying Harbor with High Availability via Helm Install Harbor Get the Harbor Helm repository: helm repo add harbor https://helm.goharbor.io helm repo update harbor Create harbor namespace: kubectl create ns harbor OpenShift Only : If installing in OpenShift, add the privileged security context constraint to default service account in the harbor namespace: oc adm policy add-scc-to-user privileged -z default -n harbor Install the chart, provide a valid HARBOR_DOMAIN : export HARBOR_DOMAIN = harbor.example.com helm install harbor harbor/harbor -n harbor --create-namespace --set externalURL = https://core. ${ HARBOR_DOMAIN } --set expose.ingress.hosts.core = core. ${ HARBOR_DOMAIN } --set expose.ingress.hosts.notary = notary. ${ HARBOR_DOMAIN } After a successful deployment and if your ingress strategy is properly configured you should be able to access your Harbor instance at https://core.harbor.example.com . To login you will find the admin password by running the following command: kubectl get secret -n harbor harbor-core -o yaml | yq .data.HARBOR_ADMIN_PASSWORD | base64 -d","title":"Harbor"},{"location":"kubernetes/software/harbor/#harbor","text":"Harbor is an open source registry that secures artifacts with policies and role-based access control, ensures images are scanned and free from vulnerabilities, and signs images as trusted. Harbor, a CNCF Graduated project, delivers compliance, performance, and interoperability to help you consistently and securely manage artifacts across cloud native compute platforms like Kubernetes and Docker.","title":"Harbor"},{"location":"kubernetes/software/harbor/#prerequisites","text":"Kubernetes/OpenShift cluster accessible with kubectl CLI Install Helm yq CLI installed on your workstation","title":"Prerequisites"},{"location":"kubernetes/software/harbor/#supporting-docs","text":"Deploying Harbor with High Availability via Helm","title":"Supporting Docs"},{"location":"kubernetes/software/harbor/#install-harbor","text":"Get the Harbor Helm repository: helm repo add harbor https://helm.goharbor.io helm repo update harbor Create harbor namespace: kubectl create ns harbor OpenShift Only : If installing in OpenShift, add the privileged security context constraint to default service account in the harbor namespace: oc adm policy add-scc-to-user privileged -z default -n harbor Install the chart, provide a valid HARBOR_DOMAIN : export HARBOR_DOMAIN = harbor.example.com helm install harbor harbor/harbor -n harbor --create-namespace --set externalURL = https://core. ${ HARBOR_DOMAIN } --set expose.ingress.hosts.core = core. ${ HARBOR_DOMAIN } --set expose.ingress.hosts.notary = notary. ${ HARBOR_DOMAIN } After a successful deployment and if your ingress strategy is properly configured you should be able to access your Harbor instance at https://core.harbor.example.com . To login you will find the admin password by running the following command: kubectl get secret -n harbor harbor-core -o yaml | yq .data.HARBOR_ADMIN_PASSWORD | base64 -d","title":"Install Harbor"},{"location":"kubernetes/software/mysql/","text":"MySQL MySQL is an open-source relational database management system (RDBMS) available under the terms of the GNU General Public License. To deploy MysQL in Kubernetes (or Red Hat OpenShift), we are going to use a Helm chart provided by Bitnami. Bitnami makes it easy to get open source software up and running on any platform, including laptop, Kubernetes and major cloud providers. Prerequisites Kubernetes cluster accessible with kubectl CLI Install Helm Supporting Docs MySQL Helm Chart Install MySQL Get the bitnami Helm repository: helm repo add bitnami https://charts.bitnami.com/bitnami helm repo update bitnami Install the chart (provide a valid ${NAMESPACE} ): helm install mysql bitnami/mysql -n ${ NAMESPACE } Optional : if running on OpenShift, grant privileged SCC to the mysql service account in ${NAMESPACE} namespace: oc adm policy add-scc-to-user privileged -z mysql -n ${ NAMESPACE } Access MySQL instance Retrieve root password from the created mysql by running the following command: kubectl get secret mysql -o yaml | yq .data.mysql-root-password | base64 -d Use port forwarding to access MySQL using kubectl port-forward command: kubectl port-forward mysql-0 3306 :3306 On a second terminal, you can then access your MySQL instance by running the following command (type your root password retrieved in above steps): mysql -h 127 .0.0.1 -p","title":"MySQL"},{"location":"kubernetes/software/mysql/#mysql","text":"MySQL is an open-source relational database management system (RDBMS) available under the terms of the GNU General Public License. To deploy MysQL in Kubernetes (or Red Hat OpenShift), we are going to use a Helm chart provided by Bitnami. Bitnami makes it easy to get open source software up and running on any platform, including laptop, Kubernetes and major cloud providers.","title":"MySQL"},{"location":"kubernetes/software/mysql/#prerequisites","text":"Kubernetes cluster accessible with kubectl CLI Install Helm","title":"Prerequisites"},{"location":"kubernetes/software/mysql/#supporting-docs","text":"MySQL Helm Chart","title":"Supporting Docs"},{"location":"kubernetes/software/mysql/#install-mysql","text":"Get the bitnami Helm repository: helm repo add bitnami https://charts.bitnami.com/bitnami helm repo update bitnami Install the chart (provide a valid ${NAMESPACE} ): helm install mysql bitnami/mysql -n ${ NAMESPACE } Optional : if running on OpenShift, grant privileged SCC to the mysql service account in ${NAMESPACE} namespace: oc adm policy add-scc-to-user privileged -z mysql -n ${ NAMESPACE }","title":"Install MySQL"},{"location":"kubernetes/software/mysql/#access-mysql-instance","text":"Retrieve root password from the created mysql by running the following command: kubectl get secret mysql -o yaml | yq .data.mysql-root-password | base64 -d Use port forwarding to access MySQL using kubectl port-forward command: kubectl port-forward mysql-0 3306 :3306 On a second terminal, you can then access your MySQL instance by running the following command (type your root password retrieved in above steps): mysql -h 127 .0.0.1 -p","title":"Access MySQL instance"},{"location":"kubernetes/software/sonarqube/","text":"SonarQube SonarQube is a self-managed, automatic code review tool that systematically helps you deliver clean code. As a core element of our Sonar solution, SonarQube integrates into your existing workflow and detects issues in your code to help you perform continuous code inspections of your projects. The tool analyses 30+ different programming languages and integrates into your CI pipeline and DevOps platform to ensure that your code meets high-quality standards. Prerequisites Kubernetes cluster accessible with kubectl CLI Install Helm Supporting Docs SonarQube Helm Chart SonarQube Get the bitnami Helm repository: helm repo add bitnami https://charts.bitnami.com/bitnami helm repo update bitnami Install the chart: export SONARQUBE_PASSWORD = <CHANGE_ME> helm install sonarqube bitnami/sonarqube -n sonarqube --create-namespace --set sonarqubePassword = ${ SONARQUBE_PASSWORD } Optional : if running on OpenShift, grant privileged SCC to the default and sonarqube service accounts in sonarqube namespace: oc adm policy add-scc-to-user privileged -z default -n sonarqube oc adm policy add-scc-to-user privileged -z sonarqube -n sonarqube","title":"SonarQube"},{"location":"kubernetes/software/sonarqube/#sonarqube","text":"SonarQube is a self-managed, automatic code review tool that systematically helps you deliver clean code. As a core element of our Sonar solution, SonarQube integrates into your existing workflow and detects issues in your code to help you perform continuous code inspections of your projects. The tool analyses 30+ different programming languages and integrates into your CI pipeline and DevOps platform to ensure that your code meets high-quality standards.","title":"SonarQube"},{"location":"kubernetes/software/sonarqube/#prerequisites","text":"Kubernetes cluster accessible with kubectl CLI Install Helm","title":"Prerequisites"},{"location":"kubernetes/software/sonarqube/#supporting-docs","text":"SonarQube Helm Chart","title":"Supporting Docs"},{"location":"kubernetes/software/sonarqube/#sonarqube_1","text":"Get the bitnami Helm repository: helm repo add bitnami https://charts.bitnami.com/bitnami helm repo update bitnami Install the chart: export SONARQUBE_PASSWORD = <CHANGE_ME> helm install sonarqube bitnami/sonarqube -n sonarqube --create-namespace --set sonarqubePassword = ${ SONARQUBE_PASSWORD } Optional : if running on OpenShift, grant privileged SCC to the default and sonarqube service accounts in sonarqube namespace: oc adm policy add-scc-to-user privileged -z default -n sonarqube oc adm policy add-scc-to-user privileged -z sonarqube -n sonarqube","title":"SonarQube"},{"location":"kubernetes/software/wikijs/","text":"Wiki.js Wiki.js is an open source Wiki software that allows you to enjoy writing documentation using a beautiful and intuitive interface! Prerequisites Kubernetes cluster accessible with kubectl CLI Cert Manager configured in K8s cluster, with letsencrypt-prod ClusterIssuer created. Install Helm Optional : Create custom PostgreSQL DB Note : This is only required for arm based deployment as the default Wiki.js PostgreSQL image is only available for amd64 architectures. helm repo add romanow https://romanow.github.io/helm-charts/ helm repo update helm install wiki-pg romanow/postgres -n wiki --create-namespace Deploy Wiki.js using Helm Edit your wiki.values.yaml file to configure your deployment: vim wiki.values.yaml # wiki.values.yaml ingress : enabled : true annotations : kubernetes.io/ingress.class : nginx kubernetes.io/tls-acme : \"true\" cert-manager.io/cluster-issuer : \"letsencrypt-prod\" hosts : - host : wiki.seillama.dev paths : - path : \"/\" pathType : Prefix tls : - secretName : wiki-tls hosts : - wiki.seillama.dev postgresql : enabled : false postgresqlHost : wiki-pg postgresqlUser : postgres postgresqlDatabase : postgres existingSecret : wiki existingSecretKey : postgresql-password Deploy Wiki.js helm repo add requarks https://charts.js.wiki helm repo update helm install wiki requarks/wiki -n wiki -f wiki.values.yaml","title":"Wiki.js"},{"location":"kubernetes/software/wikijs/#wikijs","text":"Wiki.js is an open source Wiki software that allows you to enjoy writing documentation using a beautiful and intuitive interface!","title":"Wiki.js"},{"location":"kubernetes/software/wikijs/#prerequisites","text":"Kubernetes cluster accessible with kubectl CLI Cert Manager configured in K8s cluster, with letsencrypt-prod ClusterIssuer created. Install Helm","title":"Prerequisites"},{"location":"kubernetes/software/wikijs/#optional-create-custom-postgresql-db","text":"Note : This is only required for arm based deployment as the default Wiki.js PostgreSQL image is only available for amd64 architectures. helm repo add romanow https://romanow.github.io/helm-charts/ helm repo update helm install wiki-pg romanow/postgres -n wiki --create-namespace","title":"Optional: Create custom PostgreSQL DB"},{"location":"kubernetes/software/wikijs/#deploy-wikijs-using-helm","text":"Edit your wiki.values.yaml file to configure your deployment: vim wiki.values.yaml # wiki.values.yaml ingress : enabled : true annotations : kubernetes.io/ingress.class : nginx kubernetes.io/tls-acme : \"true\" cert-manager.io/cluster-issuer : \"letsencrypt-prod\" hosts : - host : wiki.seillama.dev paths : - path : \"/\" pathType : Prefix tls : - secretName : wiki-tls hosts : - wiki.seillama.dev postgresql : enabled : false postgresqlHost : wiki-pg postgresqlUser : postgres postgresqlDatabase : postgres existingSecret : wiki existingSecretKey : postgresql-password Deploy Wiki.js helm repo add requarks https://charts.js.wiki helm repo update helm install wiki requarks/wiki -n wiki -f wiki.values.yaml","title":"Deploy Wiki.js using Helm"},{"location":"linux/ubuntu-trust-ca/","text":"Ubuntu: trust a new Certificate Authority Assuming a PEM-formatted root CA certificate is in local-ca.crt , follow the steps below to install it. Note : It is important to have the .crt extension on the file, otherwise it will not be processed. sudo apt-get install -y ca-certificates sudo cp local-ca.crt /usr/local/share/ca-certificates sudo update-ca-certificates After this point you can use Ubuntu\u2019s tools like curl , wget or docker login to connect to local sites. Source: https://ubuntu.com/server/docs/security-trust-store","title":"Ubuntu: trust a new Certificate Authority"},{"location":"linux/ubuntu-trust-ca/#ubuntu-trust-a-new-certificate-authority","text":"Assuming a PEM-formatted root CA certificate is in local-ca.crt , follow the steps below to install it. Note : It is important to have the .crt extension on the file, otherwise it will not be processed. sudo apt-get install -y ca-certificates sudo cp local-ca.crt /usr/local/share/ca-certificates sudo update-ca-certificates After this point you can use Ubuntu\u2019s tools like curl , wget or docker login to connect to local sites. Source: https://ubuntu.com/server/docs/security-trust-store","title":"Ubuntu: trust a new Certificate Authority"}]}