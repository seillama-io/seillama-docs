{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Welcome to Seillama Docs! I try to document all the technical things I do in my spare time, mostly for my own use. On this site you will find some gists around my areas of interest like Hybrid-Cloud, DevSecOps, Containers, Self-Hosted Software and Machine Learning.","title":"Home"},{"location":"#welcome-to-seillama-docs","text":"I try to document all the technical things I do in my spare time, mostly for my own use. On this site you will find some gists around my areas of interest like Hybrid-Cloud, DevSecOps, Containers, Self-Hosted Software and Machine Learning.","title":"Welcome to Seillama Docs!"},{"location":"ai/cuda-rhel/","text":"Install Nvidia CUDA on RHEL Tested on RHEL8. Master doc: https://docs.nvidia.com/cuda/cuda-installation-guide-linux/index.html Verify You Have a CUDA-Capable GPU: lspci | grep -i nvidia Verify You Have a Supported Version of Linux: uname -m && cat /etc/*release Verify the System Has gcc Installed: gcc --version Verify the System has the Correct Kernel Headers and Development Packages Installed: sudo dnf install kernel-devel- $( uname -r ) kernel-headers- $( uname -r ) Network Repo Installation for RHEL 8: export distro = rhel8 export arch = x86_64 sudo dnf config-manager --add-repo https://developer.download.nvidia.com/compute/cuda/repos/ $distro / $arch /cuda- $distro .repo sudo dnf config-manager --add-repo https://developer.download.nvidia.com/compute/cuda/repos/ $distro / $arch /cuda- $distro .repo sudo dnf clean expire-cache Install CUDA SDK: sudo dnf module install nvidia-driver:latest-dkms sudo dnf install cuda Try it: nvidia-smi","title":"Install Nvidia CUDA on RHEL"},{"location":"ai/cuda-rhel/#install-nvidia-cuda-on-rhel","text":"Tested on RHEL8. Master doc: https://docs.nvidia.com/cuda/cuda-installation-guide-linux/index.html Verify You Have a CUDA-Capable GPU: lspci | grep -i nvidia Verify You Have a Supported Version of Linux: uname -m && cat /etc/*release Verify the System Has gcc Installed: gcc --version Verify the System has the Correct Kernel Headers and Development Packages Installed: sudo dnf install kernel-devel- $( uname -r ) kernel-headers- $( uname -r ) Network Repo Installation for RHEL 8: export distro = rhel8 export arch = x86_64 sudo dnf config-manager --add-repo https://developer.download.nvidia.com/compute/cuda/repos/ $distro / $arch /cuda- $distro .repo sudo dnf config-manager --add-repo https://developer.download.nvidia.com/compute/cuda/repos/ $distro / $arch /cuda- $distro .repo sudo dnf clean expire-cache Install CUDA SDK: sudo dnf module install nvidia-driver:latest-dkms sudo dnf install cuda Try it: nvidia-smi","title":"Install Nvidia CUDA on RHEL"},{"location":"ai/foundation-models/","text":"Foundation models Download this section as a mindmap ! Overview Pre-trained on unlabeled datasets Leverage self-supervised learning Learn generalizable & adaptable data representations Can be effectively used in multiple downstream tasks (e.g., text generation, machine translation, classification for languages) Note : while transformer architecture is most prevalent in foundation models, definition not restricted by model architecture Data Modalities Natural Language Speech Business Data IT Data Sensor Data Chemistry & Materials Geospatial Programming Languages (Code) Images Dialog Architectures Encoder-only Best cost performance trade-off for non generative use cases Most classical NLP tasks: classification, entity and relation extraction, extractive summarization, extractive question answer, etc. Require task-specific labeled data for fine tuning . Examples: BERT/RoBERTa models. Encoder-Decoder Support both generative and non-generative use cases Best cost performance trade-off for generative use cases when input is large but generated output is small. Can be prompt-engineered once we hit a size of ~10B but below that can be fine tuned using labeled data . Examples: Google T5 models, UL2 models. Decoder-only Designed explicitly for generative AI use cases summarization, generative question answer, translation, copywriting Architectures used in GPT-3, ChatGPT, etc. Training and Tuning (\u2b07\ufe0f value) Base model Pre-trained on 10s of TBs of unlabeled Internet data Examples : Watson Studio base LLM models, Google T5, etc. Custom pre-trained model Pre-trained on 10s of GB of domain/industry specific data Examples : IBM-NASA collaboration models, Watson Code Assistant models Fine-tuned model Fine-tuned on a class of tasks Examples : Watson NLP OOTB Entity, Sentiment models, Google FLAN T5, watsonx sandstone.instruct Human-in-the-loop refined model ChatGPT, watsonx sandstone.chat Adaptation to multiple tasks (\u2b07\ufe0f complexity/skills, \u2b06\ufe0f model size) Prompt engineering Training : None Inference : Engineered Prompt + Input Text \u27a1\ufe0f output Designing and constructing effective prompts to obtain desired outputs Recommended way to start Advantages \u2795 Quick experimentation for various tasks Little to no training data Disadvantages \u2796 Success depends on choice of prompt and model size Mostly a trial-and-error process Number of examples limited by prompt input size limitations Lower accuracy compared to fine-tuning Longer prompts may give better accuracy but cost more Prompt-tuning Training : Pre-trained model + Labeled Data \u27a1\ufe0f Prompt-Tuning Algorithm \u27a1\ufe0f Tuned Soft Prompt Inference : Tuned Soft Prompt + Input Text \u27a1\ufe0f Pre-trained model \u27a1\ufe0f output Relatively new technique Training data format is the same as for fine tuning Pre-trained models: LLMs with decoders Advantages \u2795 Faster training as only few parameters are learnt Model accuracy comparable to fine tuning in some cases The Pre-trained model is reused for inference in multiple tasks Middle ground between fine-tuning and prompt engineering Fewer parameters compared to fine tuning Fine-tuning Training : Pre-trained model + Labeled Data \u27a1\ufe0f Fine-Tuning Algorithm \u27a1\ufe0f Fined-Tuned model Inference : Input Text \u27a1\ufe0f Fined-Tuned model \u27a1\ufe0f output \ud83d\udcc8 SotA accuracy with small models and many popular NLP tasks (classification, extraction) Requires data science expertise Requires separate instance of the model for each task (can be expensive) Difficult as model size increases (e.g., overfitting issues) i.e. typically less than 1B parameters Enterprise considerations Head-on comparison with ChatGPT is a trap A single solution does not fit all trust matters ROI determined by use case and inference cost Need to manage risks and limitations of today's LLMs Consider ability to run workloads as desired, train models, provide trusted models, backend integration, enterprise features, and other NFRs","title":"Foundation models"},{"location":"ai/foundation-models/#foundation-models","text":"Download this section as a mindmap !","title":"Foundation models"},{"location":"ai/foundation-models/#overview","text":"Pre-trained on unlabeled datasets Leverage self-supervised learning Learn generalizable & adaptable data representations Can be effectively used in multiple downstream tasks (e.g., text generation, machine translation, classification for languages) Note : while transformer architecture is most prevalent in foundation models, definition not restricted by model architecture","title":"Overview"},{"location":"ai/foundation-models/#data-modalities","text":"Natural Language Speech Business Data IT Data Sensor Data Chemistry & Materials Geospatial Programming Languages (Code) Images Dialog","title":"Data Modalities"},{"location":"ai/foundation-models/#architectures","text":"","title":"Architectures"},{"location":"ai/foundation-models/#encoder-only","text":"Best cost performance trade-off for non generative use cases Most classical NLP tasks: classification, entity and relation extraction, extractive summarization, extractive question answer, etc. Require task-specific labeled data for fine tuning . Examples: BERT/RoBERTa models.","title":"Encoder-only"},{"location":"ai/foundation-models/#encoder-decoder","text":"Support both generative and non-generative use cases Best cost performance trade-off for generative use cases when input is large but generated output is small. Can be prompt-engineered once we hit a size of ~10B but below that can be fine tuned using labeled data . Examples: Google T5 models, UL2 models.","title":"Encoder-Decoder"},{"location":"ai/foundation-models/#decoder-only","text":"Designed explicitly for generative AI use cases summarization, generative question answer, translation, copywriting Architectures used in GPT-3, ChatGPT, etc.","title":"Decoder-only"},{"location":"ai/foundation-models/#training-and-tuning-value","text":"","title":"Training and Tuning (\u2b07\ufe0f value)"},{"location":"ai/foundation-models/#base-model","text":"Pre-trained on 10s of TBs of unlabeled Internet data Examples : Watson Studio base LLM models, Google T5, etc.","title":"Base model"},{"location":"ai/foundation-models/#custom-pre-trained-model","text":"Pre-trained on 10s of GB of domain/industry specific data Examples : IBM-NASA collaboration models, Watson Code Assistant models","title":"Custom pre-trained model"},{"location":"ai/foundation-models/#fine-tuned-model","text":"Fine-tuned on a class of tasks Examples : Watson NLP OOTB Entity, Sentiment models, Google FLAN T5, watsonx sandstone.instruct","title":"Fine-tuned model"},{"location":"ai/foundation-models/#human-in-the-loop-refined-model","text":"ChatGPT, watsonx sandstone.chat","title":"Human-in-the-loop refined model"},{"location":"ai/foundation-models/#adaptation-to-multiple-tasks-complexityskills-model-size","text":"","title":"Adaptation to multiple tasks (\u2b07\ufe0f complexity/skills, \u2b06\ufe0f model size)"},{"location":"ai/foundation-models/#prompt-engineering","text":"Training : None Inference : Engineered Prompt + Input Text \u27a1\ufe0f output Designing and constructing effective prompts to obtain desired outputs Recommended way to start Advantages \u2795 Quick experimentation for various tasks Little to no training data Disadvantages \u2796 Success depends on choice of prompt and model size Mostly a trial-and-error process Number of examples limited by prompt input size limitations Lower accuracy compared to fine-tuning Longer prompts may give better accuracy but cost more","title":"Prompt engineering"},{"location":"ai/foundation-models/#prompt-tuning","text":"Training : Pre-trained model + Labeled Data \u27a1\ufe0f Prompt-Tuning Algorithm \u27a1\ufe0f Tuned Soft Prompt Inference : Tuned Soft Prompt + Input Text \u27a1\ufe0f Pre-trained model \u27a1\ufe0f output Relatively new technique Training data format is the same as for fine tuning Pre-trained models: LLMs with decoders Advantages \u2795 Faster training as only few parameters are learnt Model accuracy comparable to fine tuning in some cases The Pre-trained model is reused for inference in multiple tasks Middle ground between fine-tuning and prompt engineering Fewer parameters compared to fine tuning","title":"Prompt-tuning"},{"location":"ai/foundation-models/#fine-tuning","text":"Training : Pre-trained model + Labeled Data \u27a1\ufe0f Fine-Tuning Algorithm \u27a1\ufe0f Fined-Tuned model Inference : Input Text \u27a1\ufe0f Fined-Tuned model \u27a1\ufe0f output \ud83d\udcc8 SotA accuracy with small models and many popular NLP tasks (classification, extraction) Requires data science expertise Requires separate instance of the model for each task (can be expensive) Difficult as model size increases (e.g., overfitting issues) i.e. typically less than 1B parameters","title":"Fine-tuning"},{"location":"ai/foundation-models/#enterprise-considerations","text":"Head-on comparison with ChatGPT is a trap A single solution does not fit all trust matters ROI determined by use case and inference cost Need to manage risks and limitations of today's LLMs Consider ability to run workloads as desired, train models, provide trusted models, backend integration, enterprise features, and other NFRs","title":"Enterprise considerations"},{"location":"containers/podman/podman-wsl/","text":"Install Podman on Windows Subsystem for Linux Podman is a daemonless container engine for developing, managing, and running OCI Containers on your Linux System. Containers can either be run as root or in rootless mode. Podman is supported on Windows but by default sets up during install it's own WSL2 instance, which is great if you are fine using Windows powershell to run podman commands, but in my case I heavily rely on my custom Ubuntu WSL instance to work on Windows, so I want to be able to run podman there. This document shows how to install Podman in Ubuntu WSL2 (tested with Podman v3.4.4 on Ubuntu 22.04 ). Install required dependencies: sudo apt update -y sudo apt install -y podman qemu-system Initialize Podman machine: podman machine init Download required gvproxy binary from GitHub releases e.g. : sudo wget https://github.com/containers/gvisor-tap-vsock/releases/download/v0.6.1/gvproxy-linux -O /usr/libexec/podman/gvproxy sudo chmod +x /usr/libexec/podman/gvproxy Grant rw access to kvm , required to start Podman machine: sudo chmod 666 /dev/kvm Start Podman machine: podman machine start","title":"Install Podman on Windows Subsystem for Linux"},{"location":"containers/podman/podman-wsl/#install-podman-on-windows-subsystem-for-linux","text":"Podman is a daemonless container engine for developing, managing, and running OCI Containers on your Linux System. Containers can either be run as root or in rootless mode. Podman is supported on Windows but by default sets up during install it's own WSL2 instance, which is great if you are fine using Windows powershell to run podman commands, but in my case I heavily rely on my custom Ubuntu WSL instance to work on Windows, so I want to be able to run podman there. This document shows how to install Podman in Ubuntu WSL2 (tested with Podman v3.4.4 on Ubuntu 22.04 ). Install required dependencies: sudo apt update -y sudo apt install -y podman qemu-system Initialize Podman machine: podman machine init Download required gvproxy binary from GitHub releases e.g. : sudo wget https://github.com/containers/gvisor-tap-vsock/releases/download/v0.6.1/gvproxy-linux -O /usr/libexec/podman/gvproxy sudo chmod +x /usr/libexec/podman/gvproxy Grant rw access to kvm , required to start Podman machine: sudo chmod 666 /dev/kvm Start Podman machine: podman machine start","title":"Install Podman on Windows Subsystem for Linux"},{"location":"devops/cloud-native-toolkit/install-toolkit/","text":"Cloud-Native Toolkit - GitOps Install Overview Cloud-Native Toolkit is an open-source collection of assets that provide an environment for developing cloud-native applications for deployment within Red Hat OpenShift. Components As the name suggests, the Cloud-Native Toolkit provides a collection of tools that can be used in part or in whole to support the activities of software development life cycle. The following provides a listing of the assets that make up the Cloud-Native Toolkit: Environment components After installation, the environment consists of the following components and developer tools: A Red Hat OpenShift Service development cluster A collection of continuous delivery tools deployed into the cluster A set of backend services This diagram illustrates the environment: The diagram shows the components in the environment: the cluster, the deployment target environments, the cloud services, and the tools. The following best-of-breed open-source software tools are installed in the cluster's tools namespace: Capability Tool Description Continuous Integration Tekton CI Tekton is an emerging tool for Continuous Integration with Kubernetes and OpenShift API Contract Testing Pact Pact enables API contract testing Code Analysis SonarQube SonarQube can scan code and display the results in a dashboard Container Image Registry Container Registry Stores container images to be deployed Artifact Management Artifactory Artifactory is an artifact storage and Helm chart repository Continuous Delivery ArgoCD ArgoCD support Continuous Delivery with GitOps Web IDE Code Ready Workspace IDE for editing and managing code in a web browser Install the Toolkit Prerequisites OpenShift Cluster available with admin access docker CLI available on your workstation GitHub account yq CLI installed on your workstation On OpenShift Console On the OpenShift console, click \"Copy login command\" on the top right corner: Click \"Display Token\": Copy the token and server URL available in the provided oc login command On your workstation Create a cntk directory: mkdir cntk cd cntk Install iascable CLI: curl -sL https://iascable.cloudnativetoolkit.dev/install.sh | sh Get OpenShift GitOps and Developer Tools Bills of Materials: curl -sL https://github.com/cloud-native-toolkit/automation-solutions/releases/download/200-openshift-gitops_v1.0.1/bom.yaml > 200 -openshift-gitops.yaml curl -sL https://github.com/cloud-native-toolkit/automation-solutions/releases/download/220-dev-tools_v1.0.0/bom.yaml > 220 -dev-tools.yaml Build both BOMs using iascable : iascable build -i 200 -openshift-gitops.yaml iascable build -i 220 -dev-tools.yaml Navigate to output directory and open a helper docker container that contains terraform CLI required to run the automation: cd output ./launch.sh In your docker container, create the required environment variables required to run the terraform automation on your OpenShift cluster: export TF_VAR_config_banner_text = \"Cloud-Native Toolkit\" export TF_VAR_gitops_repo_repo = <GITOPS_REPOSITORY> export TF_VAR_server_url = <OPENSHIFT_SERVER_URL> export TF_VAR_cluster_login_token = <OPENSHIFT_SERVER_TOKEN> Help TF_VAR_server_url is the OpenShift server URL you retrieved in the first step TF_VAR_cluster_login_token is the OpenShift login token you retrieved in the first step Navigate to 200-openshift-gitops terraform module and run the automation: cd 200 -openshift-gitops/terraform/ terraform init terraform apply --auto-approve NOTE By default, when not providing a gitops_repo_host terraform variable, a Gitea instance is being deployed in the cluster to host the GitOps repository, to install the developer tools we'll need to retrieve the provisioned Gitea host, username and token. We'll now need to login to the OpenShift cluster using the oc login command from first step to retrieve Gitea credentials to pass to next terraform module: oc login --token = ${ LOGIN_TOKEN } --server = ${ SERVER_URL } Navigate to 220-dev-tools terraform module and run the automation: cd ../../220-dev-tools/terraform export TF_VAR_gitops_repo_gitea_host = $( oc get route -n gitea gitea -o yaml | yq .spec.host ) export TF_VAR_gitops_repo_gitea_username = $( oc get secret -n gitea gitea-access -o yaml | yq .data.username | base64 -d ) export TF_VAR_gitops_repo_gitea_token = $( oc get secret -n gitea gitea-access -o yaml | yq .data.password | base64 -d ) terraform init terraform apply --auto-approve Artifactory initial setup Navigate to OpenShift console. Click this icon Click \"Artifactory\" Log in using default username admin and password password . Click \"Get Started\". Reset password following Artifactory requirements, and save it somewhere safe (e.g. 1Password password manager). Then click \"Next\". Paste the URL into the Select Base URL form and remove any trailing context roots, similar to the one in this view. The next page in the wizard is the Configure a Proxy Server page. This is to setup a proxy for external resources. You can click Next to skip this step. The next page in the wizard is the Create Repositories page. Select \"Generic\", then press \"Next\". The next page in the wizard is the Onboarding Complete page. Press \"Finish\". Allow Anonymous Access to Artifactory: Click on the Settings tab on the left menu (the one with the gear icon), and then select Security Obtain the encrypted password: In the Artifactory console, press the \"Welcome, admin\" menu button in the top right corner of the console and select \"Edit profile\" In the User Profile: admin page, enter you Artifactory password and press Unlock : Below, in the Authentication Settings section, is the Encrypted Password field Press the Eye icon to view the encrypted password and press the Cut & Paste icon to copy it: In the OpenShift 4 console, go to Administrator > Workloads > Secrets . At the top, select the tools project and filter for artifactory . Select Edit Secret on artifactory-access . Add a key/value for ARTIFACTORY_ENCRYPT and set the value to your encrypt key value: Sonarqube initial setup Navigate to OpenShift console. Click this icon Click \"Sonarqube\" Log in using default username admin and password admin . Reset the default admin password by putting a custom one, save it for next step. Create the sonarqube-access secret in OpenShift tools namespace with your newly created admin password: oc create secret generic sonarqube-access -n tools --from-literal = SONARQUBE_USER = admin --from-literal = SONARQUBE_PASSWORD = ${ SONARQUBE_PASSWORD } Add SETFCAP capability to pipeline Security Context Constraints Note : This Step is only required if you are running the Toolkit on OpenShift 4.10+ with Openshift Pipelines operator < 1.7.2 . Add SETFCAP capability in allowed capability to pipelines-scc so cluster tasks can request it: oc patch scc pipelines-scc --type merge -p '{\"allowedCapabilities\":[\"SETFCAP\"]}' Update ibm-build-tag-push tekton task in tools namespace to request required SETFCAP capability, e.g. with task version 3.0.3 : oc get task ibm-build-tag-push-v3-0-3 -n tools -o yaml | yq '.spec.steps[2].securityContext.capabilities.add = [\"SETFCAP\"]' | oc apply -f - Conclusion At this stage the Cloud-Native Toolkit should be up and running, congratulations!","title":"Cloud-Native Toolkit - GitOps Install"},{"location":"devops/cloud-native-toolkit/install-toolkit/#cloud-native-toolkit-gitops-install","text":"","title":"Cloud-Native Toolkit - GitOps Install"},{"location":"devops/cloud-native-toolkit/install-toolkit/#overview","text":"Cloud-Native Toolkit is an open-source collection of assets that provide an environment for developing cloud-native applications for deployment within Red Hat OpenShift.","title":"Overview"},{"location":"devops/cloud-native-toolkit/install-toolkit/#components","text":"As the name suggests, the Cloud-Native Toolkit provides a collection of tools that can be used in part or in whole to support the activities of software development life cycle. The following provides a listing of the assets that make up the Cloud-Native Toolkit:","title":"Components"},{"location":"devops/cloud-native-toolkit/install-toolkit/#environment-components","text":"After installation, the environment consists of the following components and developer tools: A Red Hat OpenShift Service development cluster A collection of continuous delivery tools deployed into the cluster A set of backend services This diagram illustrates the environment: The diagram shows the components in the environment: the cluster, the deployment target environments, the cloud services, and the tools. The following best-of-breed open-source software tools are installed in the cluster's tools namespace: Capability Tool Description Continuous Integration Tekton CI Tekton is an emerging tool for Continuous Integration with Kubernetes and OpenShift API Contract Testing Pact Pact enables API contract testing Code Analysis SonarQube SonarQube can scan code and display the results in a dashboard Container Image Registry Container Registry Stores container images to be deployed Artifact Management Artifactory Artifactory is an artifact storage and Helm chart repository Continuous Delivery ArgoCD ArgoCD support Continuous Delivery with GitOps Web IDE Code Ready Workspace IDE for editing and managing code in a web browser","title":"Environment components"},{"location":"devops/cloud-native-toolkit/install-toolkit/#install-the-toolkit","text":"","title":"Install the Toolkit"},{"location":"devops/cloud-native-toolkit/install-toolkit/#prerequisites","text":"OpenShift Cluster available with admin access docker CLI available on your workstation GitHub account yq CLI installed on your workstation","title":"Prerequisites"},{"location":"devops/cloud-native-toolkit/install-toolkit/#on-openshift-console","text":"On the OpenShift console, click \"Copy login command\" on the top right corner: Click \"Display Token\": Copy the token and server URL available in the provided oc login command","title":"On OpenShift Console"},{"location":"devops/cloud-native-toolkit/install-toolkit/#on-your-workstation","text":"Create a cntk directory: mkdir cntk cd cntk Install iascable CLI: curl -sL https://iascable.cloudnativetoolkit.dev/install.sh | sh Get OpenShift GitOps and Developer Tools Bills of Materials: curl -sL https://github.com/cloud-native-toolkit/automation-solutions/releases/download/200-openshift-gitops_v1.0.1/bom.yaml > 200 -openshift-gitops.yaml curl -sL https://github.com/cloud-native-toolkit/automation-solutions/releases/download/220-dev-tools_v1.0.0/bom.yaml > 220 -dev-tools.yaml Build both BOMs using iascable : iascable build -i 200 -openshift-gitops.yaml iascable build -i 220 -dev-tools.yaml Navigate to output directory and open a helper docker container that contains terraform CLI required to run the automation: cd output ./launch.sh In your docker container, create the required environment variables required to run the terraform automation on your OpenShift cluster: export TF_VAR_config_banner_text = \"Cloud-Native Toolkit\" export TF_VAR_gitops_repo_repo = <GITOPS_REPOSITORY> export TF_VAR_server_url = <OPENSHIFT_SERVER_URL> export TF_VAR_cluster_login_token = <OPENSHIFT_SERVER_TOKEN> Help TF_VAR_server_url is the OpenShift server URL you retrieved in the first step TF_VAR_cluster_login_token is the OpenShift login token you retrieved in the first step Navigate to 200-openshift-gitops terraform module and run the automation: cd 200 -openshift-gitops/terraform/ terraform init terraform apply --auto-approve NOTE By default, when not providing a gitops_repo_host terraform variable, a Gitea instance is being deployed in the cluster to host the GitOps repository, to install the developer tools we'll need to retrieve the provisioned Gitea host, username and token. We'll now need to login to the OpenShift cluster using the oc login command from first step to retrieve Gitea credentials to pass to next terraform module: oc login --token = ${ LOGIN_TOKEN } --server = ${ SERVER_URL } Navigate to 220-dev-tools terraform module and run the automation: cd ../../220-dev-tools/terraform export TF_VAR_gitops_repo_gitea_host = $( oc get route -n gitea gitea -o yaml | yq .spec.host ) export TF_VAR_gitops_repo_gitea_username = $( oc get secret -n gitea gitea-access -o yaml | yq .data.username | base64 -d ) export TF_VAR_gitops_repo_gitea_token = $( oc get secret -n gitea gitea-access -o yaml | yq .data.password | base64 -d ) terraform init terraform apply --auto-approve","title":"On your workstation"},{"location":"devops/cloud-native-toolkit/install-toolkit/#artifactory-initial-setup","text":"Navigate to OpenShift console. Click this icon Click \"Artifactory\" Log in using default username admin and password password . Click \"Get Started\". Reset password following Artifactory requirements, and save it somewhere safe (e.g. 1Password password manager). Then click \"Next\". Paste the URL into the Select Base URL form and remove any trailing context roots, similar to the one in this view. The next page in the wizard is the Configure a Proxy Server page. This is to setup a proxy for external resources. You can click Next to skip this step. The next page in the wizard is the Create Repositories page. Select \"Generic\", then press \"Next\". The next page in the wizard is the Onboarding Complete page. Press \"Finish\". Allow Anonymous Access to Artifactory: Click on the Settings tab on the left menu (the one with the gear icon), and then select Security Obtain the encrypted password: In the Artifactory console, press the \"Welcome, admin\" menu button in the top right corner of the console and select \"Edit profile\" In the User Profile: admin page, enter you Artifactory password and press Unlock : Below, in the Authentication Settings section, is the Encrypted Password field Press the Eye icon to view the encrypted password and press the Cut & Paste icon to copy it: In the OpenShift 4 console, go to Administrator > Workloads > Secrets . At the top, select the tools project and filter for artifactory . Select Edit Secret on artifactory-access . Add a key/value for ARTIFACTORY_ENCRYPT and set the value to your encrypt key value:","title":"Artifactory initial setup"},{"location":"devops/cloud-native-toolkit/install-toolkit/#sonarqube-initial-setup","text":"Navigate to OpenShift console. Click this icon Click \"Sonarqube\" Log in using default username admin and password admin . Reset the default admin password by putting a custom one, save it for next step. Create the sonarqube-access secret in OpenShift tools namespace with your newly created admin password: oc create secret generic sonarqube-access -n tools --from-literal = SONARQUBE_USER = admin --from-literal = SONARQUBE_PASSWORD = ${ SONARQUBE_PASSWORD }","title":"Sonarqube initial setup"},{"location":"devops/cloud-native-toolkit/install-toolkit/#add-setfcap-capability-to-pipeline-security-context-constraints","text":"Note : This Step is only required if you are running the Toolkit on OpenShift 4.10+ with Openshift Pipelines operator < 1.7.2 . Add SETFCAP capability in allowed capability to pipelines-scc so cluster tasks can request it: oc patch scc pipelines-scc --type merge -p '{\"allowedCapabilities\":[\"SETFCAP\"]}' Update ibm-build-tag-push tekton task in tools namespace to request required SETFCAP capability, e.g. with task version 3.0.3 : oc get task ibm-build-tag-push-v3-0-3 -n tools -o yaml | yq '.spec.steps[2].securityContext.capabilities.add = [\"SETFCAP\"]' | oc apply -f -","title":"Add SETFCAP capability to pipeline Security Context Constraints"},{"location":"devops/cloud-native-toolkit/install-toolkit/#conclusion","text":"At this stage the Cloud-Native Toolkit should be up and running, congratulations!","title":"Conclusion"},{"location":"kubernetes/install/k3s/","text":"Kubernetes install - K3s Main docs: https://k3s.io/ https://docs.k3s.io/quick-start Master Node Run the K3s install script from master node: curl -sfL https://get.k3s.io | K3S_KUBECONFIG_MODE = \"644\" sh - Once the install is done, fetch node token to add nodes to the cluster and copy it for next step: sudo cat /var/lib/rancher/k3s/server/node-token K100f4662d45c9160374695...dbcc53a11::server:b108c...bcb17c Worker Node(s) Run the K3s install script from each worker node: curl -sfL https://get.k3s.io | K3S_URL = https:// ${ K3S_MASTER_NODE_IP } :6443 K3S_TOKEN = ${ K3S_NODE_TOKEN } sh - Access Cluster Once cluster has been deployed, run following from your local workstation to copy kube config file from master node: cd $HOME /.kube echo \"get /etc/rancher/k3s/k3s.yaml ${ CLUSTER_NAME } \" | sftp -s \"sudo /usr/lib/openssh/sftp-server\" ${ MASTER_NODE_USERNAME } @ ${ MASTER_NODE_IP } server = \"https:// ${ MASTER_NODE_IP } :6443\" yq -i \".clusters[0].cluster.server = env(server)\" ${ CLUSTER_NAME } export KUBECONFIG = $HOME /.kube/ ${ CLUSTER_NAME } kubectl get nodes","title":"Kubernetes install - K3s"},{"location":"kubernetes/install/k3s/#kubernetes-install-k3s","text":"Main docs: https://k3s.io/ https://docs.k3s.io/quick-start","title":"Kubernetes install - K3s"},{"location":"kubernetes/install/k3s/#master-node","text":"Run the K3s install script from master node: curl -sfL https://get.k3s.io | K3S_KUBECONFIG_MODE = \"644\" sh - Once the install is done, fetch node token to add nodes to the cluster and copy it for next step: sudo cat /var/lib/rancher/k3s/server/node-token K100f4662d45c9160374695...dbcc53a11::server:b108c...bcb17c","title":"Master Node"},{"location":"kubernetes/install/k3s/#worker-nodes","text":"Run the K3s install script from each worker node: curl -sfL https://get.k3s.io | K3S_URL = https:// ${ K3S_MASTER_NODE_IP } :6443 K3S_TOKEN = ${ K3S_NODE_TOKEN } sh -","title":"Worker Node(s)"},{"location":"kubernetes/install/k3s/#access-cluster","text":"Once cluster has been deployed, run following from your local workstation to copy kube config file from master node: cd $HOME /.kube echo \"get /etc/rancher/k3s/k3s.yaml ${ CLUSTER_NAME } \" | sftp -s \"sudo /usr/lib/openssh/sftp-server\" ${ MASTER_NODE_USERNAME } @ ${ MASTER_NODE_IP } server = \"https:// ${ MASTER_NODE_IP } :6443\" yq -i \".clusters[0].cluster.server = env(server)\" ${ CLUSTER_NAME } export KUBECONFIG = $HOME /.kube/ ${ CLUSTER_NAME } kubectl get nodes","title":"Access Cluster"},{"location":"kubernetes/install/kubespray/","text":"Kubernetes install - Kubespray Main docs: https://kubespray.io/#/docs/getting-started https://kubernetes.io/docs/setup/production-environment/tools/kubespray/ Prerequisites https://kubernetes.io/docs/setup/production-environment/tools/kubespray/#1-5-meet-the-underlay-requirements Inventory file https://kubernetes.io/docs/setup/production-environment/tools/kubespray/#2-5-compose-an-inventory-file Inside bastion: git clone https://github.com/kubernetes-sigs/kubespray cd kubespray export CLUSTER_NAME = \"CLUSTER_NAME\" cp -r inventory/sample inventory/ ${ CLUSTER_NAME } declare -a IPS =( 10 .0.0.11 10 .0.0.12 10 .0.0.13 10 .0.0.14 10 .0.0.15 10 .0.0.16 ) CONFIG_FILE = inventory/ ${ CLUSTER_NAME } /hosts.yml python3 contrib/inventory_builder/inventory.py ${ IPS [@] } Configure Deployment https://kubernetes.io/docs/setup/production-environment/tools/kubespray/#3-5-plan-your-cluster-deployment Edit inventory/${CLUSTER_NAME}/group_vars/all/all.yml to update following config variables: cloud_provider: \"external\" external_cloud_provider: \"vsphere\" Edit inventory/${CLUSTER_NAME}/group_vars/all/vsphere.yml to update following config variables: ## Values for the external vSphere Cloud Provider external_vsphere_vcenter_ip: \"vcr72.example.com\" external_vsphere_vcenter_port: \"443\" external_vsphere_insecure: \"true\" external_vsphere_user: \"<VSPHERE_USERNAME>\" external_vsphere_password: \"<VSPHERE_PASSWORD>\" external_vsphere_datacenter: \"TEST\" external_vsphere_kubernetes_cluster_id: \"<CLUSTER_NAME>\" ## Vsphere version where located VMs external_vsphere_version: \"7.0.3\" ## To use vSphere CSI plugin to provision volumes set this value to true vsphere_csi_enabled: false vsphere_csi_controller_replicas: 1 Deploy cluster https://kubernetes.io/docs/setup/production-environment/tools/kubespray/#4-5-deploy-a-cluster ansible-playbook -i inventory/ ${ CLUSTER_NAME } /hosts.yml -b -v cluster.yml Access Cluster Once cluster has been deployed, run following from your local machine to copy kube config file: cd $HOME /.kube echo \"get /root/.kube/config ${ CLUSTER_NAME } \" | sftp -s \"sudo /usr/lib/openssh/sftp-server\" ${ MASTER_NODE_USERNAME } @ ${ MASTER_NODE_IP } server = \"https:// ${ MASTER_NODE_IP } :6443\" yq -i \".clusters[0].cluster.server = env(server)\" ${ CLUSTER_NAME } export KUBECONFIG = $HOME /.kube/ ${ CLUSTER_NAME } kubectl get nodes Known issues VSphere CSI driver We've had an issue with VSphere CSI driver deployment within the cluster, to be looked at... Install without storage option When installing using VSphere external provider without VSphere CSI driver, worker nodes are unschedulable due to a taint added to worker nodes specifying that the node is not initialized nor schedulable even though the nodes seem ready, healthy and schedulable. To fix this issue run the following command for each node: kubectl taint nodes worker-1 node.cloudprovider.kubernetes.io/uninitialized = true:NoSchedule-","title":"Kubernetes install - Kubespray"},{"location":"kubernetes/install/kubespray/#kubernetes-install-kubespray","text":"Main docs: https://kubespray.io/#/docs/getting-started https://kubernetes.io/docs/setup/production-environment/tools/kubespray/","title":"Kubernetes install - Kubespray"},{"location":"kubernetes/install/kubespray/#prerequisites","text":"https://kubernetes.io/docs/setup/production-environment/tools/kubespray/#1-5-meet-the-underlay-requirements","title":"Prerequisites"},{"location":"kubernetes/install/kubespray/#inventory-file","text":"https://kubernetes.io/docs/setup/production-environment/tools/kubespray/#2-5-compose-an-inventory-file Inside bastion: git clone https://github.com/kubernetes-sigs/kubespray cd kubespray export CLUSTER_NAME = \"CLUSTER_NAME\" cp -r inventory/sample inventory/ ${ CLUSTER_NAME } declare -a IPS =( 10 .0.0.11 10 .0.0.12 10 .0.0.13 10 .0.0.14 10 .0.0.15 10 .0.0.16 ) CONFIG_FILE = inventory/ ${ CLUSTER_NAME } /hosts.yml python3 contrib/inventory_builder/inventory.py ${ IPS [@] }","title":"Inventory file"},{"location":"kubernetes/install/kubespray/#configure-deployment","text":"https://kubernetes.io/docs/setup/production-environment/tools/kubespray/#3-5-plan-your-cluster-deployment Edit inventory/${CLUSTER_NAME}/group_vars/all/all.yml to update following config variables: cloud_provider: \"external\" external_cloud_provider: \"vsphere\" Edit inventory/${CLUSTER_NAME}/group_vars/all/vsphere.yml to update following config variables: ## Values for the external vSphere Cloud Provider external_vsphere_vcenter_ip: \"vcr72.example.com\" external_vsphere_vcenter_port: \"443\" external_vsphere_insecure: \"true\" external_vsphere_user: \"<VSPHERE_USERNAME>\" external_vsphere_password: \"<VSPHERE_PASSWORD>\" external_vsphere_datacenter: \"TEST\" external_vsphere_kubernetes_cluster_id: \"<CLUSTER_NAME>\" ## Vsphere version where located VMs external_vsphere_version: \"7.0.3\" ## To use vSphere CSI plugin to provision volumes set this value to true vsphere_csi_enabled: false vsphere_csi_controller_replicas: 1","title":"Configure Deployment"},{"location":"kubernetes/install/kubespray/#deploy-cluster","text":"https://kubernetes.io/docs/setup/production-environment/tools/kubespray/#4-5-deploy-a-cluster ansible-playbook -i inventory/ ${ CLUSTER_NAME } /hosts.yml -b -v cluster.yml","title":"Deploy cluster"},{"location":"kubernetes/install/kubespray/#access-cluster","text":"Once cluster has been deployed, run following from your local machine to copy kube config file: cd $HOME /.kube echo \"get /root/.kube/config ${ CLUSTER_NAME } \" | sftp -s \"sudo /usr/lib/openssh/sftp-server\" ${ MASTER_NODE_USERNAME } @ ${ MASTER_NODE_IP } server = \"https:// ${ MASTER_NODE_IP } :6443\" yq -i \".clusters[0].cluster.server = env(server)\" ${ CLUSTER_NAME } export KUBECONFIG = $HOME /.kube/ ${ CLUSTER_NAME } kubectl get nodes","title":"Access Cluster"},{"location":"kubernetes/install/kubespray/#known-issues","text":"","title":"Known issues"},{"location":"kubernetes/install/kubespray/#vsphere-csi-driver","text":"We've had an issue with VSphere CSI driver deployment within the cluster, to be looked at...","title":"VSphere CSI driver"},{"location":"kubernetes/install/kubespray/#install-without-storage-option","text":"When installing using VSphere external provider without VSphere CSI driver, worker nodes are unschedulable due to a taint added to worker nodes specifying that the node is not initialized nor schedulable even though the nodes seem ready, healthy and schedulable. To fix this issue run the following command for each node: kubectl taint nodes worker-1 node.cloudprovider.kubernetes.io/uninitialized = true:NoSchedule-","title":"Install without storage option"},{"location":"kubernetes/install/rpi/","text":"Self-hosting platform on Raspberry Pi - K3s This doc helps deploy a Kubernetes self-hosting platform on Raspberry Pi devices with K3s . It uses a Terraform modules I have created to deploy the necessary software in the cluster. Roadmap Configure Kubernetes cluster Self-host password manager: Bitwarden Self-host IoT dev platform: Node-RED Self-host home cloud: NextCloud Self-host home Media Center Transmission Flaresolverr Jackett Sonarr Radarr Plex Self-host ads/trackers protection: Pi-Hole Prerequisites Accessible K8s/K3s cluster on your Pi. With cert-manager CustomResourceDefinition installed: kubectl apply -f https://github.com/jetstack/cert-manager/releases/download/v0.16.0/cert-manager.crds.yaml For transmission bittorrent client, an OpenVPN config file stored in openvpn.ignore.ovpn , with auth-user-pass set to /config/openvpn-credentials.txt (auto auth), including cert and key. Usage Clone the repository: $ git clone https://github.com/NoeSamaille/terraform-self-hosting-platform-rpi $ cd terraform-self-hosting-platform-rpi Configure your environment: $ mv terraform.tfvars.template terraform.tfvars $ vim terraform.tfvars Once it's done you can start deploying resources: $ source scripts/init.sh # Generates service passwords $ terraform init $ terraform plan $ terraform apply --auto-approve ... output ommited ... Apply complete! Resources: 32 added, 0 changed, 0 destroyed. To destroy all the resources: $ terraform destroy --auto-approve ... output ommited ... Apply complete! Resources: 0 added, 0 changed, 32 destroyed. How to set up nodes Base pi set up Note : here we'll set up pi-master i.e. our master pi, if you have additional workers (optional) you'll then have to repeat the following steps for each of the workers, replacing references to pi-master by pi-worker-1 , pi-worker-2 , etc. Connect via SSH to the pi: user@workstation $ ssh pi@<PI_IP> ... output ommited ... pi@raspberrypi:~ $ Change password: pi@raspberrypi:~ $ passwd ... output ommited ... passwd: password updated successfully Change host names: pi@raspberrypi:~ $ sudo -i root@raspberrypi:~ $ echo \"pi-master\" > /etc/hostname root@raspberrypi:~ $ sed -i \"s/ $HOSTNAME /pi-master/\" /etc/hosts Enable container features: root@raspberrypi:~ $ sed -i 's/$/ cgroup_enable=cpuset cgroup_memory=1 cgroup_enable=memory/' /boot/cmdline.txt Make sure the system is up-to-date: root@raspberrypi:~ $ apt update && apt upgrade -y Configure a static IP, Note that This could be also done at the network level via the router admin (DHCP): root@raspberrypi:~ $ cat <<EOF >> /etc/dhcpcd.conf interface eth0 static ip_address=<YOUR_STATIC_IP_HERE>/24 static routers=192.168.1.1 static domain_name_servers=1.1.1.1 EOF Reboot: root@raspberrypi:~ $ reboot Wait for a few sec, then connect via SSH to the pi using the new static IP you've just configured: user@workstation $ ssh pi@<PI_IP> ... output ommited ... pi@pi-master:~ $ OPTIONAL: Set up NFS disk share Create NFS Share on Master Pi On master pi, run the command fdisk -l to list all the connected disks to the system (includes the RAM) and try to identify the disk. pi@pi-master:~ $ sudo fdisk -l If your disk is new and freshly out of the package, you will need to create a partition. pi@pi-master:~ $ sudo mkfs.ext4 /dev/sda You can manually mount the disk to the directory /mnt/hdd . pi@pi-master:~ $ sudo mkdir /mnt/hdd pi@pi-master:~ $ sudo chown -R pi:pi /mnt/hdd/ pi@pi-master:~ $ sudo mount /dev/sda /mnt/hdd To automatically mount the disk on startup, you first need to find the Unique ID of the disk using the command blkid : pi@pi-master:~ $ sudo blkid ... output ommited ... /dev/sda: UUID = \"0ac98c2c-8c32-476b-9009-ffca123a2654\" TYPE = \"ext4\" Edit the file /etc/fstab and add the following line to configure auto-mount of the disk on startup: pi@pi-master:~ $ sudo -i root@pi-master:~ $ echo \"UUID=0ac98c2c-8c32-476b-9009-ffca123a2654 /mnt/hdd ext4 defaults 0 0\" >> /etc/fstab root@pi-master:~ $ exit Reboot the system pi@pi-master:~ $ sudo reboot Verify the disk is correctly mounted on startup with the following command: pi@pi-master:~ $ df -ha /dev/sda Filesystem Size Used Avail Use% Mounted on /dev/sda 458G 73M 435G 1 % /mnt/hdd Install the required dependencies: pi@pi-master:~ $ sudo apt install nfs-kernel-server -y Edit the file /etc/exports by running the following command: pi@pi-master:~ $ sudo -i root@pi-master:~ $ echo \"/mnt/hdd-2 *(rw,no_root_squash,insecure,async,no_subtree_check,anonuid=1000,anongid=1000)\" >> /etc/exports root@pi-master:~ $ exit Start the NFS Server: pi@pi-master:~ $ sudo exportfs -ra Mount NFS share on Worker(s) Note : repeat the following steps for each of the workers pi-worker-1 , pi-worker-2 , etc. Install the necessary dependencies: pi@pi-worker-x:~ $ sudo apt install nfs-common -y Create the directory to mount the NFS Share: pi@pi-worker-x:~ $ sudo mkdir /mnt/hdd pi@pi-worker-x:~ $ sudo chown -R pi:pi /mnt/hdd Configure auto-mount of the NFS Share by adding the following line, where <MASTER_IP>:/mnt/hdd is the IP of pi-master followed by the NFS share path: pi@pi-worker-x:~ $ sudo -i root@pi-worker-x:~ $ echo \"<MASTER_IP>:/mnt/hdd /mnt/hdd nfs rw 0 0\" >> /etc/fstab root@pi-worker-x:~ $ exit Reboot the system pi@pi-worker-x:~ $ sudo reboot Optional : to mount manually you can run the following command, where <MASTER_IP>:/mnt/hdd is the IP of pi-master followed by the NFS share path: pi@pi-worker-x:~ $ sudo mount -t nfs <MASTER_IP>:/mnt/hdd /mnt/hdd Setup K3s Start K3s on Master pi pi@pi-master:~ $ curl -sfL https://get.k3s.io | K3S_KUBECONFIG_MODE = \"644\" INSTALL_K3S_EXEC = \" --no-deploy servicelb --no-deploy traefik\" sh - Register workers Get K3s token on master pi, copy the result: pi@pi-master:~ $ sudo cat /var/lib/rancher/k3s/server/node-token K103166a17...eebca269271 Run K3s installer on worker (repeat on each worker): pi@pi-worker-x:~ $ curl -sfL https://get.k3s.io | K3S_KUBECONFIG_MODE = \"644\" K3S_URL = \"https://<MASTER_IP>:6443\" K3S_TOKEN = \"K103166a17...eebca269271\" sh - Access K3s cluster from workstation Copy kube config file from master pi: user@workstation:~ $ scp pi@<MASTER_IP>:/etc/rancher/k3s/k3s.yaml ~/.kube/config Edit kube config file to replace 127.0.0.1 with <MASTER_IP> : user@workstation:~ $ vim ~/.kube/config Test everything by running a kubectl command: user@workstation:~ $ kubectl get nodes -o wide Tear down K3s Worker(s) user@workstation:~ $ sudo /usr/local/bin/k3s-agent-uninstall.sh Master pi@pi-master:~ $ sudo /usr/local/bin/k3s-uninstall.sh Known issues Node-RED authentication Node-RED authentication isn't set up by default atm, you can set it up by scaling the deployment down, editing the settings.js file to enable authentication and scaling the deployment back up: pi@pi-master:~ $ kubectl scale deployment/node-red --replicas=0 -n node-red pi@pi-master:~ $ vim /path/to/node-red/settings.js pi@pi-master:~ $ kubectl scale deployment/node-red --replicas=1 -n node-red You can either set up authentication through GitHub ( Documentation ): # settings . js ... Ommited ... adminAuth : require ( 'node-red-auth-github' )({ clientID : \"<GITHUB_CLIENT_ID>\" , clientSecret : \"<GITHUB_CLIENT_SECRET>\" , baseURL : \"https://node-red.<DOMAIN>/\" , users : [ { username : \"<GITHUB_USERNAME>\" , permissions : [ \"*\" ]} ] }), ... Ommited ... Or classic user-pass authentication (generate a password hash using node -e \"console.log(require('bcryptjs').hashSync(process.argv[1], 8));\" <your-password-here> ): # settings . js ... Ommited ... adminAuth : { type : \"credentials\" , users : [ { username : \"admin\" , password : \"$2a$08$zZWtXTja0fB1pzD4sHCMyOCMYz2Z6dNbM6tl8sJogENOMcxWV9DN.\" , permissions : \"*\" }, { username : \"guest\" , password : \"$2b$08$wuAqPiKJlVN27eF5qJp.RuQYuy6ZYONW7a/UWYxDTtwKFCdB8F19y\" , permissions : \"read\" } ] }, ... Ommited ... More information in the Docs: Securing Node-RED .","title":"Self-hosting platform on Raspberry Pi - K3s"},{"location":"kubernetes/install/rpi/#self-hosting-platform-on-raspberry-pi-k3s","text":"This doc helps deploy a Kubernetes self-hosting platform on Raspberry Pi devices with K3s . It uses a Terraform modules I have created to deploy the necessary software in the cluster.","title":"Self-hosting platform on Raspberry Pi - K3s"},{"location":"kubernetes/install/rpi/#roadmap","text":"Configure Kubernetes cluster Self-host password manager: Bitwarden Self-host IoT dev platform: Node-RED Self-host home cloud: NextCloud Self-host home Media Center Transmission Flaresolverr Jackett Sonarr Radarr Plex Self-host ads/trackers protection: Pi-Hole","title":"Roadmap"},{"location":"kubernetes/install/rpi/#prerequisites","text":"Accessible K8s/K3s cluster on your Pi. With cert-manager CustomResourceDefinition installed: kubectl apply -f https://github.com/jetstack/cert-manager/releases/download/v0.16.0/cert-manager.crds.yaml For transmission bittorrent client, an OpenVPN config file stored in openvpn.ignore.ovpn , with auth-user-pass set to /config/openvpn-credentials.txt (auto auth), including cert and key.","title":"Prerequisites"},{"location":"kubernetes/install/rpi/#usage","text":"Clone the repository: $ git clone https://github.com/NoeSamaille/terraform-self-hosting-platform-rpi $ cd terraform-self-hosting-platform-rpi Configure your environment: $ mv terraform.tfvars.template terraform.tfvars $ vim terraform.tfvars Once it's done you can start deploying resources: $ source scripts/init.sh # Generates service passwords $ terraform init $ terraform plan $ terraform apply --auto-approve ... output ommited ... Apply complete! Resources: 32 added, 0 changed, 0 destroyed. To destroy all the resources: $ terraform destroy --auto-approve ... output ommited ... Apply complete! Resources: 0 added, 0 changed, 32 destroyed.","title":"Usage"},{"location":"kubernetes/install/rpi/#how-to-set-up-nodes","text":"","title":"How to set up nodes"},{"location":"kubernetes/install/rpi/#base-pi-set-up","text":"Note : here we'll set up pi-master i.e. our master pi, if you have additional workers (optional) you'll then have to repeat the following steps for each of the workers, replacing references to pi-master by pi-worker-1 , pi-worker-2 , etc. Connect via SSH to the pi: user@workstation $ ssh pi@<PI_IP> ... output ommited ... pi@raspberrypi:~ $ Change password: pi@raspberrypi:~ $ passwd ... output ommited ... passwd: password updated successfully Change host names: pi@raspberrypi:~ $ sudo -i root@raspberrypi:~ $ echo \"pi-master\" > /etc/hostname root@raspberrypi:~ $ sed -i \"s/ $HOSTNAME /pi-master/\" /etc/hosts Enable container features: root@raspberrypi:~ $ sed -i 's/$/ cgroup_enable=cpuset cgroup_memory=1 cgroup_enable=memory/' /boot/cmdline.txt Make sure the system is up-to-date: root@raspberrypi:~ $ apt update && apt upgrade -y Configure a static IP, Note that This could be also done at the network level via the router admin (DHCP): root@raspberrypi:~ $ cat <<EOF >> /etc/dhcpcd.conf interface eth0 static ip_address=<YOUR_STATIC_IP_HERE>/24 static routers=192.168.1.1 static domain_name_servers=1.1.1.1 EOF Reboot: root@raspberrypi:~ $ reboot Wait for a few sec, then connect via SSH to the pi using the new static IP you've just configured: user@workstation $ ssh pi@<PI_IP> ... output ommited ... pi@pi-master:~ $","title":"Base pi set up"},{"location":"kubernetes/install/rpi/#optional-set-up-nfs-disk-share","text":"","title":"OPTIONAL: Set up NFS disk share"},{"location":"kubernetes/install/rpi/#create-nfs-share-on-master-pi","text":"On master pi, run the command fdisk -l to list all the connected disks to the system (includes the RAM) and try to identify the disk. pi@pi-master:~ $ sudo fdisk -l If your disk is new and freshly out of the package, you will need to create a partition. pi@pi-master:~ $ sudo mkfs.ext4 /dev/sda You can manually mount the disk to the directory /mnt/hdd . pi@pi-master:~ $ sudo mkdir /mnt/hdd pi@pi-master:~ $ sudo chown -R pi:pi /mnt/hdd/ pi@pi-master:~ $ sudo mount /dev/sda /mnt/hdd To automatically mount the disk on startup, you first need to find the Unique ID of the disk using the command blkid : pi@pi-master:~ $ sudo blkid ... output ommited ... /dev/sda: UUID = \"0ac98c2c-8c32-476b-9009-ffca123a2654\" TYPE = \"ext4\" Edit the file /etc/fstab and add the following line to configure auto-mount of the disk on startup: pi@pi-master:~ $ sudo -i root@pi-master:~ $ echo \"UUID=0ac98c2c-8c32-476b-9009-ffca123a2654 /mnt/hdd ext4 defaults 0 0\" >> /etc/fstab root@pi-master:~ $ exit Reboot the system pi@pi-master:~ $ sudo reboot Verify the disk is correctly mounted on startup with the following command: pi@pi-master:~ $ df -ha /dev/sda Filesystem Size Used Avail Use% Mounted on /dev/sda 458G 73M 435G 1 % /mnt/hdd Install the required dependencies: pi@pi-master:~ $ sudo apt install nfs-kernel-server -y Edit the file /etc/exports by running the following command: pi@pi-master:~ $ sudo -i root@pi-master:~ $ echo \"/mnt/hdd-2 *(rw,no_root_squash,insecure,async,no_subtree_check,anonuid=1000,anongid=1000)\" >> /etc/exports root@pi-master:~ $ exit Start the NFS Server: pi@pi-master:~ $ sudo exportfs -ra","title":"Create NFS Share on Master Pi"},{"location":"kubernetes/install/rpi/#mount-nfs-share-on-workers","text":"Note : repeat the following steps for each of the workers pi-worker-1 , pi-worker-2 , etc. Install the necessary dependencies: pi@pi-worker-x:~ $ sudo apt install nfs-common -y Create the directory to mount the NFS Share: pi@pi-worker-x:~ $ sudo mkdir /mnt/hdd pi@pi-worker-x:~ $ sudo chown -R pi:pi /mnt/hdd Configure auto-mount of the NFS Share by adding the following line, where <MASTER_IP>:/mnt/hdd is the IP of pi-master followed by the NFS share path: pi@pi-worker-x:~ $ sudo -i root@pi-worker-x:~ $ echo \"<MASTER_IP>:/mnt/hdd /mnt/hdd nfs rw 0 0\" >> /etc/fstab root@pi-worker-x:~ $ exit Reboot the system pi@pi-worker-x:~ $ sudo reboot Optional : to mount manually you can run the following command, where <MASTER_IP>:/mnt/hdd is the IP of pi-master followed by the NFS share path: pi@pi-worker-x:~ $ sudo mount -t nfs <MASTER_IP>:/mnt/hdd /mnt/hdd","title":"Mount NFS share on Worker(s)"},{"location":"kubernetes/install/rpi/#setup-k3s","text":"","title":"Setup K3s"},{"location":"kubernetes/install/rpi/#start-k3s-on-master-pi","text":"pi@pi-master:~ $ curl -sfL https://get.k3s.io | K3S_KUBECONFIG_MODE = \"644\" INSTALL_K3S_EXEC = \" --no-deploy servicelb --no-deploy traefik\" sh -","title":"Start K3s on Master pi"},{"location":"kubernetes/install/rpi/#register-workers","text":"Get K3s token on master pi, copy the result: pi@pi-master:~ $ sudo cat /var/lib/rancher/k3s/server/node-token K103166a17...eebca269271 Run K3s installer on worker (repeat on each worker): pi@pi-worker-x:~ $ curl -sfL https://get.k3s.io | K3S_KUBECONFIG_MODE = \"644\" K3S_URL = \"https://<MASTER_IP>:6443\" K3S_TOKEN = \"K103166a17...eebca269271\" sh -","title":"Register workers"},{"location":"kubernetes/install/rpi/#access-k3s-cluster-from-workstation","text":"Copy kube config file from master pi: user@workstation:~ $ scp pi@<MASTER_IP>:/etc/rancher/k3s/k3s.yaml ~/.kube/config Edit kube config file to replace 127.0.0.1 with <MASTER_IP> : user@workstation:~ $ vim ~/.kube/config Test everything by running a kubectl command: user@workstation:~ $ kubectl get nodes -o wide","title":"Access K3s cluster from workstation"},{"location":"kubernetes/install/rpi/#tear-down-k3s","text":"Worker(s) user@workstation:~ $ sudo /usr/local/bin/k3s-agent-uninstall.sh Master pi@pi-master:~ $ sudo /usr/local/bin/k3s-uninstall.sh","title":"Tear down K3s"},{"location":"kubernetes/install/rpi/#known-issues","text":"","title":"Known issues"},{"location":"kubernetes/install/rpi/#node-red-authentication","text":"Node-RED authentication isn't set up by default atm, you can set it up by scaling the deployment down, editing the settings.js file to enable authentication and scaling the deployment back up: pi@pi-master:~ $ kubectl scale deployment/node-red --replicas=0 -n node-red pi@pi-master:~ $ vim /path/to/node-red/settings.js pi@pi-master:~ $ kubectl scale deployment/node-red --replicas=1 -n node-red You can either set up authentication through GitHub ( Documentation ): # settings . js ... Ommited ... adminAuth : require ( 'node-red-auth-github' )({ clientID : \"<GITHUB_CLIENT_ID>\" , clientSecret : \"<GITHUB_CLIENT_SECRET>\" , baseURL : \"https://node-red.<DOMAIN>/\" , users : [ { username : \"<GITHUB_USERNAME>\" , permissions : [ \"*\" ]} ] }), ... Ommited ... Or classic user-pass authentication (generate a password hash using node -e \"console.log(require('bcryptjs').hashSync(process.argv[1], 8));\" <your-password-here> ): # settings . js ... Ommited ... adminAuth : { type : \"credentials\" , users : [ { username : \"admin\" , password : \"$2a$08$zZWtXTja0fB1pzD4sHCMyOCMYz2Z6dNbM6tl8sJogENOMcxWV9DN.\" , permissions : \"*\" }, { username : \"guest\" , password : \"$2b$08$wuAqPiKJlVN27eF5qJp.RuQYuy6ZYONW7a/UWYxDTtwKFCdB8F19y\" , permissions : \"read\" } ] }, ... Ommited ... More information in the Docs: Securing Node-RED .","title":"Node-RED authentication"},{"location":"kubernetes/openshift/add-registry-ca/","text":"Adding certificate authorities to the cluster You can add certificate authorities (CA) to the cluster for use when pushing and pulling images with the following procedure. Prerequisites You must have access to the public certificates of the registry, usually a hostname/ca.crt file located in the /etc/docker/certs.d/ directory. Procedure Create a ConfigMap in the openshift-config namespace containing the trusted certificates for the registries that use self-signed certificates. For each CA file, ensure the key in the ConfigMap is the hostname of the registry in the hostname[..port] format: oc create configmap registry-cas -n openshift-config \\ --from-file = myregistry.corp.com..5000 = /etc/docker/certs.d/myregistry.corp.com:5000/ca.crt \\ --from-file = otherregistry.com = /etc/docker/certs.d/otherregistry.com/ca.crt Update the cluster image configuration: oc patch image.config.openshift.io/cluster --patch '{\"spec\":{\"additionalTrustedCA\":{\"name\":\"registry-cas\"}}}' --type = merge","title":"Trust Registry CA"},{"location":"kubernetes/openshift/add-registry-ca/#adding-certificate-authorities-to-the-cluster","text":"You can add certificate authorities (CA) to the cluster for use when pushing and pulling images with the following procedure.","title":"Adding certificate authorities to the cluster"},{"location":"kubernetes/openshift/add-registry-ca/#prerequisites","text":"You must have access to the public certificates of the registry, usually a hostname/ca.crt file located in the /etc/docker/certs.d/ directory.","title":"Prerequisites"},{"location":"kubernetes/openshift/add-registry-ca/#procedure","text":"Create a ConfigMap in the openshift-config namespace containing the trusted certificates for the registries that use self-signed certificates. For each CA file, ensure the key in the ConfigMap is the hostname of the registry in the hostname[..port] format: oc create configmap registry-cas -n openshift-config \\ --from-file = myregistry.corp.com..5000 = /etc/docker/certs.d/myregistry.corp.com:5000/ca.crt \\ --from-file = otherregistry.com = /etc/docker/certs.d/otherregistry.com/ca.crt Update the cluster image configuration: oc patch image.config.openshift.io/cluster --patch '{\"spec\":{\"additionalTrustedCA\":{\"name\":\"registry-cas\"}}}' --type = merge","title":"Procedure"},{"location":"kubernetes/openshift/setup-htpasswd/","text":"Configuring an htpasswd identity provider in OpenShift Configure the htpasswd identity provider to allow users to log in to OpenShift Container Platform with credentials from an htpasswd file. Content Configuring an htpasswd identity provider in OpenShift Content Creating the htpasswd file Creating the htpasswd secret Adding the htpasswd identity provider to your cluster Creating the htpasswd file To use the htpasswd identity provider, you must generate a flat file that contains the user names and passwords for your cluster by using htpasswd . Prerequisites Have access to the htpasswd utility. On Red Hat Enterprise Linux this is available by installing the httpd-tools package. Procedure Create or update your flat file with a user name and hashed password: htpasswd -c -B -b </path/to/users.htpasswd> <username> <password> Continue to add or update credentials to the file: htpasswd -B -b </path/to/users.htpasswd> <user_name> <password> Creating the htpasswd secret To use the htpasswd identity provider, you must define a secret that contains the htpasswd user file. Prerequisites Create an htpasswd file. Procedure Create a Secret object that contains the htpasswd users file: oc create secret generic htpass-secret --from-file = htpasswd = <path_to_users.htpasswd> -n openshift-config Adding the htpasswd identity provider to your cluster The following custom resource (CR) shows the parameters and acceptable values for an htpasswd identity provider. cat <<EOF | oc apply -f - apiVersion : config.openshift.io/v1 kind : OAuth metadata : name : cluster spec : identityProviders : - name : htpasswd mappingMethod : claim type : HTPasswd htpasswd : fileData : name : htpass-secret EOF","title":"htpasswd IdP on OpenShift"},{"location":"kubernetes/openshift/setup-htpasswd/#configuring-an-htpasswd-identity-provider-in-openshift","text":"Configure the htpasswd identity provider to allow users to log in to OpenShift Container Platform with credentials from an htpasswd file.","title":"Configuring an htpasswd identity provider in OpenShift"},{"location":"kubernetes/openshift/setup-htpasswd/#content","text":"Configuring an htpasswd identity provider in OpenShift Content Creating the htpasswd file Creating the htpasswd secret Adding the htpasswd identity provider to your cluster","title":"Content"},{"location":"kubernetes/openshift/setup-htpasswd/#creating-the-htpasswd-file","text":"To use the htpasswd identity provider, you must generate a flat file that contains the user names and passwords for your cluster by using htpasswd . Prerequisites Have access to the htpasswd utility. On Red Hat Enterprise Linux this is available by installing the httpd-tools package. Procedure Create or update your flat file with a user name and hashed password: htpasswd -c -B -b </path/to/users.htpasswd> <username> <password> Continue to add or update credentials to the file: htpasswd -B -b </path/to/users.htpasswd> <user_name> <password>","title":"Creating the htpasswd file"},{"location":"kubernetes/openshift/setup-htpasswd/#creating-the-htpasswd-secret","text":"To use the htpasswd identity provider, you must define a secret that contains the htpasswd user file. Prerequisites Create an htpasswd file. Procedure Create a Secret object that contains the htpasswd users file: oc create secret generic htpass-secret --from-file = htpasswd = <path_to_users.htpasswd> -n openshift-config","title":"Creating the htpasswd secret"},{"location":"kubernetes/openshift/setup-htpasswd/#adding-the-htpasswd-identity-provider-to-your-cluster","text":"The following custom resource (CR) shows the parameters and acceptable values for an htpasswd identity provider. cat <<EOF | oc apply -f - apiVersion : config.openshift.io/v1 kind : OAuth metadata : name : cluster spec : identityProviders : - name : htpasswd mappingMethod : claim type : HTPasswd htpasswd : fileData : name : htpass-secret EOF","title":"Adding the htpasswd identity provider to your cluster"},{"location":"kubernetes/ops/cert-manager/","text":"Cert-Manager Cert-Manager is a powerful and extensible X.509 certificate controller for Kubernetes and OpenShift workloads. It will obtain certificates from a variety of Issuers, both popular public Issuers as well as private Issuers, and ensure the certificates are valid and up-to-date, and will attempt to renew certificates at a configured time before expiry. Prerequisites Kubernetes cluster accessible with kubectl CLI Helm CLI CFSSL CLI Install (Helm) helm repo add jetstack https://charts.jetstack.io helm repo update jetstack helm install \\ cert-manager jetstack/cert-manager \\ --namespace cert-manager \\ --create-namespace \\ --version v1.11.0 \\ --set installCRDs = true Self-Signed cluster issuer kubectl apply -f - <<EOF apiVersion : cert-manager.io/v1 kind : ClusterIssuer metadata : name : selfsigned-cluster-issuer spec : selfSigned : {} EOF You can then use it as follow: # example-ingress.yaml apiVersion : networking.k8s.io/v1 kind : Ingress metadata : name : kuard annotations : kubernetes.io/ingress.class : \"nginx\" cert-manager.io/issuer : \"selfsigned-cluster-issuer\" spec : tls : - hosts : - example.example.com secretName : quickstart-example-tls rules : - host : example.example.com http : paths : - path : / pathType : Prefix backend : service : name : kuard port : number : 80","title":"Cert-Manager"},{"location":"kubernetes/ops/cert-manager/#cert-manager","text":"Cert-Manager is a powerful and extensible X.509 certificate controller for Kubernetes and OpenShift workloads. It will obtain certificates from a variety of Issuers, both popular public Issuers as well as private Issuers, and ensure the certificates are valid and up-to-date, and will attempt to renew certificates at a configured time before expiry.","title":"Cert-Manager"},{"location":"kubernetes/ops/cert-manager/#prerequisites","text":"Kubernetes cluster accessible with kubectl CLI Helm CLI CFSSL CLI","title":"Prerequisites"},{"location":"kubernetes/ops/cert-manager/#install-helm","text":"helm repo add jetstack https://charts.jetstack.io helm repo update jetstack helm install \\ cert-manager jetstack/cert-manager \\ --namespace cert-manager \\ --create-namespace \\ --version v1.11.0 \\ --set installCRDs = true","title":"Install (Helm)"},{"location":"kubernetes/ops/cert-manager/#self-signed-cluster-issuer","text":"kubectl apply -f - <<EOF apiVersion : cert-manager.io/v1 kind : ClusterIssuer metadata : name : selfsigned-cluster-issuer spec : selfSigned : {} EOF You can then use it as follow: # example-ingress.yaml apiVersion : networking.k8s.io/v1 kind : Ingress metadata : name : kuard annotations : kubernetes.io/ingress.class : \"nginx\" cert-manager.io/issuer : \"selfsigned-cluster-issuer\" spec : tls : - hosts : - example.example.com secretName : quickstart-example-tls rules : - host : example.example.com http : paths : - path : / pathType : Prefix backend : service : name : kuard port : number : 80","title":"Self-Signed cluster issuer"},{"location":"kubernetes/ops/docker-registry/","text":"Docker Registry This document shows how to deploy an on-prem private Docker Registry on Kubernetes, using Helm. Prerequisites Kubernetes cluster accessible with kubectl CLI Helm CLI CFSSL CLI (at least cfssl and cfssljson ). htpasswd CLI Nginx Ingress Controller installed in the cluster Some LB (e.g. HAProxy) is set up to balance registry.<BASE_DOMAIN> to cluster Nginx Ingress Controller load balancer. Generate TLS certs Create a working directory $HOME/docker-registry : mkdir $HOME /docker-registry cd $HOME /docker-registry Get the Docker Registry Helm repository: helm repo add twuni https://twuni.github.io/docker-registry.helm helm repo update twuni Create a script registry-tls.sh that will generate TLS CA, certificate and corresponding secrets: touch registry-tls.sh Put the following content: #!/bin/bash set -e ############# ## setup environment NAMESPACE = ${ NAMESPACE :- docker -registry } RELEASE = ${ RELEASE :- registry } DOMAIN = ${ DOMAIN :- apps .k8s.example.com } ## stop if variable is unset beyond this point set -u ## known expected patterns for SAN CERT_SANS = \"*. ${ RELEASE } . ${ DOMAIN } ,*. ${ DOMAIN } \" ############# ## generate default CA config cfssl print-defaults config > ca-config.json ## generate a CA echo '{\"CN\":\"' ${ RELEASE } . ${ DOMAIN } .ca '\",\"key\":{\"algo\":\"ecdsa\",\"size\":256}}' | \\ cfssl gencert -initca - | \\ cfssljson -bare ca - ## generate certificate echo '{\"CN\":\"' ${ RELEASE } . ${ DOMAIN } '\",\"key\":{\"algo\":\"ecdsa\",\"size\":256}}' | \\ cfssl gencert -config = ca-config.json -ca = ca.pem -ca-key = ca-key.pem -profile www -hostname = \" ${ CERT_SANS } \" - | \\ cfssljson -bare ${ RELEASE } ############# ## load certificates into K8s kubectl delete secret ${ RELEASE } -tls -n ${ NAMESPACE } kubectl delete secret ${ RELEASE } -tls-ca -n ${ NAMESPACE } kubectl -n ${ NAMESPACE } create secret tls ${ RELEASE } -tls \\ --cert = ${ RELEASE } .pem \\ --key = ${ RELEASE } -key.pem kubectl -n ${ NAMESPACE } create secret generic ${ RELEASE } -tls-ca \\ --from-file = ${ RELEASE } . ${ DOMAIN } .crt = ca.pem Run the script, provide a valid base DOMAIN e.g. apps.k8s.example.com : chmod u+x registry-tls.sh export DOMAIN = \"<YOUR_DOMAIN>\" ./registry-tls.sh Configure and Deploy Helm Chart Generate default user and password for registry, provide valid USERNAME and PASSWORD : htpasswd -Bbn ${ USERNAME } ${ PASSWORD } > .htpasswd Install Helm Chart: helm upgrade --install registry twuni/docker-registry -n docker-registry --create-namespace --set secrets.htpasswd = $( cat .htpasswd ) Configure Ingress Create Ingress file, provide valid TLS hosts and rule host : # $HOME/docker-registry/ingress.yaml apiVersion : networking.k8s.io/v1 kind : Ingress metadata : name : registry-ingress namespace : docker-registry annotations : nginx.ingress.kubernetes.io/proxy-body-size : \"0\" nginx.ingress.kubernetes.io/proxy-read-timeout : \"600\" nginx.ingress.kubernetes.io/proxy-send-timeout : \"600\" spec : ingressClassName : nginx tls : - hosts : - registry.apps.k8s.example.com # change me secretName : registry-tls rules : - host : registry.apps.k8s.example.com # change me http : paths : - path : / pathType : Prefix backend : service : name : registry-docker-registry port : number : 5000 Apply ingress configuration: kubectl apply -f ingress.yaml Optional : Registry access from external K8s cluster On each worker node of your K8s cluster that will need access to this private registry, put the content of your ca.pem in a file /usr/local/share/ca-certificates/private-docker-registry.crt and restart your container runtime e.g. on Ubuntu with docker runtime: sudo su - cat <<EOF > /usr/local/share/ca-certificates/private-docker-registry.crt -----BEGIN CERTIFICATE----- ... -----END CERTIFICATE----- EOF sudo update-ca-certificates sudo systemctl restart docker","title":"Docker Registry"},{"location":"kubernetes/ops/docker-registry/#docker-registry","text":"This document shows how to deploy an on-prem private Docker Registry on Kubernetes, using Helm.","title":"Docker Registry"},{"location":"kubernetes/ops/docker-registry/#prerequisites","text":"Kubernetes cluster accessible with kubectl CLI Helm CLI CFSSL CLI (at least cfssl and cfssljson ). htpasswd CLI Nginx Ingress Controller installed in the cluster Some LB (e.g. HAProxy) is set up to balance registry.<BASE_DOMAIN> to cluster Nginx Ingress Controller load balancer.","title":"Prerequisites"},{"location":"kubernetes/ops/docker-registry/#generate-tls-certs","text":"Create a working directory $HOME/docker-registry : mkdir $HOME /docker-registry cd $HOME /docker-registry Get the Docker Registry Helm repository: helm repo add twuni https://twuni.github.io/docker-registry.helm helm repo update twuni Create a script registry-tls.sh that will generate TLS CA, certificate and corresponding secrets: touch registry-tls.sh Put the following content: #!/bin/bash set -e ############# ## setup environment NAMESPACE = ${ NAMESPACE :- docker -registry } RELEASE = ${ RELEASE :- registry } DOMAIN = ${ DOMAIN :- apps .k8s.example.com } ## stop if variable is unset beyond this point set -u ## known expected patterns for SAN CERT_SANS = \"*. ${ RELEASE } . ${ DOMAIN } ,*. ${ DOMAIN } \" ############# ## generate default CA config cfssl print-defaults config > ca-config.json ## generate a CA echo '{\"CN\":\"' ${ RELEASE } . ${ DOMAIN } .ca '\",\"key\":{\"algo\":\"ecdsa\",\"size\":256}}' | \\ cfssl gencert -initca - | \\ cfssljson -bare ca - ## generate certificate echo '{\"CN\":\"' ${ RELEASE } . ${ DOMAIN } '\",\"key\":{\"algo\":\"ecdsa\",\"size\":256}}' | \\ cfssl gencert -config = ca-config.json -ca = ca.pem -ca-key = ca-key.pem -profile www -hostname = \" ${ CERT_SANS } \" - | \\ cfssljson -bare ${ RELEASE } ############# ## load certificates into K8s kubectl delete secret ${ RELEASE } -tls -n ${ NAMESPACE } kubectl delete secret ${ RELEASE } -tls-ca -n ${ NAMESPACE } kubectl -n ${ NAMESPACE } create secret tls ${ RELEASE } -tls \\ --cert = ${ RELEASE } .pem \\ --key = ${ RELEASE } -key.pem kubectl -n ${ NAMESPACE } create secret generic ${ RELEASE } -tls-ca \\ --from-file = ${ RELEASE } . ${ DOMAIN } .crt = ca.pem Run the script, provide a valid base DOMAIN e.g. apps.k8s.example.com : chmod u+x registry-tls.sh export DOMAIN = \"<YOUR_DOMAIN>\" ./registry-tls.sh","title":"Generate TLS certs"},{"location":"kubernetes/ops/docker-registry/#configure-and-deploy-helm-chart","text":"Generate default user and password for registry, provide valid USERNAME and PASSWORD : htpasswd -Bbn ${ USERNAME } ${ PASSWORD } > .htpasswd Install Helm Chart: helm upgrade --install registry twuni/docker-registry -n docker-registry --create-namespace --set secrets.htpasswd = $( cat .htpasswd )","title":"Configure and Deploy Helm Chart"},{"location":"kubernetes/ops/docker-registry/#configure-ingress","text":"Create Ingress file, provide valid TLS hosts and rule host : # $HOME/docker-registry/ingress.yaml apiVersion : networking.k8s.io/v1 kind : Ingress metadata : name : registry-ingress namespace : docker-registry annotations : nginx.ingress.kubernetes.io/proxy-body-size : \"0\" nginx.ingress.kubernetes.io/proxy-read-timeout : \"600\" nginx.ingress.kubernetes.io/proxy-send-timeout : \"600\" spec : ingressClassName : nginx tls : - hosts : - registry.apps.k8s.example.com # change me secretName : registry-tls rules : - host : registry.apps.k8s.example.com # change me http : paths : - path : / pathType : Prefix backend : service : name : registry-docker-registry port : number : 5000 Apply ingress configuration: kubectl apply -f ingress.yaml","title":"Configure Ingress"},{"location":"kubernetes/ops/docker-registry/#optional-registry-access-from-external-k8s-cluster","text":"On each worker node of your K8s cluster that will need access to this private registry, put the content of your ca.pem in a file /usr/local/share/ca-certificates/private-docker-registry.crt and restart your container runtime e.g. on Ubuntu with docker runtime: sudo su - cat <<EOF > /usr/local/share/ca-certificates/private-docker-registry.crt -----BEGIN CERTIFICATE----- ... -----END CERTIFICATE----- EOF sudo update-ca-certificates sudo systemctl restart docker","title":"Optional: Registry access from external K8s cluster"},{"location":"kubernetes/ops/fluxcd/","text":"Enable Infra GitOps with FluxCD Flux is a set of continuous and progressive delivery solutions for Kubernetes that are open and extensible. Prerequisites A Kubernetes cluster. A GitHub personal access token with repo permissions . See the GitHub documentation on creating a personal access token . Getting started Heavily based on following documentation: https://fluxcd.io/flux/get-started/ Install the Flux CLI: brew install fluxcd/tap/flux Export your GitHub credentials: export GITHUB_TOKEN = <your-token> export GITHUB_USER = <your-username> Check you have everything needed to run Flux by running the following command: flux check --pre Install Flux onto your cluster: flux bootstrap github \\ --owner = $GITHUB_USER \\ --repository = k8s-gitops \\ --branch = main \\ --path = ./clusters/my-cluster \\ --personal Encrypting secrets using OpenPGP Generate a GPG/OpenPGP key with no passphrase ( %no-protection ): export KEY_NAME = \"cluster0.yourdomain.com\" export KEY_COMMENT = \"flux secrets\" gpg --batch --full-generate-key <<EOF %no-protection Key-Type: 1 Key-Length: 4096 Subkey-Type: 1 Subkey-Length: 4096 Expire-Date: 0 Name-Comment: ${KEY_COMMENT} Name-Real: ${KEY_NAME} EOF The above configuration creates an rsa4096 key that does not expire. For a full list of options to consider for your environment, see Unattended GPG key generation . Retrieve the GPG key fingerprint (second row of the sec column): gpg --list-secret-keys ${ KEY_NAME } Export the public and private key pair from your local GPG keyring and create a Kubernetes secret named sops-gpg in the flux-system namespace: gpg --export-secret-keys ${ KEY_NAME } | kubectl create secret generic sops-gpg \\ --namespace = flux-system \\ --from-file = sops.asc = /dev/stdin Note : It\u2019s a good idea to back up this secret-key/K8s-Secret with a password manager or offline storage. Configure in-cluster secrets decryption Create a kustomization for reconciling the secrets on the cluster: flux create kustomization my-secrets \\ --source = flux-system \\ --path = ./clusters/cluster0 \\ --prune = true \\ --interval = 10m \\ --decryption-provider = sops \\ --decryption-secret = sops-gpg Note that the sops-gpg can contain more than one key, SOPS will try to decrypt the secrets by iterating over all the private keys until it finds one that works. Optional: Export the public key into the Git directory Commit the public key to the repository so that team members who clone the repo can encrypt new files: gpg --export --armor ${ KEY_NAME } > ./clusters/cluster0/.sops.pub.asc Check the file contents to ensure it\u2019s the public key before adding it to the repo and committing. git add ./clusters/cluster0/.sops.pub.asc git commit -am 'Share GPG public key for secrets generation' Team members can then import this key when they pull the Git repository: gpg --import ./clusters/cluster0/.sops.pub.asc Note : The public key is sufficient for creating brand new files. The secret key is required for decrypting and editing existing files because SOPS computes a MAC on all values. When using solely the public key to add or remove a field, the whole file should be deleted and recreated. Encrypting secrets using OpenPGP Generate a Kubernetes secret manifest with kubectl : kubectl -n default create secret generic basic-auth \\ --from-literal = user = admin \\ --from-literal = password = change-me \\ --dry-run = client \\ -o yaml > basic-auth.yaml Encrypt the secret with SOPS using your GPG key: sops encrypt --in-place --pgp ${ PUB_KEY_FP } basic-auth.yaml You can now commit the encrypted secret to your Git repository. Note : Note that you shouldn\u2019t apply the encrypted secrets onto the cluster with kubectl. SOPS encrypted secrets are designed to be consumed by kustomize-controller. Quick examples Create Helm Release \u276f flux create hr open-webui \\ --namespace = chat \\ --source = HelmRepository/open-webui.flux-system \\ --chart = open-webui \\ --values = ../charts/open-webui.values.yaml \\ --chart-version = \"6.4.0\" --export > ./apps/k8s-home/chat/open-webui.release.yaml","title":"Enable Infra GitOps with FluxCD"},{"location":"kubernetes/ops/fluxcd/#enable-infra-gitops-with-fluxcd","text":"Flux is a set of continuous and progressive delivery solutions for Kubernetes that are open and extensible.","title":"Enable Infra GitOps with FluxCD"},{"location":"kubernetes/ops/fluxcd/#prerequisites","text":"A Kubernetes cluster. A GitHub personal access token with repo permissions . See the GitHub documentation on creating a personal access token .","title":"Prerequisites"},{"location":"kubernetes/ops/fluxcd/#getting-started","text":"Heavily based on following documentation: https://fluxcd.io/flux/get-started/ Install the Flux CLI: brew install fluxcd/tap/flux Export your GitHub credentials: export GITHUB_TOKEN = <your-token> export GITHUB_USER = <your-username> Check you have everything needed to run Flux by running the following command: flux check --pre Install Flux onto your cluster: flux bootstrap github \\ --owner = $GITHUB_USER \\ --repository = k8s-gitops \\ --branch = main \\ --path = ./clusters/my-cluster \\ --personal","title":"Getting started"},{"location":"kubernetes/ops/fluxcd/#encrypting-secrets-using-openpgp","text":"Generate a GPG/OpenPGP key with no passphrase ( %no-protection ): export KEY_NAME = \"cluster0.yourdomain.com\" export KEY_COMMENT = \"flux secrets\" gpg --batch --full-generate-key <<EOF %no-protection Key-Type: 1 Key-Length: 4096 Subkey-Type: 1 Subkey-Length: 4096 Expire-Date: 0 Name-Comment: ${KEY_COMMENT} Name-Real: ${KEY_NAME} EOF The above configuration creates an rsa4096 key that does not expire. For a full list of options to consider for your environment, see Unattended GPG key generation . Retrieve the GPG key fingerprint (second row of the sec column): gpg --list-secret-keys ${ KEY_NAME } Export the public and private key pair from your local GPG keyring and create a Kubernetes secret named sops-gpg in the flux-system namespace: gpg --export-secret-keys ${ KEY_NAME } | kubectl create secret generic sops-gpg \\ --namespace = flux-system \\ --from-file = sops.asc = /dev/stdin Note : It\u2019s a good idea to back up this secret-key/K8s-Secret with a password manager or offline storage.","title":"Encrypting secrets using OpenPGP"},{"location":"kubernetes/ops/fluxcd/#configure-in-cluster-secrets-decryption","text":"Create a kustomization for reconciling the secrets on the cluster: flux create kustomization my-secrets \\ --source = flux-system \\ --path = ./clusters/cluster0 \\ --prune = true \\ --interval = 10m \\ --decryption-provider = sops \\ --decryption-secret = sops-gpg Note that the sops-gpg can contain more than one key, SOPS will try to decrypt the secrets by iterating over all the private keys until it finds one that works.","title":"Configure in-cluster secrets decryption"},{"location":"kubernetes/ops/fluxcd/#optional-export-the-public-key-into-the-git-directory","text":"Commit the public key to the repository so that team members who clone the repo can encrypt new files: gpg --export --armor ${ KEY_NAME } > ./clusters/cluster0/.sops.pub.asc Check the file contents to ensure it\u2019s the public key before adding it to the repo and committing. git add ./clusters/cluster0/.sops.pub.asc git commit -am 'Share GPG public key for secrets generation' Team members can then import this key when they pull the Git repository: gpg --import ./clusters/cluster0/.sops.pub.asc Note : The public key is sufficient for creating brand new files. The secret key is required for decrypting and editing existing files because SOPS computes a MAC on all values. When using solely the public key to add or remove a field, the whole file should be deleted and recreated.","title":"Optional: Export the public key into the Git directory"},{"location":"kubernetes/ops/fluxcd/#encrypting-secrets-using-openpgp_1","text":"Generate a Kubernetes secret manifest with kubectl : kubectl -n default create secret generic basic-auth \\ --from-literal = user = admin \\ --from-literal = password = change-me \\ --dry-run = client \\ -o yaml > basic-auth.yaml Encrypt the secret with SOPS using your GPG key: sops encrypt --in-place --pgp ${ PUB_KEY_FP } basic-auth.yaml You can now commit the encrypted secret to your Git repository. Note : Note that you shouldn\u2019t apply the encrypted secrets onto the cluster with kubectl. SOPS encrypted secrets are designed to be consumed by kustomize-controller.","title":"Encrypting secrets using OpenPGP"},{"location":"kubernetes/ops/fluxcd/#quick-examples","text":"","title":"Quick examples"},{"location":"kubernetes/ops/fluxcd/#create-helm-release","text":"\u276f flux create hr open-webui \\ --namespace = chat \\ --source = HelmRepository/open-webui.flux-system \\ --chart = open-webui \\ --values = ../charts/open-webui.values.yaml \\ --chart-version = \"6.4.0\" --export > ./apps/k8s-home/chat/open-webui.release.yaml","title":"Create Helm Release"},{"location":"kubernetes/ops/hashicorp-vault/","text":"HashiCorp Vault HashiCorp Vault is an identity-based secrets and encryption management system. A secret is anything that you want to tightly control access to, such as API encryption keys, passwords, and certificates. Vault provides encryption services that are gated by authentication and authorization methods. Using Vault\u2019s UI, CLI, or HTTP API, access to secrets and other sensitive data can be securely stored and managed, tightly controlled (restricted), and auditable. Prerequisites Kubernetes cluster accessible with kubectl CLI Helm CLI If you want to setup external ingress, you'll need Nginx Ingress Controller installed in the cluster and Cert-Manager configured with letsencrypt-prod cluster issuer. Install HashiCorp Vault - Helm HashiCorp Vault Helm chart documentation . Add HashiCorp Helm repository: helm repo add hashicorp https://helm.releases.hashicorp.com helm repo update hashicorp Deploy Vault chart (uncomment ingress related values if you want to enable ingress): helm upgrade --install hc-vault hashicorp/vault -n hc-vault --create-namespace \\ --set global.tlsDisable = false # --set server.ingress.enabled=true \\ # --set server.ingress.ingressClassName=nginx \\ # --set-json server.ingress.annotations='{\"cert-manager.io/cluster-issuer\":\"letsencrypt-prod\"}' \\ # --set-json server.ingress.hosts='[{\"host\":\"hc-vault.example.com\"}]'\\ # --set-json server.ingress.tls='[{\"secretName\":\"hc-vault-tls\",\"hosts\":[\"hc-vault.example.com\"]}]' Access Vault UI If you have enabled ingress in above step and configured DNS resolution to your custom domain, you should be able to access the UI at https://hc-vault.example.com . If not, forward local machine port 8200 to HC Vault pod port 8200 : kubectl port-forward hc-vault-0 8200 :8200 Then open the following URL in your web browser: 127.0.0.1:8200 You can then get started by creating your first secret .","title":"HashiCorp Vault"},{"location":"kubernetes/ops/hashicorp-vault/#hashicorp-vault","text":"HashiCorp Vault is an identity-based secrets and encryption management system. A secret is anything that you want to tightly control access to, such as API encryption keys, passwords, and certificates. Vault provides encryption services that are gated by authentication and authorization methods. Using Vault\u2019s UI, CLI, or HTTP API, access to secrets and other sensitive data can be securely stored and managed, tightly controlled (restricted), and auditable.","title":"HashiCorp Vault"},{"location":"kubernetes/ops/hashicorp-vault/#prerequisites","text":"Kubernetes cluster accessible with kubectl CLI Helm CLI If you want to setup external ingress, you'll need Nginx Ingress Controller installed in the cluster and Cert-Manager configured with letsencrypt-prod cluster issuer.","title":"Prerequisites"},{"location":"kubernetes/ops/hashicorp-vault/#install-hashicorp-vault-helm","text":"HashiCorp Vault Helm chart documentation . Add HashiCorp Helm repository: helm repo add hashicorp https://helm.releases.hashicorp.com helm repo update hashicorp Deploy Vault chart (uncomment ingress related values if you want to enable ingress): helm upgrade --install hc-vault hashicorp/vault -n hc-vault --create-namespace \\ --set global.tlsDisable = false # --set server.ingress.enabled=true \\ # --set server.ingress.ingressClassName=nginx \\ # --set-json server.ingress.annotations='{\"cert-manager.io/cluster-issuer\":\"letsencrypt-prod\"}' \\ # --set-json server.ingress.hosts='[{\"host\":\"hc-vault.example.com\"}]'\\ # --set-json server.ingress.tls='[{\"secretName\":\"hc-vault-tls\",\"hosts\":[\"hc-vault.example.com\"]}]'","title":"Install HashiCorp Vault - Helm"},{"location":"kubernetes/ops/hashicorp-vault/#access-vault-ui","text":"If you have enabled ingress in above step and configured DNS resolution to your custom domain, you should be able to access the UI at https://hc-vault.example.com . If not, forward local machine port 8200 to HC Vault pod port 8200 : kubectl port-forward hc-vault-0 8200 :8200 Then open the following URL in your web browser: 127.0.0.1:8200 You can then get started by creating your first secret .","title":"Access Vault UI"},{"location":"kubernetes/ops/image-pull-secrets/","text":"Add image pull secret on K8s Create your authentication Secret to use from your registry authentication file (e.g. .config/containers/auth.json for Podman): kubectl create secret generic regcred \\ --from-file = .dockerconfigjson = <path/to/.docker/config.json> \\ --type = kubernetes.io/dockerconfigjson Next, modify the required service account (e.g. default ) for the namespace to use this Secret as an imagePullSecret . kubectl patch serviceaccount default -p '{\"imagePullSecrets\": [{\"name\": \"regcred\"}], \"secrets\": [{\"name\": \"regcred\"}]}' Note Above process fixes the following issue: Too Many Requests - Server message: toomanyrequests: You have reached your pull rate limit. You may increase the limit by authenticating and upgrading: https://www.docker.com/increase-rate-limit due to the pull rate limit when pulling image from Docker Hub with anonymous user, this limit is usually reached very quickly due to the nature of K8s pulling a lot of images from single IP.","title":"Add image pull secret on K8s"},{"location":"kubernetes/ops/image-pull-secrets/#add-image-pull-secret-on-k8s","text":"Create your authentication Secret to use from your registry authentication file (e.g. .config/containers/auth.json for Podman): kubectl create secret generic regcred \\ --from-file = .dockerconfigjson = <path/to/.docker/config.json> \\ --type = kubernetes.io/dockerconfigjson Next, modify the required service account (e.g. default ) for the namespace to use this Secret as an imagePullSecret . kubectl patch serviceaccount default -p '{\"imagePullSecrets\": [{\"name\": \"regcred\"}], \"secrets\": [{\"name\": \"regcred\"}]}'","title":"Add image pull secret on K8s"},{"location":"kubernetes/ops/image-pull-secrets/#note","text":"Above process fixes the following issue: Too Many Requests - Server message: toomanyrequests: You have reached your pull rate limit. You may increase the limit by authenticating and upgrading: https://www.docker.com/increase-rate-limit due to the pull rate limit when pulling image from Docker Hub with anonymous user, this limit is usually reached very quickly due to the nature of K8s pulling a lot of images from single IP.","title":"Note"},{"location":"kubernetes/ops/intel-gpu-plugin/","text":"Intel GPU plugin on Kubernetes This guide will help you get started with leveraging Intel GPUs in your Kubernetes cluster. Whether you're a seasoned Kubernetes user or just getting started, you'll find everything you need to to harness the power of Intel GPUs for your workloads. Let's dive in! \ud83c\udf1f Content Intel GPU plugin on Kubernetes Content Installation with NFD Conclusion References Installation with NFD Note : Replace <RELEASE_VERSION> with the desired release tag or main to get devel images. Note : Add --dry-run=client -o yaml to the kubectl commands below to visualize the yaml content being applied. Deploy GPU plugin with the help of NFD ( Node Feature Discovery ). It detects the presence of Intel GPUs and labels them accordingly. GPU plugin\u2019s node selector is used to deploy plugin to nodes which have such a GPU label. # Start NFD - if your cluster doesn't have NFD installed yet $ kubectl apply -k 'https://github.com/intel/intel-device-plugins-for-kubernetes/deployments/nfd?ref=<RELEASE_VERSION>' # Create NodeFeatureRules for detecting GPUs on nodes $ kubectl apply -k 'https://github.com/intel/intel-device-plugins-for-kubernetes/deployments/nfd/overlays/node-feature-rules?ref=<RELEASE_VERSION>' # Create GPU plugin daemonset $ kubectl apply -k 'https://github.com/intel/intel-device-plugins-for-kubernetes/deployments/gpu_plugin/overlays/nfd_labeled_nodes?ref=<RELEASE_VERSION>' Conclusion Congratulations! You've successfully set up the Intel GPU plugin on your Kubernetes cluster. With this setup, you can now leverage the power of Intel GPUs to accelerate your workloads. Enjoy the enhanced performance and capabilities that Intel GPUs bring to your Kubernetes environment! \ud83d\ude80\u2728 References Intel GPU plugin installation","title":"Intel GPU plugin on Kubernetes"},{"location":"kubernetes/ops/intel-gpu-plugin/#intel-gpu-plugin-on-kubernetes","text":"This guide will help you get started with leveraging Intel GPUs in your Kubernetes cluster. Whether you're a seasoned Kubernetes user or just getting started, you'll find everything you need to to harness the power of Intel GPUs for your workloads. Let's dive in! \ud83c\udf1f","title":"Intel GPU plugin on Kubernetes"},{"location":"kubernetes/ops/intel-gpu-plugin/#content","text":"Intel GPU plugin on Kubernetes Content Installation with NFD Conclusion References","title":"Content"},{"location":"kubernetes/ops/intel-gpu-plugin/#installation-with-nfd","text":"Note : Replace <RELEASE_VERSION> with the desired release tag or main to get devel images. Note : Add --dry-run=client -o yaml to the kubectl commands below to visualize the yaml content being applied. Deploy GPU plugin with the help of NFD ( Node Feature Discovery ). It detects the presence of Intel GPUs and labels them accordingly. GPU plugin\u2019s node selector is used to deploy plugin to nodes which have such a GPU label. # Start NFD - if your cluster doesn't have NFD installed yet $ kubectl apply -k 'https://github.com/intel/intel-device-plugins-for-kubernetes/deployments/nfd?ref=<RELEASE_VERSION>' # Create NodeFeatureRules for detecting GPUs on nodes $ kubectl apply -k 'https://github.com/intel/intel-device-plugins-for-kubernetes/deployments/nfd/overlays/node-feature-rules?ref=<RELEASE_VERSION>' # Create GPU plugin daemonset $ kubectl apply -k 'https://github.com/intel/intel-device-plugins-for-kubernetes/deployments/gpu_plugin/overlays/nfd_labeled_nodes?ref=<RELEASE_VERSION>'","title":"Installation with NFD"},{"location":"kubernetes/ops/intel-gpu-plugin/#conclusion","text":"Congratulations! You've successfully set up the Intel GPU plugin on your Kubernetes cluster. With this setup, you can now leverage the power of Intel GPUs to accelerate your workloads. Enjoy the enhanced performance and capabilities that Intel GPUs bring to your Kubernetes environment! \ud83d\ude80\u2728","title":"Conclusion"},{"location":"kubernetes/ops/intel-gpu-plugin/#references","text":"Intel GPU plugin installation","title":"References"},{"location":"kubernetes/ops/nfs-provisioner/","text":"NFS Provisioner Note Tested with K3S Setup NFS Server Create a NFS data mount (optional but recommended) Note Setup based on Ubuntu 20.04 using LVM Add a disk to the VM Create a LVM partition on the new disk (GPT) sudo cfdisk /dev/sdb Create PV/VG/LV and format the LV # Create the Physical Volume with the newly created partition sudo pvcreate /dev/sdb1 sudo pvdisplay # Create a Volume Group with the newly PV sudo vgcreate data-vg /dev/sdb1 sudo vgdisplay # # Create a Logcal Volumein the new VG sudo lvdisplay sudo lvcreate -l 100 %FREE -n data-lv data-vg #Format the new LV sudo mkfs.ext4 /dev/data-vg/data-lv Mount the new device # Create a folder to mount the device sudo mkdir /export sudo chown nobody:nogroup /export sudo chmod 775 /export #Get the LV Path sudo lvdisplay # --- Logical volume --- # LV Path /dev/data-vg/data-lv # ... # Get the device UUID sudo blkid /dev/data-vg/data-lv #/dev/data-vg/data-lv: UUID=\"0dd3c7f9-d42c-4fb3-9bbb-cbe35d6e4f80\" TYPE=\"ext4\" # Add device to fstab for automount sudo vim /etc/fstab # UUID=0dd3c7f9-d42c-4fb3-9bbb-cbe35d6e4f80 /export ext4 defaults 0 1 # Mount everything (allow to check if it will mount or not, and be sure the server will reboot :) ) sudo mount -a #check if it's correctly mounted mount Install NFS Server # Install NFS server package sudo apt install nfs-kernel-server # Add an export sudo vim /etc/exports # /export *(rw,sync,no_subtree_check,no_root_squash) sudo exportfs -ra Optional: Configure firewall # Get NFS Server port rpcinfo -p | grep nfs # Open port in firewall sudo ufw allow 2049 sudo ufw status Install NFS client on each Kubernetes node Note Setup based on Ubuntu 20.04 nodes sudo apt install nfs-common -y Deploy the NFS Provisioner https://github.com/kubernetes-sigs/nfs-subdir-external-provisioner Setup based on K3S Create a helmchart-nfs.yaml file: vim helmchart-nfs.yaml apiVersion : helm.cattle.io/v1 kind : HelmChart metadata : name : nfs namespace : nfs-provisioner spec : chart : nfs-subdir-external-provisioner repo : https://kubernetes-sigs.github.io/nfs-subdir-external-provisioner targetNamespace : nfs-provisioner set : nfs.server : 10.3.13.1 nfs.path : /export storageClass.name : nfs Create a new nfs-provisioner namespace and apply the yaml file: kubectl create ns nfs-provisioner kubectl -f helmchart-nfs.yaml -n helmchart-nfs.yaml kubectl get pod -n helmchart-nfs.yaml Setup based on Helm CLI Prerequisites: Install Helm CLI: https://helm.sh/docs/intro/install/ $ kubectl create ns nfs-provisioner $ helm repo add nfs-subdir-external-provisioner https://kubernetes-sigs.github.io/nfs-subdir-external-provisioner/ $ helm install nfs-subdir-external-provisioner nfs-subdir-external-provisioner/nfs-subdir-external-provisioner \\ --set nfs.server = 10 .3.13.1 \\ --set nfs.path = /export \\ --set storageClass.name = nfs \\ -n nfs-provisioner Set default Storage Class kubectl patch sc nfs -p '{\"metadata\": {\"annotations\":{\"storageclass.kubernetes.io/is-default-class\":\"true\"}}}' Test NFS Provisioning Create a Persistent Volume Claim kubectl create ns test-prov cat <<EOF | kubectl apply -n test-prov -f - kind: PersistentVolumeClaim apiVersion: v1 metadata: name: test-claim spec: storageClassName: nfs accessModes: - ReadWriteMany resources: requests: storage: 1Gi EOF Create Pod cat <<EOF | kubectl apply -n test-prov -f - apiVersion: apps/v1 kind: Deployment metadata: name: test-pvc spec: selector: matchLabels: app: test-pvc strategy: type: Recreate template: metadata: labels: app: test-pvc spec: containers: - name: test-pvc-container image: alpine:3.13 command: [ \"/bin/sh\", \"-c\", \"--\" ] args: [ \"while true; do echo $(hostname) $(date) >> /data/test;sleep 1; done;\" ] volumeMounts: - name: vol1 mountPath: \"/data\" volumes: - name: vol1 persistentVolumeClaim: claimName: test-claim EOF","title":"NFS Provisioner"},{"location":"kubernetes/ops/nfs-provisioner/#nfs-provisioner","text":"Note Tested with K3S","title":"NFS Provisioner"},{"location":"kubernetes/ops/nfs-provisioner/#setup-nfs-server","text":"","title":"Setup NFS Server"},{"location":"kubernetes/ops/nfs-provisioner/#create-a-nfs-data-mount-optional-but-recommended","text":"Note Setup based on Ubuntu 20.04 using LVM Add a disk to the VM Create a LVM partition on the new disk (GPT) sudo cfdisk /dev/sdb Create PV/VG/LV and format the LV # Create the Physical Volume with the newly created partition sudo pvcreate /dev/sdb1 sudo pvdisplay # Create a Volume Group with the newly PV sudo vgcreate data-vg /dev/sdb1 sudo vgdisplay # # Create a Logcal Volumein the new VG sudo lvdisplay sudo lvcreate -l 100 %FREE -n data-lv data-vg #Format the new LV sudo mkfs.ext4 /dev/data-vg/data-lv Mount the new device # Create a folder to mount the device sudo mkdir /export sudo chown nobody:nogroup /export sudo chmod 775 /export #Get the LV Path sudo lvdisplay # --- Logical volume --- # LV Path /dev/data-vg/data-lv # ... # Get the device UUID sudo blkid /dev/data-vg/data-lv #/dev/data-vg/data-lv: UUID=\"0dd3c7f9-d42c-4fb3-9bbb-cbe35d6e4f80\" TYPE=\"ext4\" # Add device to fstab for automount sudo vim /etc/fstab # UUID=0dd3c7f9-d42c-4fb3-9bbb-cbe35d6e4f80 /export ext4 defaults 0 1 # Mount everything (allow to check if it will mount or not, and be sure the server will reboot :) ) sudo mount -a #check if it's correctly mounted mount","title":"Create a NFS data mount (optional but recommended)"},{"location":"kubernetes/ops/nfs-provisioner/#install-nfs-server","text":"# Install NFS server package sudo apt install nfs-kernel-server # Add an export sudo vim /etc/exports # /export *(rw,sync,no_subtree_check,no_root_squash) sudo exportfs -ra","title":"Install NFS Server"},{"location":"kubernetes/ops/nfs-provisioner/#optional-configure-firewall","text":"# Get NFS Server port rpcinfo -p | grep nfs # Open port in firewall sudo ufw allow 2049 sudo ufw status","title":"Optional: Configure firewall"},{"location":"kubernetes/ops/nfs-provisioner/#install-nfs-client-on-each-kubernetes-node","text":"Note Setup based on Ubuntu 20.04 nodes sudo apt install nfs-common -y","title":"Install NFS client on each Kubernetes node"},{"location":"kubernetes/ops/nfs-provisioner/#deploy-the-nfs-provisioner","text":"https://github.com/kubernetes-sigs/nfs-subdir-external-provisioner","title":"Deploy the NFS Provisioner"},{"location":"kubernetes/ops/nfs-provisioner/#setup-based-on-k3s","text":"Create a helmchart-nfs.yaml file: vim helmchart-nfs.yaml apiVersion : helm.cattle.io/v1 kind : HelmChart metadata : name : nfs namespace : nfs-provisioner spec : chart : nfs-subdir-external-provisioner repo : https://kubernetes-sigs.github.io/nfs-subdir-external-provisioner targetNamespace : nfs-provisioner set : nfs.server : 10.3.13.1 nfs.path : /export storageClass.name : nfs Create a new nfs-provisioner namespace and apply the yaml file: kubectl create ns nfs-provisioner kubectl -f helmchart-nfs.yaml -n helmchart-nfs.yaml kubectl get pod -n helmchart-nfs.yaml","title":"Setup based on K3S"},{"location":"kubernetes/ops/nfs-provisioner/#setup-based-on-helm-cli","text":"Prerequisites: Install Helm CLI: https://helm.sh/docs/intro/install/ $ kubectl create ns nfs-provisioner $ helm repo add nfs-subdir-external-provisioner https://kubernetes-sigs.github.io/nfs-subdir-external-provisioner/ $ helm install nfs-subdir-external-provisioner nfs-subdir-external-provisioner/nfs-subdir-external-provisioner \\ --set nfs.server = 10 .3.13.1 \\ --set nfs.path = /export \\ --set storageClass.name = nfs \\ -n nfs-provisioner","title":"Setup based on Helm CLI"},{"location":"kubernetes/ops/nfs-provisioner/#set-default-storage-class","text":"kubectl patch sc nfs -p '{\"metadata\": {\"annotations\":{\"storageclass.kubernetes.io/is-default-class\":\"true\"}}}'","title":"Set default Storage Class"},{"location":"kubernetes/ops/nfs-provisioner/#test-nfs-provisioning","text":"","title":"Test NFS Provisioning"},{"location":"kubernetes/ops/nfs-provisioner/#create-a-persistent-volume-claim","text":"kubectl create ns test-prov cat <<EOF | kubectl apply -n test-prov -f - kind: PersistentVolumeClaim apiVersion: v1 metadata: name: test-claim spec: storageClassName: nfs accessModes: - ReadWriteMany resources: requests: storage: 1Gi EOF","title":"Create a Persistent Volume Claim"},{"location":"kubernetes/ops/nfs-provisioner/#create-pod","text":"cat <<EOF | kubectl apply -n test-prov -f - apiVersion: apps/v1 kind: Deployment metadata: name: test-pvc spec: selector: matchLabels: app: test-pvc strategy: type: Recreate template: metadata: labels: app: test-pvc spec: containers: - name: test-pvc-container image: alpine:3.13 command: [ \"/bin/sh\", \"-c\", \"--\" ] args: [ \"while true; do echo $(hostname) $(date) >> /data/test;sleep 1; done;\" ] volumeMounts: - name: vol1 mountPath: \"/data\" volumes: - name: vol1 persistentVolumeClaim: claimName: test-claim EOF","title":"Create Pod"},{"location":"kubernetes/ops/nginx-ingress/","text":"NGINX Ingress Controller The NGINX Ingress Controller is an implementation of a Kubernetes Ingress Controller for NGINX. The Ingress is a Kubernetes resource that lets you configure an HTTP load balancer for applications running on Kubernetes, represented by one or more Services. Such a load balancer is necessary to deliver those applications to clients outside of the Kubernetes cluster. The Ingress resource supports Content-based routing and TLS/SSL termination for each hostname. Prerequisites Kubernetes cluster accessible with kubectl CLI Install Helm Install helm upgrade --install ingress-nginx ingress-nginx \\ --repo https://kubernetes.github.io/ingress-nginx \\ --namespace ingress-nginx --create-namespace","title":"NGINX Ingress Controller"},{"location":"kubernetes/ops/nginx-ingress/#nginx-ingress-controller","text":"The NGINX Ingress Controller is an implementation of a Kubernetes Ingress Controller for NGINX. The Ingress is a Kubernetes resource that lets you configure an HTTP load balancer for applications running on Kubernetes, represented by one or more Services. Such a load balancer is necessary to deliver those applications to clients outside of the Kubernetes cluster. The Ingress resource supports Content-based routing and TLS/SSL termination for each hostname.","title":"NGINX Ingress Controller"},{"location":"kubernetes/ops/nginx-ingress/#prerequisites","text":"Kubernetes cluster accessible with kubectl CLI Install Helm","title":"Prerequisites"},{"location":"kubernetes/ops/nginx-ingress/#install","text":"helm upgrade --install ingress-nginx ingress-nginx \\ --repo https://kubernetes.github.io/ingress-nginx \\ --namespace ingress-nginx --create-namespace","title":"Install"},{"location":"kubernetes/ops/tools/","text":"Useful Tools Useful tools (aliases, plugins, commands) that I found useful on a daily basis of managing K8s clusters: Aliases Useful aliases you can add to your .bashrc or .zshrc for day to day Kubernetes operations: alias k = 'kubectl' # kn $1 to switch current namespace alias kn = 'f() { [ \"$1\" ] && kubectl config set-context --current --namespace $1 || kubectl config view --minify | grep namespace | cut -d\" \" -f6 ; } ; f' # kx $1 to switch current context alias kx = 'f() { [ \"$1\" ] && kubectl config use-context $1 || kubectl config current-context ; } ; f' Plugins Useful plugins you can install to speed your day to day Kubernetes operations: Prerequisites kubectl CLI Kubernetes krew plugin manager: ( set -x ; cd \" $( mktemp -d ) \" && OS = \" $( uname | tr '[:upper:]' '[:lower:]' ) \" && ARCH = \" $( uname -m | sed -e 's/x86_64/amd64/' -e 's/\\(arm\\)\\(64\\)\\?.*/\\1\\2/' -e 's/aarch64$/arm64/' ) \" && KREW = \"krew- ${ OS } _ ${ ARCH } \" && curl -fsSLO \"https://github.com/kubernetes-sigs/krew/releases/latest/download/ ${ KREW } .tar.gz\" && tar zxvf \" ${ KREW } .tar.gz\" && ./ \" ${ KREW } \" install krew ) echo 'export PATH=\"${KREW_ROOT:-$HOME/.krew}/bin:$PATH\"' >> ~/.bashrc source ~/.bashrc kubectl ctx : Context switcher kubectl krew install ctx kubectl ctx # List contexts kubectl ctx ${ NEW_CTX } # Switch current context to ${NEW_CTX} kubectl ns : Namespace switcher kubectl krew install ns kubectl ns # List namespaces kubectl ns ${ NEW_NS } # Switch current namespace to ${NEW_NS} Commands Delete a namespace that is stuck terminating : export NAMESPACE = <CHANGEME> kubectl proxy & kubectl get namespace $NAMESPACE -o json | jq '.spec = {\"finalizers\":[]}' >/tmp/patch.json curl -k -H \"Content-Type: application/json\" -X PUT --data-binary @/tmp/patch.json 127 .0.0.1:8001/api/v1/namespaces/ $NAMESPACE /finalize Delete stuck objects: kubectl patch ${ TYPE } ${ NAME } -p '{\"metadata\":{\"finalizers\": []}}' --type = merge","title":"Useful Tools"},{"location":"kubernetes/ops/tools/#useful-tools","text":"Useful tools (aliases, plugins, commands) that I found useful on a daily basis of managing K8s clusters:","title":"Useful Tools"},{"location":"kubernetes/ops/tools/#aliases","text":"Useful aliases you can add to your .bashrc or .zshrc for day to day Kubernetes operations: alias k = 'kubectl' # kn $1 to switch current namespace alias kn = 'f() { [ \"$1\" ] && kubectl config set-context --current --namespace $1 || kubectl config view --minify | grep namespace | cut -d\" \" -f6 ; } ; f' # kx $1 to switch current context alias kx = 'f() { [ \"$1\" ] && kubectl config use-context $1 || kubectl config current-context ; } ; f'","title":"Aliases"},{"location":"kubernetes/ops/tools/#plugins","text":"Useful plugins you can install to speed your day to day Kubernetes operations:","title":"Plugins"},{"location":"kubernetes/ops/tools/#prerequisites","text":"kubectl CLI Kubernetes krew plugin manager: ( set -x ; cd \" $( mktemp -d ) \" && OS = \" $( uname | tr '[:upper:]' '[:lower:]' ) \" && ARCH = \" $( uname -m | sed -e 's/x86_64/amd64/' -e 's/\\(arm\\)\\(64\\)\\?.*/\\1\\2/' -e 's/aarch64$/arm64/' ) \" && KREW = \"krew- ${ OS } _ ${ ARCH } \" && curl -fsSLO \"https://github.com/kubernetes-sigs/krew/releases/latest/download/ ${ KREW } .tar.gz\" && tar zxvf \" ${ KREW } .tar.gz\" && ./ \" ${ KREW } \" install krew ) echo 'export PATH=\"${KREW_ROOT:-$HOME/.krew}/bin:$PATH\"' >> ~/.bashrc source ~/.bashrc","title":"Prerequisites"},{"location":"kubernetes/ops/tools/#kubectl-ctx-context-switcher","text":"kubectl krew install ctx kubectl ctx # List contexts kubectl ctx ${ NEW_CTX } # Switch current context to ${NEW_CTX}","title":"kubectl ctx: Context switcher"},{"location":"kubernetes/ops/tools/#kubectl-ns-namespace-switcher","text":"kubectl krew install ns kubectl ns # List namespaces kubectl ns ${ NEW_NS } # Switch current namespace to ${NEW_NS}","title":"kubectl ns: Namespace switcher"},{"location":"kubernetes/ops/tools/#commands","text":"Delete a namespace that is stuck terminating : export NAMESPACE = <CHANGEME> kubectl proxy & kubectl get namespace $NAMESPACE -o json | jq '.spec = {\"finalizers\":[]}' >/tmp/patch.json curl -k -H \"Content-Type: application/json\" -X PUT --data-binary @/tmp/patch.json 127 .0.0.1:8001/api/v1/namespaces/ $NAMESPACE /finalize Delete stuck objects: kubectl patch ${ TYPE } ${ NAME } -p '{\"metadata\":{\"finalizers\": []}}' --type = merge","title":"Commands"},{"location":"kubernetes/software/artifactory/","text":"JFrog Artifactory Prerequisites Kubernetes cluster accessible with kubectl CLI Install Helm Supporting Docs Artifactory Helm Chart Artifactory Get the JFrog Helm repository: helm repo add jfrog https://charts.jfrog.io helm repo update jfrog Install the chart: helm upgrade --install artifactory --namespace artifactory jfrog/artifactory-oss --create-namespace Optional : if running on OpenShift, grant privileged SCC to the default service account in artifactory namespace: oc adm policy add-scc-to-user privileged -z default -n artifactory","title":"JFrog Artifactory"},{"location":"kubernetes/software/artifactory/#jfrog-artifactory","text":"","title":"JFrog Artifactory"},{"location":"kubernetes/software/artifactory/#prerequisites","text":"Kubernetes cluster accessible with kubectl CLI Install Helm","title":"Prerequisites"},{"location":"kubernetes/software/artifactory/#supporting-docs","text":"Artifactory Helm Chart","title":"Supporting Docs"},{"location":"kubernetes/software/artifactory/#artifactory","text":"Get the JFrog Helm repository: helm repo add jfrog https://charts.jfrog.io helm repo update jfrog Install the chart: helm upgrade --install artifactory --namespace artifactory jfrog/artifactory-oss --create-namespace Optional : if running on OpenShift, grant privileged SCC to the default service account in artifactory namespace: oc adm policy add-scc-to-user privileged -z default -n artifactory","title":"Artifactory"},{"location":"kubernetes/software/gitlab/","text":"GitLab EE This document shows how to deploy an on-prem private GitLab Instance on Kubernetes, using Helm. Prerequisites Kubernetes cluster accessible with kubectl CLI Install Helm Install CFSSL CLI (at least cfssl and cfssljson ). Supporting Docs GitLab EE GitLab Runner GitLab on K8s Create a working directory $HOME/gitlab : mkdir $HOME/gitlab cd $HOME/gitlab Get the GitLab Helm repository: helm repo add gitlab https://charts.gitlab.io helm repo update gitlab Create a script gitlab-tls.sh that will generate GitLab TLS CA and certificate and corresponding secrets: touch gitlab-tls.sh Put the following content: #!/bin/bash set -e ############# ## setup environment NAMESPACE = ${ NAMESPACE :- gitlab } RELEASE = ${ RELEASE :- gitlab } DOMAIN = ${ DOMAIN :- apps .k8s.example.com } ## stop if variable is unset beyond this point set -u ## known expected patterns for SAN CERT_SANS = \"*. ${ RELEASE } . ${ DOMAIN } ,*. ${ DOMAIN } \" ############# ## generate default CA config cfssl print-defaults config > ca-config.json ## generate a CA echo '{\"CN\":\"' ${ RELEASE } . ${ DOMAIN } .ca '\",\"key\":{\"algo\":\"ecdsa\",\"size\":256}}' | \\ cfssl gencert -initca - | \\ cfssljson -bare ca - ## generate certificate echo '{\"CN\":\"' ${ RELEASE } . ${ DOMAIN } '\",\"key\":{\"algo\":\"ecdsa\",\"size\":256}}' | \\ cfssl gencert -config = ca-config.json -ca = ca.pem -ca-key = ca-key.pem -profile www -hostname = \" ${ CERT_SANS } \" - | \\ cfssljson -bare ${ RELEASE } ############# ## load certificates into K8s kubectl create ns ${ NAMESPACE } kubectl -n ${ NAMESPACE } create secret tls ${ RELEASE } -tls \\ --cert = ${ RELEASE } .pem \\ --key = ${ RELEASE } -key.pem kubectl -n ${ NAMESPACE } create secret generic ${ RELEASE } -tls-ca \\ --from-file = ${ RELEASE } . ${ DOMAIN } .crt = ca.pem Run the script, provide a valid base DOMAIN e.g. apps.k8s.example.com : chmod u+x gitlab-tls.sh export DOMAIN = \"<YOUR_DOMAIN>\" ./gitlab-tls.sh Populate your Helm chart config gitlab.values.yaml like the following, provide a valid domain : # $HOME/gitlab/gitlab.values.yaml global : hosts : domain : apps.k8s.example.com ingress : configureCertmanager : false tls : enabled : true secretName : gitlab-tls certificates : customCAs : - secret : gitlab-tls-ca certmanager : installCRDs : false install : false gitlab-runner : install : false Note : you can check all available configuration by running helm show values gitlab/gitlab . Install Helm chart using your custom configuration: helm install gitlab gitlab/gitlab --timeout = 600s --values gitlab.values.yaml -n gitlab Wait until all pods are running: watch kubectl get pods -n gitlab GitLab Runner Create a sub folder gitlab-runner and move to there: mkdir gitlab-runner cd gitlab-runner Create your GitLab Runner configuration gitlab-runner.values.yaml like the following, provide a valid gitlabUrl and runnerRegistrationToken : # $HOME/gitlab/gitlab-runner/gitlab-runner.values.yaml gitlabUrl : https://gitlab.apps.k8s.example.com runnerRegistrationToken : \"AoDG...31SGP\" certsSecretName : gitlab-tls-ca Note : see how to retrieve your runner registration token . Install Helm chart using your custom configuration: helm install --namespace gitlab gitlab-runner -f gitlab-runner.values.yaml gitlab/gitlab-runner Wait for runner pod to be up and running: watch kubectl get pods -n gitlab Gitlab on OpenShift Create Gitlab namespace: kubectl create ns gitlab Create your Gitlab values file: cat <<EOF > gitlab.values.yaml nginx-ingress : enabled : false gitlab-runner : install : false certmanager-issuer : email : john.doe@example.com #CHANGEME global : hosts : domain : apps.openshift.example.com # CHANGEME ingress : class : none annotations : route.openshift.io/termination : edge serviceAccount : enabled : true create : false name : gitlab EOF Grant privileged SCC to service accounts used by Gitlab: oc adm policy add-scc-to-user privileged -z = default,gitlab,gitlab-shared-secrets,gitlab-certmanager,gitlab-certmanager-cainjector,gitlab-certmanager-issuer,gitlab-certmanager-webhook,gitlab-prometheus-server,gitlab-redis -n gitlab Get the Gitlab Helm repository: helm repo add gitlab https://charts.gitlab.io helm repo update gitlab Deploy your Helm chart: helm upgrade --install gitlab gitlab/gitlab \\ --timeout 600s --values gitlab.values.yaml Now grab a cup of and wait for Gitlab to be deployed.","title":"GitLab EE"},{"location":"kubernetes/software/gitlab/#gitlab-ee","text":"This document shows how to deploy an on-prem private GitLab Instance on Kubernetes, using Helm.","title":"GitLab EE"},{"location":"kubernetes/software/gitlab/#prerequisites","text":"Kubernetes cluster accessible with kubectl CLI Install Helm Install CFSSL CLI (at least cfssl and cfssljson ).","title":"Prerequisites"},{"location":"kubernetes/software/gitlab/#supporting-docs","text":"GitLab EE GitLab Runner","title":"Supporting Docs"},{"location":"kubernetes/software/gitlab/#gitlab-on-k8s","text":"Create a working directory $HOME/gitlab : mkdir $HOME/gitlab cd $HOME/gitlab Get the GitLab Helm repository: helm repo add gitlab https://charts.gitlab.io helm repo update gitlab Create a script gitlab-tls.sh that will generate GitLab TLS CA and certificate and corresponding secrets: touch gitlab-tls.sh Put the following content: #!/bin/bash set -e ############# ## setup environment NAMESPACE = ${ NAMESPACE :- gitlab } RELEASE = ${ RELEASE :- gitlab } DOMAIN = ${ DOMAIN :- apps .k8s.example.com } ## stop if variable is unset beyond this point set -u ## known expected patterns for SAN CERT_SANS = \"*. ${ RELEASE } . ${ DOMAIN } ,*. ${ DOMAIN } \" ############# ## generate default CA config cfssl print-defaults config > ca-config.json ## generate a CA echo '{\"CN\":\"' ${ RELEASE } . ${ DOMAIN } .ca '\",\"key\":{\"algo\":\"ecdsa\",\"size\":256}}' | \\ cfssl gencert -initca - | \\ cfssljson -bare ca - ## generate certificate echo '{\"CN\":\"' ${ RELEASE } . ${ DOMAIN } '\",\"key\":{\"algo\":\"ecdsa\",\"size\":256}}' | \\ cfssl gencert -config = ca-config.json -ca = ca.pem -ca-key = ca-key.pem -profile www -hostname = \" ${ CERT_SANS } \" - | \\ cfssljson -bare ${ RELEASE } ############# ## load certificates into K8s kubectl create ns ${ NAMESPACE } kubectl -n ${ NAMESPACE } create secret tls ${ RELEASE } -tls \\ --cert = ${ RELEASE } .pem \\ --key = ${ RELEASE } -key.pem kubectl -n ${ NAMESPACE } create secret generic ${ RELEASE } -tls-ca \\ --from-file = ${ RELEASE } . ${ DOMAIN } .crt = ca.pem Run the script, provide a valid base DOMAIN e.g. apps.k8s.example.com : chmod u+x gitlab-tls.sh export DOMAIN = \"<YOUR_DOMAIN>\" ./gitlab-tls.sh Populate your Helm chart config gitlab.values.yaml like the following, provide a valid domain : # $HOME/gitlab/gitlab.values.yaml global : hosts : domain : apps.k8s.example.com ingress : configureCertmanager : false tls : enabled : true secretName : gitlab-tls certificates : customCAs : - secret : gitlab-tls-ca certmanager : installCRDs : false install : false gitlab-runner : install : false Note : you can check all available configuration by running helm show values gitlab/gitlab . Install Helm chart using your custom configuration: helm install gitlab gitlab/gitlab --timeout = 600s --values gitlab.values.yaml -n gitlab Wait until all pods are running: watch kubectl get pods -n gitlab","title":"GitLab on K8s"},{"location":"kubernetes/software/gitlab/#gitlab-runner","text":"Create a sub folder gitlab-runner and move to there: mkdir gitlab-runner cd gitlab-runner Create your GitLab Runner configuration gitlab-runner.values.yaml like the following, provide a valid gitlabUrl and runnerRegistrationToken : # $HOME/gitlab/gitlab-runner/gitlab-runner.values.yaml gitlabUrl : https://gitlab.apps.k8s.example.com runnerRegistrationToken : \"AoDG...31SGP\" certsSecretName : gitlab-tls-ca Note : see how to retrieve your runner registration token . Install Helm chart using your custom configuration: helm install --namespace gitlab gitlab-runner -f gitlab-runner.values.yaml gitlab/gitlab-runner Wait for runner pod to be up and running: watch kubectl get pods -n gitlab","title":"GitLab Runner"},{"location":"kubernetes/software/gitlab/#gitlab-on-openshift","text":"Create Gitlab namespace: kubectl create ns gitlab Create your Gitlab values file: cat <<EOF > gitlab.values.yaml nginx-ingress : enabled : false gitlab-runner : install : false certmanager-issuer : email : john.doe@example.com #CHANGEME global : hosts : domain : apps.openshift.example.com # CHANGEME ingress : class : none annotations : route.openshift.io/termination : edge serviceAccount : enabled : true create : false name : gitlab EOF Grant privileged SCC to service accounts used by Gitlab: oc adm policy add-scc-to-user privileged -z = default,gitlab,gitlab-shared-secrets,gitlab-certmanager,gitlab-certmanager-cainjector,gitlab-certmanager-issuer,gitlab-certmanager-webhook,gitlab-prometheus-server,gitlab-redis -n gitlab Get the Gitlab Helm repository: helm repo add gitlab https://charts.gitlab.io helm repo update gitlab Deploy your Helm chart: helm upgrade --install gitlab gitlab/gitlab \\ --timeout 600s --values gitlab.values.yaml Now grab a cup of and wait for Gitlab to be deployed.","title":"Gitlab on OpenShift"},{"location":"kubernetes/software/harbor/","text":"Harbor Harbor is an open source registry that secures artifacts with policies and role-based access control, ensures images are scanned and free from vulnerabilities, and signs images as trusted. Harbor, a CNCF Graduated project, delivers compliance, performance, and interoperability to help you consistently and securely manage artifacts across cloud native compute platforms like Kubernetes and Docker. Prerequisites Kubernetes/OpenShift cluster accessible with kubectl CLI Install Helm yq CLI installed on your workstation Supporting Docs Deploying Harbor with High Availability via Helm Install Harbor Get the Harbor Helm repository: helm repo add harbor https://helm.goharbor.io helm repo update harbor Create harbor namespace: kubectl create ns harbor OpenShift Only : If installing in OpenShift, add the privileged security context constraint to default service account in the harbor namespace: oc adm policy add-scc-to-user privileged -z default -n harbor Install the chart, provide a valid HARBOR_DOMAIN : export HARBOR_DOMAIN = harbor.example.com helm install harbor harbor/harbor -n harbor --create-namespace --set externalURL = https://core. ${ HARBOR_DOMAIN } --set expose.ingress.hosts.core = core. ${ HARBOR_DOMAIN } --set expose.ingress.hosts.notary = notary. ${ HARBOR_DOMAIN } After a successful deployment and if your ingress strategy is properly configured you should be able to access your Harbor instance at https://core.harbor.example.com . To login you will find the admin password by running the following command: kubectl get secret -n harbor harbor-core -o yaml | yq .data.HARBOR_ADMIN_PASSWORD | base64 -d","title":"Harbor"},{"location":"kubernetes/software/harbor/#harbor","text":"Harbor is an open source registry that secures artifacts with policies and role-based access control, ensures images are scanned and free from vulnerabilities, and signs images as trusted. Harbor, a CNCF Graduated project, delivers compliance, performance, and interoperability to help you consistently and securely manage artifacts across cloud native compute platforms like Kubernetes and Docker.","title":"Harbor"},{"location":"kubernetes/software/harbor/#prerequisites","text":"Kubernetes/OpenShift cluster accessible with kubectl CLI Install Helm yq CLI installed on your workstation","title":"Prerequisites"},{"location":"kubernetes/software/harbor/#supporting-docs","text":"Deploying Harbor with High Availability via Helm","title":"Supporting Docs"},{"location":"kubernetes/software/harbor/#install-harbor","text":"Get the Harbor Helm repository: helm repo add harbor https://helm.goharbor.io helm repo update harbor Create harbor namespace: kubectl create ns harbor OpenShift Only : If installing in OpenShift, add the privileged security context constraint to default service account in the harbor namespace: oc adm policy add-scc-to-user privileged -z default -n harbor Install the chart, provide a valid HARBOR_DOMAIN : export HARBOR_DOMAIN = harbor.example.com helm install harbor harbor/harbor -n harbor --create-namespace --set externalURL = https://core. ${ HARBOR_DOMAIN } --set expose.ingress.hosts.core = core. ${ HARBOR_DOMAIN } --set expose.ingress.hosts.notary = notary. ${ HARBOR_DOMAIN } After a successful deployment and if your ingress strategy is properly configured you should be able to access your Harbor instance at https://core.harbor.example.com . To login you will find the admin password by running the following command: kubectl get secret -n harbor harbor-core -o yaml | yq .data.HARBOR_ADMIN_PASSWORD | base64 -d","title":"Install Harbor"},{"location":"kubernetes/software/media-center/","text":"Self-hosted media center on K8s Overview FlareSolverr : proxy server to bypass Cloudflare protection. qflood : qBittorrent client with Flood UI. FlareSolverr : Indexer manager/proxy built on the popular arr net base stack to integrate with your various PVR apps. Prowlarr Indexer manager/proxy built on the popular arr net base stack to integrate with your various PVR apps. Sonarr : tv series collection manager for Usenet and BitTorrent users. Radarr : movie collection manager for Usenet and BitTorrent users. Jellyfin : Free Software Media System that puts you in control of managing and streaming your media. Jellyseerr : fork of Overseerr with support for Jellyfin and Emby. It can be used to manage requests for your media library. Jellystat : A free and open source Statistics App for Jellyfin. Prerequisites Kubernetes cluster running on amd64 (because of TrueCharts dependency) accessible with kubectl CLI Note : Tested on K3s cluster with default traefik ONI. Install Helm Helm repo Get the TrueCharts Helm repository: helm repo add truecharts https://charts.truecharts.org helm repo update truecharts Setup environment variables export MEDIA_NAMESPACE = \"media-center\" export STORAGE_CLASS = \"nfs-hdd\" # changeme export MEDIA_VOLUME_SIZE = \"4Ti\" # changeme export LB_IP = \"192.168.1.81\" # changeme Create base resources Create namespace: kubectl create ns ${ MEDIA_NAMESPACE } Create PVC: cat <<EOF | kubectl apply -f - apiVersion: v1 kind: PersistentVolumeClaim metadata: name: media-pvc namespace: ${MEDIA_NAMESPACE} spec: storageClassName: ${STORAGE_CLASS} resources: requests: storage: ${MEDIA_VOLUME_SIZE} volumeMode: Filesystem accessModes: - ReadWriteMany EOF BitTorrent client Install the chart: cat <<EOF > qflood.values.yaml ingress: main: enabled: true hosts: - host: qflood.${LB_IP}.nip.io paths: - path: / integrations: traefik: enabled: false operator: verify: enabled: false persistence: media: enabled: true existingClaim: media-pvc mountPath: \"/media\" EOF helm install qflood truecharts/qflood --values qflood.values.yaml --namespace ${ MEDIA_NAMESPACE } After install you should be able to reach your Flood UI on your local network at qflood.${LB_IP}.nip.io . Flaresolverr Install the chart: helm install flaresolverr truecharts/flaresolverr Prowlarr Install the chart: cat <<EOF > prowlarr.values.yaml ingress: main: enabled: true hosts: - host: prowlarr.${LB_IP}.nip.io paths: - path: / integrations: traefik: enabled: false operator: verify: enabled: false EOF helm install prowlarr truecharts/prowlarr --values prowlarr.values.yaml --namespace ${ MEDIA_NAMESPACE } After install you should be able to reach your Prowlarr instance on your local network at prowlarr.${LB_IP}.nip.io . Sonarr Install the chart: cat <<EOF > sonarr.values.yaml ingress: main: enabled: true hosts: - host: sonarr.${LB_IP}.nip.io paths: - path: / integrations: traefik: enabled: false operator: verify: enabled: false persistence: media: enabled: true existingClaim: media-pvc mountPath: \"/media\" EOF helm install sonarr truecharts/sonarr --values sonarr.values.yaml --namespace ${ MEDIA_NAMESPACE } After install you should be able to reach your Sonarr instance on your local network at sonarr.${LB_IP}.nip.io . Radarr Install the chart: cat <<EOF > radarr.values.yaml ingress: main: enabled: true hosts: - host: radarr.${LB_IP}.nip.io paths: - path: / integrations: traefik: enabled: false operator: verify: enabled: false persistence: media: enabled: true existingClaim: media-pvc mountPath: \"/media\" EOF helm install radarr truecharts/radarr --values radarr.values.yaml --namespace ${ MEDIA_NAMESPACE } After install you should be able to reach your Radarr instance on your local network at radarr.${LB_IP}.nip.io . You can now configure Radarr: - In Prowlarr UI, Jellyfin Install the chart: cat <<EOF > jf.values.yaml ingress: main: enabled: true hosts: - host: jf.${LB_IP}.nip.io paths: - path: / integrations: traefik: enabled: false operator: verify: enabled: false persistence: media: enabled: true existingClaim: media-pvc mountPath: \"/media\" EOF helm install jellyfin truecharts/jellyfin --values jf.values.yaml --namespace ${ MEDIA_NAMESPACE } After install you should be able to reach your Jellyfin instance on your local network at jf.${LB_IP}.nip.io . Jellyseerr Install the chart: cat <<EOF > jellyseerr.values.yaml ingress: main: enabled: true hosts: - host: jellyseerr.${LB_IP}.nip.io paths: - path: / integrations: traefik: enabled: false operator: verify: enabled: false EOF helm install jellyseerr truecharts/jellyseerr --values jellyseerr.values.yaml --namespace ${ MEDIA_NAMESPACE } After install you should be able to reach your Jellyseerr instance on your local network at jellyseerr.${LB_IP}.nip.io . Configure Jellyseerr: - Open UI at jellyseerr.${LB_IP}.nip.io . - Connect using your Jellyfin server and account. - Setup connection to Radarr and Sonarr. Jellystat Install the chart: cat <<EOF > jellystat.values.yaml ingress: main: enabled: true hosts: - host: jellystat.${LB_IP}.nip.io paths: - path: / integrations: traefik: enabled: false operator: verify: enabled: false EOF helm install jellystat truecharts/jellystat --values jellystat.values.yaml --namespace ${ MEDIA_NAMESPACE } After install you should be able to reach your Jellystat instance on your local network at jellystat.${LB_IP}.nip.io . Optional : BitTorrent client + VPN setup Prerequisites Download a wireguard configuration from your VPN provider. Note : the following example is done with ProtonVPN that supports both wireguard and port forwarding for fast seeding. Use your existing media-pvc PVC (or whatever name you use for it). Setup Create a qbittorrent.values.yaml file with the following template updated with your wireguard configuration and ingress hostname: export QBIT_PASSWORD = <changeme> cat <<EOF > qbittorrent.values.yaml addons: vpn: config: | # PUT YOUR WIREGUARD CONFIG HERE env: VPN_SERVICE_PROVIDER: custom VPN_TYPE: wireguard VPN_PORT_FORWARDING: on VPN_PORT_FORWARDING_PROVIDER: protonvpn excludedNetworks_IPv4: - 192.168.1.0/24 # Replace with your local network CIDR type: gluetun ingress: main: enabled: true hosts: - host: qbit.${LB_IP}.nip.io paths: - path: / integrations: traefik: enabled: false metrics: main: enabled: false operator: verify: enabled: false persistence: config: enabled: true mountPath: /config media: enabled: true existingClaim: media-pvc mountPath: /media qbitportforward: QBT_PASSWORD: ${QBIT_PASSWORD} QBT_USERNAME: admin enabled: true EOF Install the Helm chart: helm install qbittorrent truecharts/qbittorrent --values qbittorrent.values.yaml Once deployed, update the qbittorrent deployment to edit the mountPath of the vpnconfig in the qbittorrent-vpn (Gluetun) container as follow: - mountPath: /gluetun/wireguard/wg0.conf name: vpnconfig subPath: vpn.conf After checking that qbittorrent pod successfully running, navigate to your qbittorrent UI at qbit.${LB_IP}.nip.io , login with temporary admin password found in the qbittorrent pod logs, and go to settings > Web UI to update admin password and set it to ${QBIT_PASSWORD} . Now wait for qbittorrent-qbitportforward job to complete, then go back to your qBittorrent UI settings to update the settings: In Downloads , your downloads and torrents folders to target your media volume, e.g.: In Connection you check that the listening port is correct by comparing it with the one returned by your qbittorrent-gluetun service ( curl http://qbittorrent-gluetun:8000/v1/openvpn/portforwarded from K8s namespace network). In BitTorrent make sure Enable Local Peer Discovery to find more peers is disabled. In Advanced > Network interface: select the tun0 interface. You can validate that qbittorrent is using the VPN interface by using the magnet link from whatismyip.net and checking the IP that's returned.","title":"Self-hosted media center on K8s"},{"location":"kubernetes/software/media-center/#self-hosted-media-center-on-k8s","text":"","title":"Self-hosted media center on K8s"},{"location":"kubernetes/software/media-center/#overview","text":"FlareSolverr : proxy server to bypass Cloudflare protection. qflood : qBittorrent client with Flood UI. FlareSolverr : Indexer manager/proxy built on the popular arr net base stack to integrate with your various PVR apps. Prowlarr Indexer manager/proxy built on the popular arr net base stack to integrate with your various PVR apps. Sonarr : tv series collection manager for Usenet and BitTorrent users. Radarr : movie collection manager for Usenet and BitTorrent users. Jellyfin : Free Software Media System that puts you in control of managing and streaming your media. Jellyseerr : fork of Overseerr with support for Jellyfin and Emby. It can be used to manage requests for your media library. Jellystat : A free and open source Statistics App for Jellyfin.","title":"Overview"},{"location":"kubernetes/software/media-center/#prerequisites","text":"Kubernetes cluster running on amd64 (because of TrueCharts dependency) accessible with kubectl CLI Note : Tested on K3s cluster with default traefik ONI. Install Helm","title":"Prerequisites"},{"location":"kubernetes/software/media-center/#helm-repo","text":"Get the TrueCharts Helm repository: helm repo add truecharts https://charts.truecharts.org helm repo update truecharts","title":"Helm repo"},{"location":"kubernetes/software/media-center/#setup-environment-variables","text":"export MEDIA_NAMESPACE = \"media-center\" export STORAGE_CLASS = \"nfs-hdd\" # changeme export MEDIA_VOLUME_SIZE = \"4Ti\" # changeme export LB_IP = \"192.168.1.81\" # changeme","title":"Setup environment variables"},{"location":"kubernetes/software/media-center/#create-base-resources","text":"Create namespace: kubectl create ns ${ MEDIA_NAMESPACE } Create PVC: cat <<EOF | kubectl apply -f - apiVersion: v1 kind: PersistentVolumeClaim metadata: name: media-pvc namespace: ${MEDIA_NAMESPACE} spec: storageClassName: ${STORAGE_CLASS} resources: requests: storage: ${MEDIA_VOLUME_SIZE} volumeMode: Filesystem accessModes: - ReadWriteMany EOF","title":"Create base resources"},{"location":"kubernetes/software/media-center/#bittorrent-client","text":"Install the chart: cat <<EOF > qflood.values.yaml ingress: main: enabled: true hosts: - host: qflood.${LB_IP}.nip.io paths: - path: / integrations: traefik: enabled: false operator: verify: enabled: false persistence: media: enabled: true existingClaim: media-pvc mountPath: \"/media\" EOF helm install qflood truecharts/qflood --values qflood.values.yaml --namespace ${ MEDIA_NAMESPACE } After install you should be able to reach your Flood UI on your local network at qflood.${LB_IP}.nip.io .","title":"BitTorrent client"},{"location":"kubernetes/software/media-center/#flaresolverr","text":"Install the chart: helm install flaresolverr truecharts/flaresolverr","title":"Flaresolverr"},{"location":"kubernetes/software/media-center/#prowlarr","text":"Install the chart: cat <<EOF > prowlarr.values.yaml ingress: main: enabled: true hosts: - host: prowlarr.${LB_IP}.nip.io paths: - path: / integrations: traefik: enabled: false operator: verify: enabled: false EOF helm install prowlarr truecharts/prowlarr --values prowlarr.values.yaml --namespace ${ MEDIA_NAMESPACE } After install you should be able to reach your Prowlarr instance on your local network at prowlarr.${LB_IP}.nip.io .","title":"Prowlarr"},{"location":"kubernetes/software/media-center/#sonarr","text":"Install the chart: cat <<EOF > sonarr.values.yaml ingress: main: enabled: true hosts: - host: sonarr.${LB_IP}.nip.io paths: - path: / integrations: traefik: enabled: false operator: verify: enabled: false persistence: media: enabled: true existingClaim: media-pvc mountPath: \"/media\" EOF helm install sonarr truecharts/sonarr --values sonarr.values.yaml --namespace ${ MEDIA_NAMESPACE } After install you should be able to reach your Sonarr instance on your local network at sonarr.${LB_IP}.nip.io .","title":"Sonarr"},{"location":"kubernetes/software/media-center/#radarr","text":"Install the chart: cat <<EOF > radarr.values.yaml ingress: main: enabled: true hosts: - host: radarr.${LB_IP}.nip.io paths: - path: / integrations: traefik: enabled: false operator: verify: enabled: false persistence: media: enabled: true existingClaim: media-pvc mountPath: \"/media\" EOF helm install radarr truecharts/radarr --values radarr.values.yaml --namespace ${ MEDIA_NAMESPACE } After install you should be able to reach your Radarr instance on your local network at radarr.${LB_IP}.nip.io . You can now configure Radarr: - In Prowlarr UI,","title":"Radarr"},{"location":"kubernetes/software/media-center/#jellyfin","text":"Install the chart: cat <<EOF > jf.values.yaml ingress: main: enabled: true hosts: - host: jf.${LB_IP}.nip.io paths: - path: / integrations: traefik: enabled: false operator: verify: enabled: false persistence: media: enabled: true existingClaim: media-pvc mountPath: \"/media\" EOF helm install jellyfin truecharts/jellyfin --values jf.values.yaml --namespace ${ MEDIA_NAMESPACE } After install you should be able to reach your Jellyfin instance on your local network at jf.${LB_IP}.nip.io .","title":"Jellyfin"},{"location":"kubernetes/software/media-center/#jellyseerr","text":"Install the chart: cat <<EOF > jellyseerr.values.yaml ingress: main: enabled: true hosts: - host: jellyseerr.${LB_IP}.nip.io paths: - path: / integrations: traefik: enabled: false operator: verify: enabled: false EOF helm install jellyseerr truecharts/jellyseerr --values jellyseerr.values.yaml --namespace ${ MEDIA_NAMESPACE } After install you should be able to reach your Jellyseerr instance on your local network at jellyseerr.${LB_IP}.nip.io . Configure Jellyseerr: - Open UI at jellyseerr.${LB_IP}.nip.io . - Connect using your Jellyfin server and account. - Setup connection to Radarr and Sonarr.","title":"Jellyseerr"},{"location":"kubernetes/software/media-center/#jellystat","text":"Install the chart: cat <<EOF > jellystat.values.yaml ingress: main: enabled: true hosts: - host: jellystat.${LB_IP}.nip.io paths: - path: / integrations: traefik: enabled: false operator: verify: enabled: false EOF helm install jellystat truecharts/jellystat --values jellystat.values.yaml --namespace ${ MEDIA_NAMESPACE } After install you should be able to reach your Jellystat instance on your local network at jellystat.${LB_IP}.nip.io .","title":"Jellystat"},{"location":"kubernetes/software/media-center/#optional-bittorrent-client-vpn-setup","text":"","title":"Optional: BitTorrent client + VPN setup"},{"location":"kubernetes/software/media-center/#prerequisites_1","text":"Download a wireguard configuration from your VPN provider. Note : the following example is done with ProtonVPN that supports both wireguard and port forwarding for fast seeding. Use your existing media-pvc PVC (or whatever name you use for it).","title":"Prerequisites"},{"location":"kubernetes/software/media-center/#setup","text":"Create a qbittorrent.values.yaml file with the following template updated with your wireguard configuration and ingress hostname: export QBIT_PASSWORD = <changeme> cat <<EOF > qbittorrent.values.yaml addons: vpn: config: | # PUT YOUR WIREGUARD CONFIG HERE env: VPN_SERVICE_PROVIDER: custom VPN_TYPE: wireguard VPN_PORT_FORWARDING: on VPN_PORT_FORWARDING_PROVIDER: protonvpn excludedNetworks_IPv4: - 192.168.1.0/24 # Replace with your local network CIDR type: gluetun ingress: main: enabled: true hosts: - host: qbit.${LB_IP}.nip.io paths: - path: / integrations: traefik: enabled: false metrics: main: enabled: false operator: verify: enabled: false persistence: config: enabled: true mountPath: /config media: enabled: true existingClaim: media-pvc mountPath: /media qbitportforward: QBT_PASSWORD: ${QBIT_PASSWORD} QBT_USERNAME: admin enabled: true EOF Install the Helm chart: helm install qbittorrent truecharts/qbittorrent --values qbittorrent.values.yaml Once deployed, update the qbittorrent deployment to edit the mountPath of the vpnconfig in the qbittorrent-vpn (Gluetun) container as follow: - mountPath: /gluetun/wireguard/wg0.conf name: vpnconfig subPath: vpn.conf After checking that qbittorrent pod successfully running, navigate to your qbittorrent UI at qbit.${LB_IP}.nip.io , login with temporary admin password found in the qbittorrent pod logs, and go to settings > Web UI to update admin password and set it to ${QBIT_PASSWORD} . Now wait for qbittorrent-qbitportforward job to complete, then go back to your qBittorrent UI settings to update the settings: In Downloads , your downloads and torrents folders to target your media volume, e.g.: In Connection you check that the listening port is correct by comparing it with the one returned by your qbittorrent-gluetun service ( curl http://qbittorrent-gluetun:8000/v1/openvpn/portforwarded from K8s namespace network). In BitTorrent make sure Enable Local Peer Discovery to find more peers is disabled. In Advanced > Network interface: select the tun0 interface. You can validate that qbittorrent is using the VPN interface by using the magnet link from whatismyip.net and checking the IP that's returned.","title":"Setup"},{"location":"kubernetes/software/mysql/","text":"MySQL MySQL is an open-source relational database management system (RDBMS) available under the terms of the GNU General Public License. To deploy MysQL in Kubernetes (or Red Hat OpenShift), we are going to use a Helm chart provided by Bitnami. Bitnami makes it easy to get open source software up and running on any platform, including laptop, Kubernetes and major cloud providers. Prerequisites Kubernetes cluster accessible with kubectl CLI Install Helm Supporting Docs MySQL Helm Chart Install MySQL Get the bitnami Helm repository: helm repo add bitnami https://charts.bitnami.com/bitnami helm repo update bitnami Install the chart (provide a valid ${NAMESPACE} ): helm install mysql bitnami/mysql -n ${ NAMESPACE } Optional : if running on OpenShift, grant privileged SCC to the mysql service account in ${NAMESPACE} namespace: oc adm policy add-scc-to-user privileged -z mysql -n ${ NAMESPACE } Access MySQL instance Retrieve root password from the created mysql by running the following command: kubectl get secret mysql -o yaml | yq .data.mysql-root-password | base64 -d Use port forwarding to access MySQL using kubectl port-forward command: kubectl port-forward mysql-0 3306 :3306 On a second terminal, you can then access your MySQL instance by running the following command (type your root password retrieved in above steps): mysql -h 127 .0.0.1 -p","title":"MySQL"},{"location":"kubernetes/software/mysql/#mysql","text":"MySQL is an open-source relational database management system (RDBMS) available under the terms of the GNU General Public License. To deploy MysQL in Kubernetes (or Red Hat OpenShift), we are going to use a Helm chart provided by Bitnami. Bitnami makes it easy to get open source software up and running on any platform, including laptop, Kubernetes and major cloud providers.","title":"MySQL"},{"location":"kubernetes/software/mysql/#prerequisites","text":"Kubernetes cluster accessible with kubectl CLI Install Helm","title":"Prerequisites"},{"location":"kubernetes/software/mysql/#supporting-docs","text":"MySQL Helm Chart","title":"Supporting Docs"},{"location":"kubernetes/software/mysql/#install-mysql","text":"Get the bitnami Helm repository: helm repo add bitnami https://charts.bitnami.com/bitnami helm repo update bitnami Install the chart (provide a valid ${NAMESPACE} ): helm install mysql bitnami/mysql -n ${ NAMESPACE } Optional : if running on OpenShift, grant privileged SCC to the mysql service account in ${NAMESPACE} namespace: oc adm policy add-scc-to-user privileged -z mysql -n ${ NAMESPACE }","title":"Install MySQL"},{"location":"kubernetes/software/mysql/#access-mysql-instance","text":"Retrieve root password from the created mysql by running the following command: kubectl get secret mysql -o yaml | yq .data.mysql-root-password | base64 -d Use port forwarding to access MySQL using kubectl port-forward command: kubectl port-forward mysql-0 3306 :3306 On a second terminal, you can then access your MySQL instance by running the following command (type your root password retrieved in above steps): mysql -h 127 .0.0.1 -p","title":"Access MySQL instance"},{"location":"kubernetes/software/sonarqube/","text":"SonarQube SonarQube is a self-managed, automatic code review tool that systematically helps you deliver clean code. As a core element of our Sonar solution, SonarQube integrates into your existing workflow and detects issues in your code to help you perform continuous code inspections of your projects. The tool analyses 30+ different programming languages and integrates into your CI pipeline and DevOps platform to ensure that your code meets high-quality standards. Prerequisites Kubernetes cluster accessible with kubectl CLI Install Helm Supporting Docs SonarQube Helm Chart SonarQube Get the bitnami Helm repository: helm repo add bitnami https://charts.bitnami.com/bitnami helm repo update bitnami Install the chart: export SONARQUBE_PASSWORD = <CHANGE_ME> helm install sonarqube bitnami/sonarqube -n sonarqube --create-namespace --set sonarqubePassword = ${ SONARQUBE_PASSWORD } Optional : if running on OpenShift, grant privileged SCC to the default and sonarqube service accounts in sonarqube namespace: oc adm policy add-scc-to-user privileged -z default -n sonarqube oc adm policy add-scc-to-user privileged -z sonarqube -n sonarqube","title":"SonarQube"},{"location":"kubernetes/software/sonarqube/#sonarqube","text":"SonarQube is a self-managed, automatic code review tool that systematically helps you deliver clean code. As a core element of our Sonar solution, SonarQube integrates into your existing workflow and detects issues in your code to help you perform continuous code inspections of your projects. The tool analyses 30+ different programming languages and integrates into your CI pipeline and DevOps platform to ensure that your code meets high-quality standards.","title":"SonarQube"},{"location":"kubernetes/software/sonarqube/#prerequisites","text":"Kubernetes cluster accessible with kubectl CLI Install Helm","title":"Prerequisites"},{"location":"kubernetes/software/sonarqube/#supporting-docs","text":"SonarQube Helm Chart","title":"Supporting Docs"},{"location":"kubernetes/software/sonarqube/#sonarqube_1","text":"Get the bitnami Helm repository: helm repo add bitnami https://charts.bitnami.com/bitnami helm repo update bitnami Install the chart: export SONARQUBE_PASSWORD = <CHANGE_ME> helm install sonarqube bitnami/sonarqube -n sonarqube --create-namespace --set sonarqubePassword = ${ SONARQUBE_PASSWORD } Optional : if running on OpenShift, grant privileged SCC to the default and sonarqube service accounts in sonarqube namespace: oc adm policy add-scc-to-user privileged -z default -n sonarqube oc adm policy add-scc-to-user privileged -z sonarqube -n sonarqube","title":"SonarQube"},{"location":"kubernetes/software/wikijs/","text":"Wiki.js Wiki.js is an open source Wiki software that allows you to enjoy writing documentation using a beautiful and intuitive interface! Prerequisites Kubernetes cluster accessible with kubectl CLI Cert Manager configured in K8s cluster, with letsencrypt-prod ClusterIssuer created. Install Helm Optional : Create custom PostgreSQL DB Note : This is only required for arm based deployment as the default Wiki.js PostgreSQL image is only available for amd64 architectures. helm repo add romanow https://romanow.github.io/helm-charts/ helm repo update helm install wiki-pg romanow/postgres -n wiki --create-namespace Deploy Wiki.js using Helm Edit your wiki.values.yaml file to configure your deployment: vim wiki.values.yaml # wiki.values.yaml ingress : enabled : true annotations : kubernetes.io/ingress.class : nginx kubernetes.io/tls-acme : \"true\" cert-manager.io/cluster-issuer : \"letsencrypt-prod\" hosts : - host : wiki.seillama.dev paths : - path : \"/\" pathType : Prefix tls : - secretName : wiki-tls hosts : - wiki.seillama.dev postgresql : enabled : false postgresqlHost : wiki-pg postgresqlUser : postgres postgresqlDatabase : postgres existingSecret : wiki existingSecretKey : postgresql-password Deploy Wiki.js helm repo add requarks https://charts.js.wiki helm repo update helm install wiki requarks/wiki -n wiki -f wiki.values.yaml","title":"Wiki.js"},{"location":"kubernetes/software/wikijs/#wikijs","text":"Wiki.js is an open source Wiki software that allows you to enjoy writing documentation using a beautiful and intuitive interface!","title":"Wiki.js"},{"location":"kubernetes/software/wikijs/#prerequisites","text":"Kubernetes cluster accessible with kubectl CLI Cert Manager configured in K8s cluster, with letsencrypt-prod ClusterIssuer created. Install Helm","title":"Prerequisites"},{"location":"kubernetes/software/wikijs/#optional-create-custom-postgresql-db","text":"Note : This is only required for arm based deployment as the default Wiki.js PostgreSQL image is only available for amd64 architectures. helm repo add romanow https://romanow.github.io/helm-charts/ helm repo update helm install wiki-pg romanow/postgres -n wiki --create-namespace","title":"Optional: Create custom PostgreSQL DB"},{"location":"kubernetes/software/wikijs/#deploy-wikijs-using-helm","text":"Edit your wiki.values.yaml file to configure your deployment: vim wiki.values.yaml # wiki.values.yaml ingress : enabled : true annotations : kubernetes.io/ingress.class : nginx kubernetes.io/tls-acme : \"true\" cert-manager.io/cluster-issuer : \"letsencrypt-prod\" hosts : - host : wiki.seillama.dev paths : - path : \"/\" pathType : Prefix tls : - secretName : wiki-tls hosts : - wiki.seillama.dev postgresql : enabled : false postgresqlHost : wiki-pg postgresqlUser : postgres postgresqlDatabase : postgres existingSecret : wiki existingSecretKey : postgresql-password Deploy Wiki.js helm repo add requarks https://charts.js.wiki helm repo update helm install wiki requarks/wiki -n wiki -f wiki.values.yaml","title":"Deploy Wiki.js using Helm"},{"location":"linux/dns-server-bind/","text":"Private DNS server using BIND To set up a private DNS server on Linux using BIND (Berkeley Internet Name Domain), you can follow these steps: Update the package lists and install BIND by running the following commands in the terminal: sudo apt update sudo apt install bind9 Configure BIND by editing the main configuration file /etc/bind/named.conf.options . Open the file using a text editor: sudo vim /etc/bind/named.conf.options Inside the file, you can modify the options section to include the IP addresses of your DNS server. You can add the following lines at the end of the options section: forwarders { 8.8.8.8; 8.8.4.4; }; These IP addresses are Google's public DNS servers, which will be used as forwarders for external DNS queries if your DNS server doesn't have the necessary information. Create a new zone file for your domain. Zone files contain the DNS records for a specific domain. In this example, we will create a zone file for a domain named \"example.com\": sudo vim /etc/bind/named.conf.local Add the following lines to the file: zone \"example.com\" { type master; file \"/etc/bind/db.example.com\"; }; Create the zone file /etc/bind/db.example.com and add the necessary DNS records. Replace \"your_IP_address\" with the IP address of your DNS server: sudo vim /etc/bind/db.example.com Add the following lines to the file: $TTL 86400 @ IN SOA ns1.example.com. admin.example.com. ( 2023062401 ; Serial 3600 ; Refresh 1800 ; Retry 604800 ; Expire 86400 ) ; Minimum TTL @ IN NS ns1.example.com. ns1 IN A your_IP_address Restart the BIND service to apply the changes: sudo service bind9 restart Configure your DNS client machines to use your private DNS server. You can usually do this by modifying the DNS server settings in the network configuration of each client. That's it! You have now set up a private DNS server using BIND on Linux. You can add additional DNS records to the zone file to suit your needs. Remember to replace \"example.com\" with your actual domain name and \"your_IP_address\" with the IP address of your DNS server.","title":"Private DNS server using BIND"},{"location":"linux/dns-server-bind/#private-dns-server-using-bind","text":"To set up a private DNS server on Linux using BIND (Berkeley Internet Name Domain), you can follow these steps: Update the package lists and install BIND by running the following commands in the terminal: sudo apt update sudo apt install bind9 Configure BIND by editing the main configuration file /etc/bind/named.conf.options . Open the file using a text editor: sudo vim /etc/bind/named.conf.options Inside the file, you can modify the options section to include the IP addresses of your DNS server. You can add the following lines at the end of the options section: forwarders { 8.8.8.8; 8.8.4.4; }; These IP addresses are Google's public DNS servers, which will be used as forwarders for external DNS queries if your DNS server doesn't have the necessary information. Create a new zone file for your domain. Zone files contain the DNS records for a specific domain. In this example, we will create a zone file for a domain named \"example.com\": sudo vim /etc/bind/named.conf.local Add the following lines to the file: zone \"example.com\" { type master; file \"/etc/bind/db.example.com\"; }; Create the zone file /etc/bind/db.example.com and add the necessary DNS records. Replace \"your_IP_address\" with the IP address of your DNS server: sudo vim /etc/bind/db.example.com Add the following lines to the file: $TTL 86400 @ IN SOA ns1.example.com. admin.example.com. ( 2023062401 ; Serial 3600 ; Refresh 1800 ; Retry 604800 ; Expire 86400 ) ; Minimum TTL @ IN NS ns1.example.com. ns1 IN A your_IP_address Restart the BIND service to apply the changes: sudo service bind9 restart Configure your DNS client machines to use your private DNS server. You can usually do this by modifying the DNS server settings in the network configuration of each client. That's it! You have now set up a private DNS server using BIND on Linux. You can add additional DNS records to the zone file to suit your needs. Remember to replace \"example.com\" with your actual domain name and \"your_IP_address\" with the IP address of your DNS server.","title":"Private DNS server using BIND"},{"location":"linux/gpu-passthru-proxmox/","text":"Intel iGPU passthru to Proxmox VM Hey there! \ud83d\udc4b In this section, we'll be covering the Full iGPU Passthrough to a Proxmox Virtual Machine for an Intel integrated iGPU. Warning : You will lose the ability to use the onboard graphics card to access the Proxmox console since Proxmox won't be able to use the Intel's GPU. Edit the grub configuration file /etc/default/grub and find the line that starts with GRUB_CMDLINE_LINUX_DEFAULT . By default, it should look like this: GRUB_CMD_LINUX_DEFAULT=\"quiet\" Add intel_iommu=on iommu=pt to this line. Update the grub configuration for the next reboot with update-grub . Add vfio modules to /etc/modules to allow PCI passthrough: # Modules required for PCI passthrough vfio vfio_iommu_type1 vfio_pci vfio_virqfd # Modules required for Intel GVT-g Split kvmgt Update configuration changes made in your /etc filesystem: update-initramfs -u -k all Reboot Proxmox. Verify that IOMMU is enabled: dmesg | grep -e DMAR -e IOMMU Note : There should be a line that looks like DMAR: IOMMU enabled . If there is no output, something is wrong. Find the PCI address of the iGPU: lspci -nnv | grep VGA . This should result in output similar to this (If you have multiple VGA, look for the one that has the Intel in the name): 00:02.0 VGA compatible controller [0300]: Intel Corporation CometLake-S GT2 [UHD Graphics 630] [8086:3e92] (prog-if 00 [VGA controller]) Create or edit your VM. Make sure the Machine type is set to q35 . Open the web GUI and navigate to the Hardware tab of the VM you want to add a vGPU. Click Add above the device list and then choose PCI Device . Open the Device dropdown and select the iGPU, which you can find using its PCI address. This list uses a different format for the PCI addresses id, 00:02.0 is listed as `0000:00:02.0 Select All Functions , ROM-Bar , PCI-Express and then click Add . Connect to the VM via SSH or any other remote access protocol you prefer. Install the latest version of Intel's Graphics Driver or use the Intel Driver & Support Assistant. Linux VM: Boot the VM. To test the iGPU passthrough was successful, you can use the following command: sudo lspci -nnv | grep VGA The output should include the Intel iGPU: 00:10.0 VGA compatible controller [0300]: Intel Corporation UHD Graphics 630 (Desktop) [8086:3e92] (prog-if 00 [VGA controller]) Now we need to check if the GPU's Driver initialization is working. cd /dev/dri && ls -la The output should include the renderD128 . In conclusion, successfully passing through an Intel iGPU to a Proxmox VM involves several critical steps, from modifying the GRUB configuration to ensuring the correct modules are loaded and the VM is properly configured. By following these steps, you can harness the power of your integrated GPU within a virtual environment, enhancing the performance and capabilities of your virtual machines. This process not only allows for better resource utilization but also opens up new possibilities for running graphically intensive applications within your VMs. References Proxmox iGPU Passthrough to VM","title":"Intel iGPU passthru to Proxmox VM"},{"location":"linux/gpu-passthru-proxmox/#intel-igpu-passthru-to-proxmox-vm","text":"Hey there! \ud83d\udc4b In this section, we'll be covering the Full iGPU Passthrough to a Proxmox Virtual Machine for an Intel integrated iGPU. Warning : You will lose the ability to use the onboard graphics card to access the Proxmox console since Proxmox won't be able to use the Intel's GPU. Edit the grub configuration file /etc/default/grub and find the line that starts with GRUB_CMDLINE_LINUX_DEFAULT . By default, it should look like this: GRUB_CMD_LINUX_DEFAULT=\"quiet\" Add intel_iommu=on iommu=pt to this line. Update the grub configuration for the next reboot with update-grub . Add vfio modules to /etc/modules to allow PCI passthrough: # Modules required for PCI passthrough vfio vfio_iommu_type1 vfio_pci vfio_virqfd # Modules required for Intel GVT-g Split kvmgt Update configuration changes made in your /etc filesystem: update-initramfs -u -k all Reboot Proxmox. Verify that IOMMU is enabled: dmesg | grep -e DMAR -e IOMMU Note : There should be a line that looks like DMAR: IOMMU enabled . If there is no output, something is wrong. Find the PCI address of the iGPU: lspci -nnv | grep VGA . This should result in output similar to this (If you have multiple VGA, look for the one that has the Intel in the name): 00:02.0 VGA compatible controller [0300]: Intel Corporation CometLake-S GT2 [UHD Graphics 630] [8086:3e92] (prog-if 00 [VGA controller]) Create or edit your VM. Make sure the Machine type is set to q35 . Open the web GUI and navigate to the Hardware tab of the VM you want to add a vGPU. Click Add above the device list and then choose PCI Device . Open the Device dropdown and select the iGPU, which you can find using its PCI address. This list uses a different format for the PCI addresses id, 00:02.0 is listed as `0000:00:02.0 Select All Functions , ROM-Bar , PCI-Express and then click Add . Connect to the VM via SSH or any other remote access protocol you prefer. Install the latest version of Intel's Graphics Driver or use the Intel Driver & Support Assistant. Linux VM: Boot the VM. To test the iGPU passthrough was successful, you can use the following command: sudo lspci -nnv | grep VGA The output should include the Intel iGPU: 00:10.0 VGA compatible controller [0300]: Intel Corporation UHD Graphics 630 (Desktop) [8086:3e92] (prog-if 00 [VGA controller]) Now we need to check if the GPU's Driver initialization is working. cd /dev/dri && ls -la The output should include the renderD128 . In conclusion, successfully passing through an Intel iGPU to a Proxmox VM involves several critical steps, from modifying the GRUB configuration to ensuring the correct modules are loaded and the VM is properly configured. By following these steps, you can harness the power of your integrated GPU within a virtual environment, enhancing the performance and capabilities of your virtual machines. This process not only allows for better resource utilization but also opens up new possibilities for running graphically intensive applications within your VMs.","title":"Intel iGPU passthru to Proxmox VM"},{"location":"linux/gpu-passthru-proxmox/#references","text":"Proxmox iGPU Passthrough to VM","title":"References"},{"location":"linux/lvm-volume/","text":"Create LVM volume This section explains the steps to create an LVM (Logical Volume Management) volume and mount it in Linux. LVM allows you to manage disk storage more flexibly and efficiently by abstracting physical disks into logical volumes. Check available disks First, let's check the available disks on your system. Open a terminal and run: sudo fdisk -l Identify the disk you want to use for creating the LVM volume. For example, it might be something like /dev/sdb . Create a Physical Volume (PV) Next, we need to initialize the chosen disk as a Physical Volume for LVM, e.g.: sudo pvcreate /dev/sdb Create a Volume Group (VG) Now, create a Volume Group that will hold the Physical Volume(s), e.g.: sudo vgcreate data-vg /dev/sdb Here, data-vg is the name of the Volume Group, and you can choose a different name if you prefer. Create a Logical Volume (LV) Create a Logical Volume within the Volume Group. This is the volume you will be able to mount and use as a regular disk. sudo lvcreate -l 100 %FREE -n data-lv data-vg Here, 100%FREE represents the size of the Logical Volume, it means that the volume will take all remaining space. data-lv is the name of the Logical Volume. Feel free to adjust the size and name according to your needs. Format the Logical Volume Format the Logical Volume with a file system (e.g., ext4) to make it usable, e.g: sudo mkfs.ext4 /dev/data-vg/data-lv Mount the Logical Volume Create a directory where you want to mount the Logical Volume, and then mount it, e.g: sudo mkdir /mnt/data-lv-mountpoint sudo mount /dev/data-vg/data-lv /mnt/data-lv-mountpoint Automatically Mount at Boot (Optional) If you want the Logical Volume to be automatically mounted when the system starts, you can add an entry to the /etc/fstab file. Open /etc/fstab in a text editor: sudo nano /etc/fstab Add the following line at the end of the file: /dev/data-vg/data-lv /mnt/data-lv-mountpoint ext4 defaults 0 0 Note : replace data-vg and data-lv with your Volume Group and Logical Volume names, and make sure the mount point directory exists. Your LVM volume is now created, formatted, mounted, and ready to use. You can access it from the specified mount point ( /mnt/data-lv-mountpoint in this example).","title":"Create LVM volume"},{"location":"linux/lvm-volume/#create-lvm-volume","text":"This section explains the steps to create an LVM (Logical Volume Management) volume and mount it in Linux. LVM allows you to manage disk storage more flexibly and efficiently by abstracting physical disks into logical volumes.","title":"Create LVM volume"},{"location":"linux/lvm-volume/#check-available-disks","text":"First, let's check the available disks on your system. Open a terminal and run: sudo fdisk -l Identify the disk you want to use for creating the LVM volume. For example, it might be something like /dev/sdb .","title":"Check available disks"},{"location":"linux/lvm-volume/#create-a-physical-volume-pv","text":"Next, we need to initialize the chosen disk as a Physical Volume for LVM, e.g.: sudo pvcreate /dev/sdb","title":"Create a Physical Volume (PV)"},{"location":"linux/lvm-volume/#create-a-volume-group-vg","text":"Now, create a Volume Group that will hold the Physical Volume(s), e.g.: sudo vgcreate data-vg /dev/sdb Here, data-vg is the name of the Volume Group, and you can choose a different name if you prefer.","title":"Create a Volume Group (VG)"},{"location":"linux/lvm-volume/#create-a-logical-volume-lv","text":"Create a Logical Volume within the Volume Group. This is the volume you will be able to mount and use as a regular disk. sudo lvcreate -l 100 %FREE -n data-lv data-vg Here, 100%FREE represents the size of the Logical Volume, it means that the volume will take all remaining space. data-lv is the name of the Logical Volume. Feel free to adjust the size and name according to your needs.","title":"Create a Logical Volume (LV)"},{"location":"linux/lvm-volume/#format-the-logical-volume","text":"Format the Logical Volume with a file system (e.g., ext4) to make it usable, e.g: sudo mkfs.ext4 /dev/data-vg/data-lv","title":"Format the Logical Volume"},{"location":"linux/lvm-volume/#mount-the-logical-volume","text":"Create a directory where you want to mount the Logical Volume, and then mount it, e.g: sudo mkdir /mnt/data-lv-mountpoint sudo mount /dev/data-vg/data-lv /mnt/data-lv-mountpoint","title":"Mount the Logical Volume"},{"location":"linux/lvm-volume/#automatically-mount-at-boot-optional","text":"If you want the Logical Volume to be automatically mounted when the system starts, you can add an entry to the /etc/fstab file. Open /etc/fstab in a text editor: sudo nano /etc/fstab Add the following line at the end of the file: /dev/data-vg/data-lv /mnt/data-lv-mountpoint ext4 defaults 0 0 Note : replace data-vg and data-lv with your Volume Group and Logical Volume names, and make sure the mount point directory exists. Your LVM volume is now created, formatted, mounted, and ready to use. You can access it from the specified mount point ( /mnt/data-lv-mountpoint in this example).","title":"Automatically Mount at Boot (Optional)"},{"location":"linux/open-port-firewalld-rhel/","text":"Open port on RHEL with firewalld To open a port on your RHEL (Red Hat Enterprise Linux) virtual machine, you'll need to configure the firewall settings on the VM itself, not on your macOS host. To open a port on your RHEL VM, you can follow these steps: Connect to your RHEL VM: Use your preferred method to connect to your RHEL VM, such as SSH or a console. Check the status of the firewall: Run the following command to check the status of the firewalld service: sudo systemctl status firewalld Open the desired port: If the firewalld service is active, you can use the firewall-cmd command to open the port. Run the following command, replacing <port> with the actual port number: sudo firewall-cmd --zone = public --add-port = <port>/tcp --permanent This command adds a permanent rule to the public zone of the firewall, opening the specified TCP port. Reload the firewall configuration: After adding the rule, you need to reload the firewall configuration for the changes to take effect. Run the following command: sudo firewall-cmd --reload Verify the rule: You can use the firewall-cmd command with the --list-all option to verify that the rule was added correctly. Run the following command: sudo firewall-cmd --list-all This command displays a list of all the active firewall rules, including the newly added rule for the open port. Once you've completed these steps on your RHEL VM, the specified port should be open and accessible. Please note that these instructions assume you have administrative privileges on the RHEL VM and that you have the firewall-cmd command available.","title":"Open port on RHEL with firewalld"},{"location":"linux/open-port-firewalld-rhel/#open-port-on-rhel-with-firewalld","text":"To open a port on your RHEL (Red Hat Enterprise Linux) virtual machine, you'll need to configure the firewall settings on the VM itself, not on your macOS host. To open a port on your RHEL VM, you can follow these steps: Connect to your RHEL VM: Use your preferred method to connect to your RHEL VM, such as SSH or a console. Check the status of the firewall: Run the following command to check the status of the firewalld service: sudo systemctl status firewalld Open the desired port: If the firewalld service is active, you can use the firewall-cmd command to open the port. Run the following command, replacing <port> with the actual port number: sudo firewall-cmd --zone = public --add-port = <port>/tcp --permanent This command adds a permanent rule to the public zone of the firewall, opening the specified TCP port. Reload the firewall configuration: After adding the rule, you need to reload the firewall configuration for the changes to take effect. Run the following command: sudo firewall-cmd --reload Verify the rule: You can use the firewall-cmd command with the --list-all option to verify that the rule was added correctly. Run the following command: sudo firewall-cmd --list-all This command displays a list of all the active firewall rules, including the newly added rule for the open port. Once you've completed these steps on your RHEL VM, the specified port should be open and accessible. Please note that these instructions assume you have administrative privileges on the RHEL VM and that you have the firewall-cmd command available.","title":"Open port on RHEL with firewalld"},{"location":"linux/pwdless-sudo/","text":"Passwordless sudo on Ubuntu To enable passwordless sudo for a specific user on Ubuntu, you need to make some changes to the sudoers configuration. Here's a step-by-step guide: Open a terminal on your Ubuntu system. Type the following command to edit the sudoers file using the visudo command, which ensures you don't accidentally introduce syntax errors: sudo visudo The visudo command will open the sudoers file in a text editor (usually nano). Look for the line that says %sudo ALL=(ALL:ALL) ALL . This line grants sudo access to the \"sudo\" group. Below the %sudo line, add a new line for the specific user you want to enable passwordless sudo for. The line should follow this format: username ALL=(ALL) NOPASSWD:ALL Replace username with the actual username of the user you want to configure. For example, if the user you want to enable passwordless sudo for is called \"john,\" the line would look like this: john ALL=(ALL) NOPASSWD:ALL Save the changes and exit the text editor. In nano, you can do this by pressing Ctrl + X , then Y to confirm, and finally Enter to save the file with the same name. Once you're back at the terminal, the passwordless sudo configuration should be in effect for the specified user. They will be able to run sudo commands without entering a password. Please note that modifying the sudoers file requires administrative privileges, so you may need to enter your own password to perform these steps. Additionally, exercise caution when granting passwordless sudo access, as it can pose security risks. Make sure you only enable it for trusted users who need it for specific purposes.","title":"Passwordless sudo on Ubuntu"},{"location":"linux/pwdless-sudo/#passwordless-sudo-on-ubuntu","text":"To enable passwordless sudo for a specific user on Ubuntu, you need to make some changes to the sudoers configuration. Here's a step-by-step guide: Open a terminal on your Ubuntu system. Type the following command to edit the sudoers file using the visudo command, which ensures you don't accidentally introduce syntax errors: sudo visudo The visudo command will open the sudoers file in a text editor (usually nano). Look for the line that says %sudo ALL=(ALL:ALL) ALL . This line grants sudo access to the \"sudo\" group. Below the %sudo line, add a new line for the specific user you want to enable passwordless sudo for. The line should follow this format: username ALL=(ALL) NOPASSWD:ALL Replace username with the actual username of the user you want to configure. For example, if the user you want to enable passwordless sudo for is called \"john,\" the line would look like this: john ALL=(ALL) NOPASSWD:ALL Save the changes and exit the text editor. In nano, you can do this by pressing Ctrl + X , then Y to confirm, and finally Enter to save the file with the same name. Once you're back at the terminal, the passwordless sudo configuration should be in effect for the specified user. They will be able to run sudo commands without entering a password. Please note that modifying the sudoers file requires administrative privileges, so you may need to enter your own password to perform these steps. Additionally, exercise caution when granting passwordless sudo access, as it can pose security risks. Make sure you only enable it for trusted users who need it for specific purposes.","title":"Passwordless sudo on Ubuntu"},{"location":"linux/resize-disks-proxmox/","text":"Resize Proxmox Linux VM disk Supporting Docs Resize disks Resizing guest disk On pve shell: qm list # Find VM id qm resize <vmid> <disk> <size> # e.g. qm disk resize 201 scsi1 +4T Enlarge the partition(s) in the virtual disk On linux VM shell: sudo fdisk -l sudo parted /dev/sdb Resize the partition 1 (LVM PV) to occupy the whole remaining space of the hard drive): ( parted ) print Warning: Not all of the space available to /dev/sdb appears to be used, you can fix the GPT to use all of the space ( an extra 268435456 blocks ) or continue with the current setting? Fix/Ignore? F ( parted ) resizepart 1 100 % ( parted ) print ( parted ) quit Enlarge the filesystem(s) in the partitions on the virtual disk On linux VM shell: sudo pvresize /dev/sdb1 sudo lvdisplay sudo lvresize --extents +100%FREE --resizefs /dev/hdd-vg/hdd-lv","title":"Resize Proxmox Linux VM disk"},{"location":"linux/resize-disks-proxmox/#resize-proxmox-linux-vm-disk","text":"","title":"Resize Proxmox Linux VM disk"},{"location":"linux/resize-disks-proxmox/#supporting-docs","text":"Resize disks","title":"Supporting Docs"},{"location":"linux/resize-disks-proxmox/#resizing-guest-disk","text":"On pve shell: qm list # Find VM id qm resize <vmid> <disk> <size> # e.g. qm disk resize 201 scsi1 +4T","title":"Resizing guest disk"},{"location":"linux/resize-disks-proxmox/#enlarge-the-partitions-in-the-virtual-disk","text":"On linux VM shell: sudo fdisk -l sudo parted /dev/sdb Resize the partition 1 (LVM PV) to occupy the whole remaining space of the hard drive): ( parted ) print Warning: Not all of the space available to /dev/sdb appears to be used, you can fix the GPT to use all of the space ( an extra 268435456 blocks ) or continue with the current setting? Fix/Ignore? F ( parted ) resizepart 1 100 % ( parted ) print ( parted ) quit","title":"Enlarge the partition(s) in the virtual disk"},{"location":"linux/resize-disks-proxmox/#enlarge-the-filesystems-in-the-partitions-on-the-virtual-disk","text":"On linux VM shell: sudo pvresize /dev/sdb1 sudo lvdisplay sudo lvresize --extents +100%FREE --resizefs /dev/hdd-vg/hdd-lv","title":"Enlarge the filesystem(s) in the partitions on the virtual disk"},{"location":"linux/rhel-dns-server/","text":"Update DNS server configuration on RHEL Add DNS A Records To update the DNS server configuration on RHEL (Red Hat Enterprise Linux) and add new A records, you can follow these general steps: Log in to your RHEL server with administrative privileges (such as the root user or a user with sudo access). Determine which DNS server software you are using. RHEL typically uses BIND (Berkeley Internet Name Domain) as the default DNS server. However, there are alternative DNS server options like dnsmasq or PowerDNS. The specific steps may vary slightly depending on the DNS server software you have installed. Locate the configuration file for your DNS server. For BIND, the configuration file is typically located at /etc/named.conf . For dnsmasq, it is /etc/dnsmasq.conf . Open the configuration file using a text editor. For example, you can use the vi editor by running the following command: sudo vi /etc/named.conf Within the configuration file, find the zone section that corresponds to the domain for which you want to add A records. The zone section typically begins with a line similar to: zone \"example.com\" IN { Inside the zone section, locate the resource or record directive. This directive is used to define A records. It might look like this: example.com. IN A 192.168.1.100 Add a new line for each A record you want to add. The format is typically: hostname IN A IP_address Replace hostname with the desired host or subdomain name and IP_address with the corresponding IP address. Save the changes and exit the text editor. Restart the DNS server to apply the new configuration. The command to restart the DNS server will depend on the software you are using. For BIND, you can use the following command: sudo systemctl restart named If you are using dnsmasq, the command would be: sudo systemctl restart dnsmasq Verify that the A records have been added successfully by using a DNS lookup tool like nslookup or dig . For example, you can run: nslookup hostname.example.com Replace hostname.example.com with the specific hostname or subdomain you added. That's it! You have now updated the DNS server configuration on RHEL to add new A records. Remember to adjust the steps if you are using a different DNS server software. Create new DNS zone If you want to create a brand new zone in your /etc/named.conf file on RHEL, follow these steps: Open the /etc/named.conf file using a text editor with administrative privileges. For example: sudo vi /etc/named.conf Inside the file, locate the options section. This section contains global configuration options for the BIND DNS server. Within the options section, add a new zone definition by using the following syntax: zone \"example.com\" { type master; file \"/var/named/example.com.zone\"; }; Replace example.com with the name of your desired domain or subdomain. Modify the file directive to specify the path and filename where you want to store the zone file. In this example, the zone file will be stored at /var/named/example.com.zone . Save the changes and exit the text editor. Create the zone file at the specified location ( /var/named/example.com.zone in the example above). The zone file contains the DNS records for the zone you created. You can use a text editor to create and edit the zone file. Here's an example of a basic zone file for reference: $TTL 86400 @ IN SOA ns1.example.com. admin.example.com. ( 2023060101 ; Serial number 3600 ; Refresh 1800 ; Retry 604800 ; Expire 86400 ; Minimum TTL ) @ IN NS ns1.example.com. @ IN NS ns2.example.com. ns1 IN A 192.168.1.10 ns2 IN A 192.168.1.11 Modify the records according to your specific requirements, including the SOA (Start of Authority) record, NS (Name Server) records, and A (Address) records. Save the zone file. Restart the BIND DNS server to apply the changes: sudo systemctl restart named Verify that the new zone is functioning correctly by performing DNS lookups or using DNS testing tools. That's it! You have now created a new zone in your /etc/named.conf file on RHEL and defined the corresponding zone file.","title":"Update DNS server configuration on RHEL"},{"location":"linux/rhel-dns-server/#update-dns-server-configuration-on-rhel","text":"","title":"Update DNS server configuration on RHEL"},{"location":"linux/rhel-dns-server/#add-dns-a-records","text":"To update the DNS server configuration on RHEL (Red Hat Enterprise Linux) and add new A records, you can follow these general steps: Log in to your RHEL server with administrative privileges (such as the root user or a user with sudo access). Determine which DNS server software you are using. RHEL typically uses BIND (Berkeley Internet Name Domain) as the default DNS server. However, there are alternative DNS server options like dnsmasq or PowerDNS. The specific steps may vary slightly depending on the DNS server software you have installed. Locate the configuration file for your DNS server. For BIND, the configuration file is typically located at /etc/named.conf . For dnsmasq, it is /etc/dnsmasq.conf . Open the configuration file using a text editor. For example, you can use the vi editor by running the following command: sudo vi /etc/named.conf Within the configuration file, find the zone section that corresponds to the domain for which you want to add A records. The zone section typically begins with a line similar to: zone \"example.com\" IN { Inside the zone section, locate the resource or record directive. This directive is used to define A records. It might look like this: example.com. IN A 192.168.1.100 Add a new line for each A record you want to add. The format is typically: hostname IN A IP_address Replace hostname with the desired host or subdomain name and IP_address with the corresponding IP address. Save the changes and exit the text editor. Restart the DNS server to apply the new configuration. The command to restart the DNS server will depend on the software you are using. For BIND, you can use the following command: sudo systemctl restart named If you are using dnsmasq, the command would be: sudo systemctl restart dnsmasq Verify that the A records have been added successfully by using a DNS lookup tool like nslookup or dig . For example, you can run: nslookup hostname.example.com Replace hostname.example.com with the specific hostname or subdomain you added. That's it! You have now updated the DNS server configuration on RHEL to add new A records. Remember to adjust the steps if you are using a different DNS server software.","title":"Add DNS A Records"},{"location":"linux/rhel-dns-server/#create-new-dns-zone","text":"If you want to create a brand new zone in your /etc/named.conf file on RHEL, follow these steps: Open the /etc/named.conf file using a text editor with administrative privileges. For example: sudo vi /etc/named.conf Inside the file, locate the options section. This section contains global configuration options for the BIND DNS server. Within the options section, add a new zone definition by using the following syntax: zone \"example.com\" { type master; file \"/var/named/example.com.zone\"; }; Replace example.com with the name of your desired domain or subdomain. Modify the file directive to specify the path and filename where you want to store the zone file. In this example, the zone file will be stored at /var/named/example.com.zone . Save the changes and exit the text editor. Create the zone file at the specified location ( /var/named/example.com.zone in the example above). The zone file contains the DNS records for the zone you created. You can use a text editor to create and edit the zone file. Here's an example of a basic zone file for reference: $TTL 86400 @ IN SOA ns1.example.com. admin.example.com. ( 2023060101 ; Serial number 3600 ; Refresh 1800 ; Retry 604800 ; Expire 86400 ; Minimum TTL ) @ IN NS ns1.example.com. @ IN NS ns2.example.com. ns1 IN A 192.168.1.10 ns2 IN A 192.168.1.11 Modify the records according to your specific requirements, including the SOA (Start of Authority) record, NS (Name Server) records, and A (Address) records. Save the zone file. Restart the BIND DNS server to apply the changes: sudo systemctl restart named Verify that the new zone is functioning correctly by performing DNS lookups or using DNS testing tools. That's it! You have now created a new zone in your /etc/named.conf file on RHEL and defined the corresponding zone file.","title":"Create new DNS zone"},{"location":"linux/ubuntu-trust-ca/","text":"Ubuntu: trust a new Certificate Authority Assuming a PEM-formatted root CA certificate is in local-ca.crt , follow the steps below to install it. Note : It is important to have the .crt extension on the file, otherwise it will not be processed. sudo apt-get install -y ca-certificates sudo cp local-ca.crt /usr/local/share/ca-certificates sudo update-ca-certificates After this point you can use Ubuntu\u2019s tools like curl , wget or docker login to connect to local sites. Source: https://ubuntu.com/server/docs/security-trust-store","title":"Ubuntu: trust a new Certificate Authority"},{"location":"linux/ubuntu-trust-ca/#ubuntu-trust-a-new-certificate-authority","text":"Assuming a PEM-formatted root CA certificate is in local-ca.crt , follow the steps below to install it. Note : It is important to have the .crt extension on the file, otherwise it will not be processed. sudo apt-get install -y ca-certificates sudo cp local-ca.crt /usr/local/share/ca-certificates sudo update-ca-certificates After this point you can use Ubuntu\u2019s tools like curl , wget or docker login to connect to local sites. Source: https://ubuntu.com/server/docs/security-trust-store","title":"Ubuntu: trust a new Certificate Authority"},{"location":"software/vault/delete-key/","text":"Delete Hashicorp Vault key Because this is a potentially catastrophic operation, the deletion_allowed attribute of a key is disabled by default and must therefore be set using the key's /config endpoint: export PATH_TO_KEY = <CHANGEME> # Set vault path to your key cat <<EOF > payload.json { \"deletion_allowed\": true } EOF curl --header X-Vault-Token: $VAULT_TOKEN \\ --request POST \\ --data @payload.json \\ $VAULT_ADDR /v1/transit/ $PATH_TO_KEY /config vault delete $PATH_TO_KEY rm payload.json","title":"Delete Hashicorp Vault key"},{"location":"software/vault/delete-key/#delete-hashicorp-vault-key","text":"Because this is a potentially catastrophic operation, the deletion_allowed attribute of a key is disabled by default and must therefore be set using the key's /config endpoint: export PATH_TO_KEY = <CHANGEME> # Set vault path to your key cat <<EOF > payload.json { \"deletion_allowed\": true } EOF curl --header X-Vault-Token: $VAULT_TOKEN \\ --request POST \\ --data @payload.json \\ $VAULT_ADDR /v1/transit/ $PATH_TO_KEY /config vault delete $PATH_TO_KEY rm payload.json","title":"Delete Hashicorp Vault key"}]}